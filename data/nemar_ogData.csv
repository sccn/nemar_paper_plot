id,created,uploader,latestSnapshot,name,publishDate,onBrainlife,sessionsNum,file_size,byte_size_format,totalFiles,participants,age_min,age_max,BIDSVersion,License,Authors,Acknowledgements,HowToAcknowledge,Funding,ReferencesAndLinks,DatasetDOI,EthicsApprovals,tasks,HEDVersion,modalities,readme,local_dataset,processed,hedAnnotation
ds000248,2018-03-30 14:58:53,Mainak Jas,1.2.4,ds000248,2018-03-30 14:58:53,1,1,186217000,0 TB,22,1,0,0,1.4.0,CC0,"Alexandre Gramfort, Matti S Hämäläinen","Alexandre Gramfort, Mainak Jas, and Stefan Appelhoff prepared and updated the data in BIDS format.","If you reference this dataset in a publication, please acknowledge its authors and cite MNE papers: A. Gramfort, M. Luessi, E. Larson, D. Engemann, D. Strohmeier, C. Brodbeck, L. Parkkonen, M. Hämäläinen, MNE software for processing MEG and EEG data, NeuroImage, Volume 86, 1 February 2014, Pages 446-460, ISSN 1053-8119 and A. Gramfort, M. Luessi, E. Larson, D. Engemann, D. Strohmeier, C. Brodbeck, R. Goj, M. Jas, T. Brooks, L. Parkkonen, M. Hämäläinen, MEG and EEG data analysis with MNE-Python, Frontiers in Neuroscience, Volume 7, 2013, ISSN 1662-453X",NIH 5R01EB009048 ===NEMAR-SEP=== NIH 1R01EB009048 ===NEMAR-SEP=== NIH R01EB006385 ===NEMAR-SEP=== NIH 1R01HD40712 ===NEMAR-SEP=== NIH 1R01NS44319 ===NEMAR-SEP=== NIH 2R01NS37462 ===NEMAR-SEP=== NIH P41EB015896 ===NEMAR-SEP=== ANR-11-IDEX-0003-02 ===NEMAR-SEP=== ERC-StG-263584 ===NEMAR-SEP=== ERC-StG-676943 ===NEMAR-SEP=== ANR-14-NEUC-0002-01,https://doi.org/10.1016/j.neuroimage.2014.02.017 ===NEMAR-SEP=== https://doi.org/10.3389/fnins.2013.00267 ===NEMAR-SEP=== https://mne.tools/stable/overview/datasets\_index.html#sample,10.18112/openneuro.ds000248.v1.2.4,,"audiovisual, noise",,"MEG, MRI","MNE-Sample-Data
 ---------------
 

 The MNE software is accompanied by a sample data set. These data were acquired with the Neuromag Vectorview system at MGH/HMS/MIT Athinoula A. Martinos Center Biomedical Imaging. EEG data from a 60-channel electrode cap was acquired simultaneously with the MEG. The original MRI data set was acquired with a Siemens 1.5 T Sonata scanner using an MPRAGE sequence.
 

 In the MEG/EEG experiment, checkerboard patterns were presented into the left and right visual field, interspersed by tones to the left or right ear. The interval between the stimuli was 750 ms. Occasionally a smiley face was presented at the center of the visual field. The subject was asked to press a key with the right index finger as soon as possible after the appearance of the face.
 

 

 Freesurfer derivatives
 ----------------------
 

 - Calls from the command line:
  - `recon-all -i sub-01/anat/sub-01\_T1w.nii.gz -s sub-01 -all`
  - `mne make\_scalp\_surfaces -s sub-01 --overwrite --force`
  - `mne flash\_bem -s sub-01 --overwrite`
  - `mne watershed\_bem -s sub-01 --overwrite`
 

 References
 ----------
 

 A. Gramfort, M. Luessi, E. Larson, D. Engemann, D. Strohmeier, C. Brodbeck, L. Parkkonen, M. Hämäläinen, MNE software for processing MEG and EEG data, NeuroImage, Volume 86, 1 February 2014, Pages 446-460, ISSN 1053-8119
 

 A. Gramfort, M. Luessi, E. Larson, D. Engemann, D. Strohmeier, C. Brodbeck, R. Goj, M. Jas, T. Brooks, L. Parkkonen, M. Hämäläinen, MEG and EEG data analysis with MNE-Python, Frontiers in Neuroscience, Volume 7, 2013, ISSN 1662-453X""
 

 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., Moreau, J., Oostenveld, R., Schoffelen, J., Tadel, F., Wexler, J., Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5, 180110. http://doi.org/10.1038/sdata.2018.110
 

 

 

 References
 ----------
 Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., Moreau, J., Oostenveld, R., Schoffelen, J., Tadel, F., Wexler, J., Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5, 180110. https://doi.org/10.1038/sdata.2018.110",1,1,0
ds000117,2018-03-30 13:14:28,Richard Henson,1.0.6,"Multisubject, multimodal face processing",2018-03-30 13:14:28,1,2,91073400000,84.8 GB,1671,16,23,31,1.0.2,CC0,"Wakeman, DG, Henson, RN","This work was supported by the UK Medical Research Council (SUAG/010
 RG91365) and Elekta Ltd. We would also like to acknowledge the contribution of Andre van der Kouwe and the A.A. Martinos Center for Biomedical Imaging (MGH) for providing the Multi-Echo FLASH sequences used in this work.",Cite this paper: https://www.ncbi.nlm.nih.gov/pubmed/25977808 and consider including the following message: 'This data was obtained from the OpenNeuro database. Its accession number is ds000117',"UK Medical Research Council (SUAG/010 RG91365), Elekta Ltd.",https://www.ncbi.nlm.nih.gov/pubmed/25977808 ===NEMAR-SEP=== https://openfmri.org/dataset/ds000117/ ===NEMAR-SEP=== ftp://ftp.mrc-cbu.cam.ac.uk/personal/rik.henson/wakemandg_hensonrn/Publications/,doi:10.18112/openneuro.ds000117.v1.0.6,,facerecognition,,"MRI, MEG","This dataset was obtained from the OpenNeuro project (https://www.openneuro.org). Accession #: ds000117
 

 The same dataset is also available here: ftp://ftp.mrc-cbu.cam.ac.uk/personal/rik.henson/wakemandg_hensonrn/, but in a non-BIDS format (which may be easier to download by subject rather than by modality)
 

 Note that it is a subset of the data available on OpenfMRI (http://www.openfmri.org; Accession #: ds000117).
 

 

 Description: Multi-subject, multi-modal (sMRI+fMRI+MEG+EEG) neuroimaging dataset on face processing
 

 Please cite the following reference if you use these data:
 

  Wakeman, D.G. & Henson, R.N. (2015). A multi-subject, multi-modal human neuroimaging dataset. Sci. Data 2:150001 doi: 10.1038/sdata.2015.1
 

 The data have been used in several publications including, for example:
 

  Henson, R.N., Abdulrahman, H., Flandin, G. & Litvak, V. (2019). Multimodal integration of M/EEG and f/MRI data in SPM12. Frontiers in Neuroscience, Methods, 13, 300.
 

  Henson, R.N., Wakeman, D.G., Litvak, V. & Friston, K.J. (2011). A Parametric Empirical Bayesian framework for the EEG/MEG inverse problem: generative models for multisubject and multimodal integration. Frontiers in Human Neuroscience, 5, 76, 1-16.
 

  Chapter 42 of the SPM12 manual (http://www.fil.ion.ucl.ac.uk/spm/doc/manual.pdf)
 

 (see ftp://ftp.mrc-cbu.cam.ac.uk/personal/rik.henson/wakemandg_hensonrn/Publications for full list), as well as the BioMag2010 data competition and the Kaggle competition: https://www.kaggle.com/c/decoding-the-human-brain)
 

 ==================================================================================
 

 func/
 -----
 Unlike in v1-v3 of this dataset, the first two (dummy) volumes have now been removed (as stated in *.json), so event onset times correctly refer to t=0 at start of third volume 
 

 Note that, owing to scanner error, Subject 10 only has 170 volumes in last run (Run 9) (hence the BIDS warning of some onsets in events.tsv file being later than the data)
 

 meg/
 ----
 Three anatomical fiducials were digitized for aligning the MEG with the MRI: the nasion 
 (lowest depression between the eyes) and the left and right ears (lowest depression 
 between the tragus and the helix, above the tragus). This procedure is illustrated here:
 http://neuroimage.usc.edu/brainstorm/CoordinateSystems#Subject_Coordinate_System_.28SCS_.2F_CTF.29
 and in task-facerecognition_fidinfo.pdf 
 

 The following triggers are included in the .fif files and are also used in the “trigger” column of the meg and bold events files:
 

 Trigger Label  Simplified Label
 

 5  Initial Famous Face  FAMOUS
 6  Immediate Repeat Famous Face FAMOUS
 7  Delayed Repeat Famous Face FAMOUS
 13 Initial Unfamiliar Face  UNFAMILIAR
 14 Immediate Repeat Unfamiliar Face UNFAMILIAR
 15 Delayed Repeat Unfamiliar Face UNFAMILIAR
 17 Initial Scrambled Face SCRAMBLED
 18 Immediate Repeat Scrambled Face  SCRAMBLED
 19 Delayed Repeat Scrambled Face  SCRAMBLED
 

 stimuli/meg/
 ------------
 The .bmp files correspond to those described in the text. There are 6 additional images in this directory, which were used in the practice experiment to familiarize participants with the task (hence some more BIDS validator warnings)
 

 stimuli/mri/
 ------------
 The .bmp files correspond to those described in the text.
  
 Defacing
 --------
 Defacing of MPRAGE T1 images was performed by the submitter. A subset of subjects have given consent for non-defaced versions to be shared - in which case, please contact rik.henson@mrc-cbu.cam.ac.uk.
  
 Quality Control
 ---------------
 Mriqc was run on the dataset. Results are located in derivatives/mriqc. Learn more about it here: https://mriqc.readthedocs.io/en/latest/
  
 Known Issues
 ------------
 N/A
 

 Relationship of Subject Numbering relative to other versions of Dataset
 ------------
 

 There are multiple versions of the dataset available on the web (see notes above), and these entailed a renumbering of the subjects for various reasons. Here are all the versions and how to match subjects between them (plus some rationale and history for different versions):
  
 1. Original Paper (N=19): Wakeman & Henson (2015): doi:10.1038/sdata.2015.1 
  Number refers to order that tested (and some, eg 4, 7, 13 etc were excluded for not completing both MRI and MEG sessions)
  
 2. openfMRI, renumbered from paper: http://openfmri.org/s3-browser/?prefix=ds000117/ds000117_R0.1.1/uncompressed/
  Numbers 1-19 just made contiguous
  
 3. FTP subset of N=16: ftp: ftp://ftp.mrc-cbu.cam.ac.uk/personal/rik.henson/wakemandg_hensonrn/  
  This set was used for SPM Courses
  Designed to illustrate multimodal integration, so wanted good MRI+MEG+EEG data for all subjects
  Removed original subject_01 and subject_06 because bad EEG data; subject_19 because poor EEG and fMRI data
  (And renumbered subject_14 for some reason).
  
 4. Current OpenNeuro subset N=16 used for (BIDS): https://openneuro.org/datasets/ds000117 
  OpenNeuro was rebranding of openfMRI, and enforced BIDS format
  Since this version designed to illustrate multi-modal BIDS, kept same numbering as FTP
  
 W&H2015  openfMRI FTP openNeuro
 ========  ====== ===  =======
 subject_01 sub001
 subject_02 sub002 Sub01  sub-01
 subject_03 sub003 Sub02  sub-02
 subject_05 sub004 Sub03  sub-03
 subject_06 sub005
 subject_08 sub006 Sub05  sub-05
 subject_09 sub007 Sub06  sub-06
 subject_10 sub008 Sub07  sub-07
 subject_11 sub009 Sub08  sub-08
 subject_12 sub010 Sub09  sub-09
 subject_14 sub011 Sub04  sub-04
 subject_15 sub012 Sub10  sub-10
 subject_16 sub013 Sub11  sub-11
 subject_17 sub014 Sub12  sub-12
 subject_18 sub015 Sub13  sub-13
 subject_19 sub016
 subject_23 sub017 Sub14  sub-14
 subject_24 sub018 Sub15  sub-15
 subject_25 sub019 Sub16  sub-16",1,0,0
ds000247,2018-03-30 18:40:38,Julia Guiomar Niso Galán,1.0.2,OMEGA_RestingState_sample,2018-03-30 18:40:38,1,1,11024100000,10.3 GB,190,5,21,35,1.0.2,CC0,"Guiomar Niso, Jeremy Moreau, Elizabeth Bock, Francois Tadel, Sylvain Baillet",,"- You are free to use all data in OMEGA for research purposes; please aknowledge its authors and cite the following reference in your publications if you have used data from OMEGA: 
 

 - Niso G., Rogers C., Moreau J.T., Chen L.Y., Madjar C., Das S., Bock E., Tadel F., Evans A.C., Jolicoeur P., Baillet S. (2016). OMEGA: The Open MEG Archive. NeuroImage 124, 1182-1187. doi: https://doi.org/10.1016/j.neuroimage.2015.04.028. 
 OMEGA is available at: https://omega.bic.mni.mcgill.ca 
 

 - And please include the following message: *'This data was obtained from the OpenNeuro database. Its accession number is ds000247'*",Quebec Bioimaging Network Strategic Initiative (QBIN 5886),"Niso G., Rogers C., Moreau J.T., Chen L.Y., Madjar C., Das S., Bock E., Tadel F., Evans A.C., Jolicoeur P., Baillet S. (2016). OMEGA: The Open MEG Archive. NeuroImage 124, 1182-1187. doi: https://doi.org/10.1016/j.neuroimage.2015.04.028. ===NEMAR-SEP=== https://www.mcgill.ca/bic/resources/omega ===NEMAR-SEP=== https://openneuro.org/datasets/ds000247",doi:10.18112/openneuro.ds000247.v1.0.2,,"rest, noise",,"MEG, MRI","# OMEGA - Resting State Sample Dataset
 

 ## License
 

 - This dataset was obtained from **The Open MEG Archive** (OMEGA, https://omega.bic.mni.mcgill.ca).
 

 - You are free to use all data in OMEGA for research purposes; please acknowledge its authors and cite the following reference in your publications if you have used data from OMEGA: 
 

 - Niso G., Rogers C., Moreau J.T., Chen L.Y., Madjar C., Das S., Bock E., Tadel F., Evans A.C., Jolicoeur P., Baillet S. (2016). OMEGA: The Open MEG Archive. NeuroImage 124, 1182-1187. doi: https://doi.org/10.1016/j.neuroimage.2015.04.028. OMEGA is available at: https://omega.bic.mni.mcgill.ca
 

 

 ## Description
  
 

 **Experiment**
 

 - 5 subjects x 5 minute resting sessions, eyes open 
 

 **MEG acquisition**
 

 - Recorded at the Montreal Neurological Institute in 2012-2016
 - Acquisition with CTF 275 MEG system at 2400Hz sampling rate
 - Anti-aliasing low-pass filter at 600Hz, files may be saved with or without the CTF 3rd order gradient compensation
 - Recorded channels (at least 297), include:
  * 26 MEG reference sensors (#2-#27)
  * 270 MEG axial gradiometers (#28-#297)
  * 1 ECG bipolar (EEG057/#298) - Not available in the empty room recordings
  * 1 vertical EOG bipolar (EEG058/#299) - Not available in the empty room recordings
  * 1 horizontal EOG bipolar (EEG059/#300) - Not available in the empty room recordings 
 

 **Head shape and fiducial points**
 

 - 3D digitization using a Polhemus Fastrak device driven by Brainstorm. The .pos files contain:
  * The center of the CTF coils
  * The anatomical references we use in Brainstorm: nasion and ears as illustrated here
  * Around 100 head points distributed on the hard parts of the head (no soft tissues). 
 

 **Subject anatomy**
 

 - Structural T1 image (defaced for anonymization purposes)
 - Processed with FreeSurfer 5.3
 - The anatomical fiducials (NAS, LPA, RPA) have already been marked and saved in the files fiducials.m 
 

 

 **BIDS**
 

 - The data in this dataset has been organized according to the MEG-BIDS specification (Brain Imaging Data Structure, http://bids.neuroimaging.io) (Niso et al. 2018)
 

 - Niso G., Gorgolewski K.J., Bock E., Brooks T.L., Flandin G., Gramfort A., Henson R.N., Jas M., Litvak V., Moreau J., Oostenveld R., Schoffelen J.M., Tadel F., Wexler J., Baillet S. (2018). MEG-BIDS: an extension to the Brain Imaging Data Structure for magnetoencephalography. Scientific Data; 5, 180110. https://doi.org/10.1038/sdata.2018.110
 

 

 **Release history:**
 

 - 2016-12-01: initial release
 - 2018-07-18: release OpenNeuro ds000247 (00001 and 00002)",1,0,0
ds001784,2019-03-08 5:11:02,Sam Zorowitz,1.1.2,EMOTE-afMSIT,2019-03-08 5:11:02,1,1,190079000,0 TB,76,14,0,0,1.2.0,CC0,"Alik Widge, Sam Zorowitz","We gratefully acknowledge technical assistance from Amanda Arulpragasam, Andrew Corse, Nina Levar, and Tommi Raij with subject recruitment and data collection.",Please cite the references below and consider including the following message: 'This data was obtained from the OpenNeuro database. Its accession number is ds001784.',"This work was supported by grants from the Brain & Behavior Research Foundation, Picower Family Foundation, MIT Picower Institute Innovation Fund, Defense Advanced Research Projects Agency (DARPA) under Cooperative Agreement Number W911NF-14-2-0045 issued by the Army Research Organization (ARO) contracting office in support of DARPA's SUBNETS Program, Office of Naval Research MURI N00014-16-1-2832, and National Institutes of Health (R21MH109722; R03MH111320; UH3NS100548; R37MH087027). The views, opinions, and findings expressed are those of the authors. They should not be interpreted as representing the official views or policies of the Department of Defense, Department of Health & Human Services, any other branch of the U.S. Government, or any other funding entity.","Widge, A. S., Zorowitz, S., Basu, I., Paulk, A. C., Cash, S. S., Eskandar, E. N., Deckersbach, T., Miller, E. K., Dougherty, D. D. Deep Brain Stimulation of the Internal Capsule Enhances Human Cognitive Control and Prefrontal Cortex Function. https://doi.org/10.1038/s41467-019-09557-4 ===NEMAR-SEP=== Widge, A. S., Zorowitz, S., Link, K., Miller, E. K., Deckersbach, T., Eskandar, E. N., & Dougherty, D. D. (2016). Ventral capsule/ventral striatum deep brain stimulation does not consistently diminish occipital cross-frequency coupling. Biological psychiatry, 80(7), e59-e60. http://doi.org/10.1016/j.biopsych.2015.10.029",10.18112/openneuro.ds001784.v1.1.2,,"afmsit, rest",,"T1w, beh, channels","Effects of ON/OFF deep brain stimulation on cognitive control in treatment-resistant depression (EEG)
 

 Please cite the following reference if you use the afMSIT data:
 

 Widge, A. S., Zorowitz, S., Basu, I., Paulk, A. C., Cash, S. S., Eskandar, E. N., Deckersbach, T., Miller, E. K., Dougherty, D. D. Deep Brain Stimulation of the Internal Capsule Enhances Human Cognitive Control and Prefrontal Cortex Function. Nature Communications. https://doi.org/10.1038/s41467-019-09557-4
 

 Please cite the following reference if you use the resting state data:
 

 Widge, A. S., Zorowitz, S., Link, K., Miller, E. K., Deckersbach, T., Eskandar, E. N., & Dougherty, D. D. (2016). Ventral capsule/ventral striatum deep brain stimulation does not consistently diminish occipital cross-frequency coupling. Biological psychiatry, 80(7), e59-e60. http://doi.org/10.1016/j.biopsych.2015.10.029
 

 afmsit
 --------
 To probe cognitive flexibility, we employed a modified version of the Multi-Source Interference Task (MSIT). The MSIT requires subjects to identify which of a set of three numbers is different than its neighbors. Subjects must keep three fingers of their right hand positioned over response keys corresponding to the digits 1-3. In Control (non-interference) trials, the target is in the same spatial position as its corresponding response key, and the flanking digits are not valid responses (i.e., they are 0s). In Interference trials, the target is out-of-position relative to its corresponding key-press and is flanked by other viable targets. MSIT has been shown to produce robust functional MRI and electrophysiologic changes, with a significant (Interference-Control) difference often detectable at the individual subject level. 
 

 We further added an emotional interference dimension, based on a hypothesis that subjects with severe treatment-resistant illness would be attentionally biased towards negative pictures. Before each MSIT trial, an image selected from the International Affective Picture System, or IAPS, was presented. The image remained on-screen, partially obscured by the MSIT stimulus, for the trial duration. A fixed subset of 144 images were selected from the overall IAPS dataset to cover the range of available valence (positive, neutral, negative) and emotional arousal ratings.
 

 Each block of trials contained 72 Control and 72 Interference trials. We assigned positive, neutral, and negative IAPS images assigned to each trial type in a counterbalanced fashion, such that each image was presented once in a Control and once in an Interference context. The 144 images were split between these two 144-trial blocks in a manner that minimized the mean squared pairwise differences between image ratings when rank-ordered by their valence. To prevent response sets or habituation, trial sequence in each block was pseudo-randomized so that subjects never had more than two trials in a row that shared the same valence, interference level, or desired response finger. This highly interleaved trial design was expected to place greater demands on cognitive control systems by reducing predictability of the stimuli. Subjects viewed the IAPS picture alone for 400 ms, were presented with the MSIT stimulus and given up to 1500 ms to respond, and then viewed a fixation cross for 3-5 seconds (randomized with a uniform distribution). They were instructed to minimize eye blinking during the trial and to blink freely during the fixation period. Before data collection, subjects performed a block of 20 trials where they received correct/incorrect feedback, followed by another block of 40 trials without feedback. They repeated this practice, if necessary, until they achieved over 90\% correct responses (counting missed trials as incorrect). 
 

 eeg
 ----
 Electroencephalographic data were acquired at 1450 Hz (Nexstim eXimia EEG) from 60 channels placed according to the international 10-20 system and the manufacturer's standard cap. The ground electrode was placed on the bridge of the nose. One diagonal bipolar electro-oculogram (EOG) channel was placed around the right eye. Channels were prepared to less than 5 kiloOhms impedance. The scalp location of each channel was digitized after cap preparation and prior to recordings. We also digitized the nasion and both pre-auricular points, plus 100 additional scalp points not corresponding to any EEG sensor, to improve the quality of MRI-to-digitization co-registration. 
 

 All subjects first completed an MSIT block with their DBS on at its usual clinical settings (DBS ON). A trained clinician then de-activated the bilateral implanted neurostimulators, and the subject rested for at least 1 hour without removing the EEG cap. After re-preparing any high-impedance channels, subjects again performed MSIT (DBS OFF) before neurostimulator re-activation. Subjects were aware of their device status, as were the experimenters, although no subject experienced adverse psychological consequences from the study manipulation.
 

 Three anatomical fiducials were digitized for aligning the EEG with the pre-operative MRI: the nasion (lowest depression between the eyes) and the left and right ears (lowest depression between the tragus and the helix, above the tragus). This procedure is illustrated here: http://neuroimage.usc.edu/brainstorm/CoordinateSystems#Subject\_Coordinate\_System\_.28SCS\_.2F\_CTF.29
 

 The following triggers are included in the trigger channels of the .fif files:
 

 Channel Event  Description
 Trig1 Onset Start of trial (onset of IAPS)
 Trig2 Onset Start of response window (onset of MSIT)
 Trig1 Offset Response time
 Trig2 Offset End of trial
 

 rest
 ------
 In addition, we were able to collect several minutes of eyes-open resting state data with both DBS ON and OFF in several (but not all) patients. Please see participants.tsv to find which patients have resting state data.
 

 code
 ------
 The code used for data analysis is publicly available at:
 https://github.com/mghneurotherapeutics/EMOTE-afMSIT",1,1,0
ds001785,2019-03-08 8:46:20,Michael Pereira,1.1.1,Perithreshold Tactile detection task with confidence,2021-02-26 13:26:53,1,1,26701800000,0.027 TB,242,18,18,34,1.1.1,CC0,"Michael Pereira, Pierre Mégevand, Mi Xue Tan, Wenwen Chang, Shuo Wang, Ali Rezai, Margitta Seeck, Marco Corniola, Shahan Momjian, Fosco Bernasconi, Olaf Blanke, Nathan Faivre",,"If you reference this dataset in your publications, please acknowledge its authors. Also, please include the following message: 'This data was obtained from the Open Neuro database.'",Bertarelli Foundation ===NEMAR-SEP=== Swiss National Science Foundation ===NEMAR-SEP=== European Research Council (ERC),List of papers or websites,10.18112/openneuro.ds001785.v1.1.1,,tactile detection,,EEG,"This dataset contains the EEG used in the paper: Evidence accumulation relates to perceptual consciousness and monitoring, 2021, Nature Communications
 

 Participants:
 Twenty healthy participants (7 woman; age: 25.2, SD = 4.1) participated in this study for a 40 CHF compensation. Subjects gave written informed consent prior to participating and all experimental procedures were approved by the Commission Cantonale d'Ethique de la Recherche de la République et Canton de Genève (2015-00092 15-273). 
 

 One patient with intractable epilepsy. The patient provided informed written consent for the present study which was approved by the Commission Cantonale d’Ethique de la Recherche de la République et Canton de Genève (2016-01856). 
 

 Stimuli were applied on the lateral palmar side of the right wrist using a MMC3 Haptuator vibrotactile device from TactileLabs Inc. (Montreal, Canada) driven by a 230 Hz sinusoid audio signal lasting 100 ms. Experiments started by a simple estimation of the individual detection threshold. The tactile stimulus was applied with decreasing intensity with steps corresponding to 2\% of the initial intensity until the participant reported not feeling it anymore three times in a row. We then repeated the same procedure but with increasing intensity and until the participant reported feeling the vibration three times in a row. The perceptual threshold was estimated to be the average between the two thresholds found using this procedure. This approximation was then used as a seed value for an adaptive staircase during the main experiment.
  
 Participants sat in front of a computer screen. A white fixation cross appeared in the middle of the screen for 2 s. From the moment the fixation cross turned green, participants were told that a tactile stimulus could be applied at any moment during the next 2 s. During this period, stimulus onset was uniformly distributed in 80\% of trials, the 20\% remaining trials served as catch trials. In all trials, 1 second after the green cross disappeared, participants were prompted to answer with the keyboard whether they felt the stimulus or not. Following a 500 ms stimulus onset asynchrony, participants were asked to report the confidence in their first order response by moving a slider on a visual analog scale with marks at 0 (certainty that the first-order response was erroneous), 0.5 (unsure about the first-order response) and 1.0 (certainty that the first-order response was correct). Detection and confidence reports were provided with the left (non-stimulated) hand, using different keys. The total experiment included 500 trials divided in 10 blocks and lasted about 2 hours. 
 

 

 Recordings:
 Electroencephalographic data were acquired from 62 active electrodes (10-20 montage) using a WaveGuard EEG cap and amplifier (ANTNeuro, Hengelo, The Netherlands) and digitized at a sampling rate of 1024 Hz. Horizontal and vertical electrooculography (EOG) was derived using bipolar referenced electrodes placed around participants' eyes. The audio signal driving the vibrotactile actuator was recorded as an extra channel to precisely realign data to stimulus onset.
 

 For the patient, electrocorticographical data was obtained through a 24 electrode ECoG grid (Ad-Tech Medical) covering the left hemisphere from the premotor cortex to the superior parietal lobule. The electrodes had a 4 mm diameter with 2.3 mm exposed corresponding to an area of 4.15 mm2. The data was amplified and sampled at 2048 Hz (Brain Quick LTM, Micromed, Treviso, Italy).",1,1,0
ds001787,2019-03-09 0:48:05,Arnaud Delorme,1.1.1,EEG meditation study,2019-03-09 0:48:05,1,3,6112010000,5.7 GB,141,24,29,78,1.1.1,CC0,"Arnaud Delorme, Tracy Brandmeyer",,,,https://www.ncbi.nlm.nih.gov/pubmed/27815577,doi:10.18112/openneuro.ds001787.v1.1.1,,meditation,,EEG,"This meditation experiment contains 24 subjects. Subjects were
 meditating and were interupted about every 2 minutes to indicate
 their level of concentration and mind wandering. The scientific
 article (see Reference) contains all methodological details.
 

 Note that although the original files were recorded at 2048 Hz, they were downsampled to 256 Hz using the BDF decimator provided by BIOSEMI (https://www.biosemi.com/download.htm).
 

 - Arnaud Delorme (October 17, 2018; updated June 2024)",1,0,0
ds001810,2019-03-21 17:46:19,Leon Reteig,1.1.0,"EEG study of the attentional blink; before, during, and after transcranial Direct Current Stimulation (tDCS)",2019-05-28 11:48:47,1,6,49117400000,0.049 TB,1678,47,0,0,v1.2.0-dev,CC0,"Leon C. Reteig, Lionel A. Newman, K. Richard Ridderinkhof, Heleen A. Slagter",Daphne Box and Esther van der Giessen for assistance with data collection,Please cite this paper: [FIXME] PUBLICATION LINK HERE,NWO Research Talent Grant 406-12-079,"1. Noury, N., Hipp, J. F., & Siegel, M. (2016). Physiological processes non-linearly affect electrophysiological recordings during transcranial electric stimulation. NeuroImage. http://doi.org/10.1016/j.neuroimage.2016.03.065 ===NEMAR-SEP=== 2. Gebodh, N., Esmaeilpour, Z., Adair, D., Chelette, K., Dmochowski, J., Woods, A. J., … Bikson, M. (2019). Inherent physiological artifacts in EEG during tDCS. NeuroImage, 185(September 2018), 408–424. http://doi.org/10.1016/j.neuroimage.2018.10.025",10.18112/openneuro.ds001810.v1.1.0,,Attentional blink,,"channels, eeg, events","Overview
 --------
 AB\_tDCS-EEG: EEG data from participants who performed an attentional blink (AB) task before, during and after transcranial Direct Current Stimulation (tDCS). They visited the lab twice (complete data for 40 participants; some did only one session); on each visit they received either anodal or cathodal tDCS.
 

 Sessions
 --------
 Both lab visits were about one week apart. During each visit, three EEG files were recorded, each about 20 minutes long: before applying tDCS (""pre""), during tDCS (""tDCS""), and after tDCS (""post""). The order of the visits (anodal vs. cathodal) differs across participants.
 

 Each block for each visit is considered a session here (you could also argue this dataset should have 2 ""sessions"" [visits] with 3 ""runs"" [blocks], but given there was a manipulation between the runs, it seemed better to count everything as a session). So each sub*-folder with complete data has 6 sessions:
 

 1. anodalpost (data from anodal visit, after tDCS)
 2. anodalpre (data from anodal visit, before tDCS)
 3. anodaltDCS (data from anodal visit, during tDCS)
 4. cathodalpost (data from cathodal visit, after tDCS)
 5. cathodalpre (data from cathodal visit, before tDCS)
 6. cathodaltDCS (data from cathodal visit, during tDCS)
 

 See the \_sessions.tsv and \_sessions.json files in each sub*-folder for details.
 

 Concurrent tDCS-EEG
 --------
 Recording EEG during tDCS introduces some problems, most importantly blocked channels and artifacts.
 

 Blocked channels
 ========
 The rubber tDCS electrodes were affixed to the scalp using conductive paste. Because they sat under the EEG headcap, the electrodes blocked a few of the slots, meaning these channels could not be plugged in. The channels affected vary from subject to subject (and even slightly from visit to visit). They are marked with ""status"" = ""bad"" in each session's \_channels.tsv file.
 

 The tDCS montage was the following:
 

 1. 7x5 mm electrode centered over F3 (long side parallel to midline; connector at posterior end)
 2. 7x5 mm electrode centered on right forehead (approximately Fp2) (short side parallel to midline, connector at lateral end)
 

 Anodal or cathodal tDCS is defined according to the electrode over F3, i.e. ""anodal tDCS"" means this was the anode (with the cathode over Fp2); ""cathodal tDCS"" means this was the cathode (with the anode over Fp2).
 

 tDCS artifacts
 ========
 

 tDCS was applied at an intensity of 1 mA for 20m (with a 1-minute ramp-up period before, and a 1-minute ramp-down period after). This often caused channels close to the tDCS electrodes to drift outside of the amplifier range, or become very noisy otherwise. This occurred mostly in the block during tDCS obviously, but sometimes channels take a while to ""recover"", so they can still be affected during the post-block.
 

 The signal on channels further away from the electrodes normally looks fine (one exception is at the exact moment the ramp-down ends, which causes a huge artifact across all channels). However, though most of the artifacts should be gone after DC correction, this does not mean that the channels are completely artifact-free. Compared to the EEG signal, the tDCS artifact is massive, and can cause further problems by nonlinearly interacting with other physiological signals, such as heart-rate and respiration. However, assuming these artifacts do not vary systematically across conditions and time, comparing event-related responses across conditions should be less of a problem. See [1,2] for more information.
 

 Triggers
 --------
 

 The event codes (see \_events.json and each session's \_events.tsv file) are all in the 61000 range. This differs from the original values sent out (see below for a list of those). The transformation happens when reading the .*bdf sourcedata files, e.g. with both EEGLAB and fieldtrip. Because the data were converted to the BIDS-compliant BrainVision format using fieldtrip, the trigger codes are now all >61000.
 

 The problem comes from that the event codes were sent in 8-bit, but are read in 16 bits. Since bits 13-16 on the trigger channel are always open, the event codes are shifted by 1111000000000000 in binary, which is 61440.
 

 The original trigger codes were:
 

 ""10"": ""onset of pre-stream fixation cross (duration: 1500 ms)"",
 ""23"": ""onset of letter stream; lag 3 trial (duration: 1375 ms)"",
 ""28"": ""onset of letter stream; lag 8 trial (duration: 1375 ms)"",
 ""31"": ""onset of T1 (first target, in red, duration: 91.66 ms)"",
 ""32"": ""onset of T2 (second target, in green duration: 91.66 ms)"",
 ""40"": ""onset of post-stream fixation cross (duration: 1000 ms)"",
 ""50"": ""onset of T1 question ('Which letter was red?')"",
 ""60"": ""onset of T2 question ('Which letter was green?'); T1 question answered incorrectly"",
 ""61"": ""onset of T2 question ('Which letter was green?'); T1 question answered correctly"",
 ""70"": ""onset of post-response fixation cross (duration: 250 ms); T2 question answered incorrectly"",
 ""71"": ""onset of post-response fixation cross (duration: 250 ms); T2 question answered correctly"",
 ""254"": ""trigger to start EEG recording (occurs at start of block after a break)""
 

 When reading in the data, 61440 will likely be added to each of them.
 

 Exceptions
 ---------
 

 Was unable to create BrainVision format data for sub-07/cathodal-post; fieldtrip crashed on the .bdf file in sourcedata:
 

 > Error in read\_biosemi\_bdf>readLowLevel (line 274)
 >  buf = read\_24bit(filename, offset, numwords);
 

 The .bdf seems to be read fine by EEGLAB, so still included it in case this can be solved.",1,1,0
ds001849,2019-04-11 21:35:05,Jack Reeves,1.0.2,RS\_TMSEEG\_Data,2019-08-22 14:07:44,1,1,47790400000,0.048 TB,363,20,0,0,2.7.0,CC0,"Michael Freedberg, Jack A. Reeves, Sara J. Hussain, Kareem A. Zaghloul, Eric M. Wassermann",,"The data can be cited under the following paper:
 

 Freedberg, M.* and Reeves, J.A.*, Hussain, S.J., Zaghloul, K.A., Wassermann, E.M. (2020). Identifying site- and stimulation-specific TMS-evoked EEG potentials using a quantitative cosine similarity metric. PLoS ONE 15(1): e0216185. doi:10.1371/journal.pone.0216185.
 

 *Equal contributions",,,10.18112/openneuro.ds001849.v1.0.2,,tmseegrest,,EEG,,1,1,0
ds001971,2019-06-06 22:03:30,Johanna Wagner,1.1.1,Audiocue walking study,2019-06-07 19:41:04,1,1,34339200000,0.034 TB,1917,20,0,0,v1.2.0,Creative commons,"Johanna Wagner, Ramon Martinez-Cancino, Scott Makeig, Arnaud Delorme, Christa Neuper, Teodoro Solis-Escalante, Gernot Mueller-Putz",,,"Future Labs Reloaded 2013 of the Faculty of Computer Science at the Graz University of Technology, Austria, OeAD (Austrian Agency for International Mobility). ===NEMAR-SEP=== Marietta Blau Grant to J.W., the BMWF (Austrian Federal Ministry of Science and Research). ===NEMAR-SEP=== Gift to University of California San Diego by the Swartz Foundation (Old Field, NY).",http://www.jneurosci.org/content/36/7/2212.short,10.18112/openneuro.ds001971.v1.1.1,,AudioCueWalkingStudy,,EEG,"This mobile brain body imaging (MoBI) gait adaptation experiment contains 18 subjects. 
 Participants were walking on a treadmill at a constant speed and were required to step in 
 time to an auditory tone sequence and adapt their step length and rate to occasional
 shifts in tempo of the pacing stimulus (i.e., following shifts to a faster or slower tempo).
 The scientific article (see Reference) contains all methodological details
 

 - Johanna Wagner (June 6, 2019)",1,1,0
ds003633,2021-04-24 3:09:05,Xingyu Liu,1.0.4,ForrestGump-MEG,2021-05-16 7:35:57,1,1,78926600000,73.5 GB,2298,11,19,25,1.4.0,CC0,"Xingyu Liu, Yuxuan Dai, Hailun Xie, Zonglei Zhen",We would like to thank the team of the *studyforrest* project for their great contribution in acquiring and sharing the *studyforrest* dataset (https://openneuro.org/datasets/ds000113/versions/1.3.0).,,National Key R&D Program of China (Grant No. 2019YFA0709503) ===NEMAR-SEP=== National Natural Science Foundation of China (Grant No. 31771251),,doi:10.18112/openneuro.ds003633.v1.0.4,"Institutional Review Board of the Faculty of Psychology, Beijing Normal University","movie, noise",,"MEG, MRI","**ForrestGump-MEG: A audio-visual movie watching MEG dataset**
 

 For details please refer to our paper on https://www.biorxiv.org/content/10.1101/2021.06.04.446837v1.
 

 This dataset contains MEG data recorded from 11 subjects while watching the 2h long Chinese-dubbed audio-visual movie 'Forrest Gump'. The data were acquired with a 275-channel CTF MEG. Auxiliary data (T1w) as well as derivation data such as preprocessed data and MEG-MRI co-registration are also included. 
 

 Please noted that For sub-01, the MEG machine collapsed at the 500th second in run 7 (segment 7), a supplementary run (run 8, segment 7, 200 s) was recorded subsequently after the collapse. So there are 9 runs in total for sub-01 with run 7&8 for segment 7 and run 9 for segment 8. 
 

 

 **Pre-process procedure description**
 

 The T1w images stored as NIFTI files were minimally-preprocessed using the anatomical preprocessing pipeline from fMRIPrep with default settings. 
 

 MEG data were pre-processed using MNE following a three-step procedure: 1. bad channels were detected and removed. 2. a high-pass filter of 1 Hz was applied to remove possible slow drifts from the continuous MEG data. 3. artifacts removal was performed with ICA.
 

 

 **Stimulus material**
 

 The audio-visual stimulus materials were from the Chinese-dubbed 'Forrest Gump' DVD released in 2013 (ISBN: 978-7-7991-3934-0), which cannot be publicly released due to copyright restrictions. The stimulus materials are available upon reasonable request and on condition of a research-only data use agreement (correspondence with Xingyu Liu, liuxingyu987@foxmail.com).
 

 

 **Dataset content overview**
 

 the data were organized following the MEG-BIDS using MNE-BIDS toolbox.
 

 *the pre-processed MEG data*
 

 The preprocessed MEG recordings including the preprocessed MEG data, the event files, the ICA decomposition and label files and the MEG-MRI coordinate transformation file are hosted here. 
 

  |---./derivatives/preproc_meg-mne_mri-fmriprep/sub-xx/ses-movie/meg/
   |---sub-xx_ses-movie_coordsystem.json
   |---sub-xx_ses-movie_task-movie_run-xx_channels.tsv
   |---sub-xx_ses-movie_task-movie_run-xx_decomposition.tsv
   |---sub-xx_ses-movie_task-movie_run-xx_events.tsv
   |---sub-xx_ses-movie_task-movie_run-xx_ica.fif.gz
   |---sub-xx_ses-movie_task-movie_run-xx_meg.fif
   |---sub-xx_ses-movie_task-movie_run-xx_meg.json
   |---...
   |---sub-xx_ses-movie_task-movie_trans.fif
 

 

 *the pre-processed MRI data*
 

 The preprocessed MRI volume, reconstructed surface, and other associations including transformation files are hosted here
 

  |---./derivatives/preproc_meg-mne_mri-fmriprep/sub-xx/ses-movie/anat/
   |---sub-xx_ses-movie_desc-preproc_T1w.nii.gz
   |---sub-xx_ses-movie_hemi-L_inflated.surf.gii
   |---sub-xx_ses-movie_hemi-L_midthickness.surf.gii
   |---sub-xx_ses-movie_hemi-L_pial.surf.gii
   |---sub-xx_ses-movie_hemi-L_smoothwm.surf.gii
   |---sub-xx_ses-movie_hemi-R_inflated.surf.gii
   |---sub-xx_ses-movie_hemi-R_midthickness.surf.gii
   |---sub-xx_ses-movie_hemi-R_pial.surf.gii
   |---sub-xx_ses-movie_hemi-R_smoothwm.surf.gii
   |---sub-xx_ses-movie_space-MNI152NLin2009cAsym_desc-preproc_T1w.nii.gz
   |---sub-xx_ses-movie_space-MNI152NLin6Asym_desc-preproc_T1w.nii.gz
   |---...
 

 the FreeSurfer surface data, the high-resolution head surface and the MRI-fiducials are provided here
 

  |---./derivatives/preproc_meg-mne_mri-fmriprep/sourcedata/
   |---freesurfer 
    |---sub-xx
    |---...
 

 

 *the raw data*
 

  |---./sub-xx/ses-movie/ 
   |---meg/
   | |---sub-xx_ses-movie_coordsystem.json
   | |---sub-xx_ses-movie_task-movie_run-xx_channels.tsv
   | |---sub-xx_ses-movie_task-movie_run-xx_events.tsv
   | |---sub-xx_ses-movie_task-movie_run-xx_meg.ds
   | |---sub-xx_ses-movie_task-movie_run-xx_meg.json
   | |---...  
   |---anat/
    |---sub-xx_ses-movie_T1w.json
    |---sub-xx_ses-movie_T1w.nii.gz",1,0,0
ds002034,2019-07-13 8:51:21,Michael Pereira,1.0.3,Real-time EEG feedback on alpha power lateralization leads to behavioral improvements in a covert attention task,2019-08-24 11:03:34,1,3,10842700000,0.011 TB,882,14,21,27,1.1.1,CC0,"Christoph Schneider, Michael Pereira, Luca Tonin, Jose del R. Millan",,"If you reference this dataset in your publications, please acknowledge its authors and cite the paper specified under 'ReferenceAndLinks'. Also, please include the following message: 'This data was obtained from the Open Neuro database.'",No external funding sources,"Schneider, C., Pereira, M., Tonin, L. et al. Brain Topogr (2019). https://doi.org/10.1007/s10548-019-00725-9",doi:10.18112/openneuro.ds002034.v1.0.3,,"task-offline, task-offlinecatch, task-onlinesham, task-onlinereal",,EEG,"This dataset contains the EEG recordings used in the paper: ""Real-time EEG Feedback on Alpha Power Lateralization Leads to Behavioral Improvements in a Covert Attention Task"" (Schneider, C., Pereira, M., Tonin, L. et al. Brain Topogr (2019). https://doi.org/10.1007/s10548-019-00725-9)
 

 Participants:
 Fourteen healthy subjects (seven female, seven male), age 23±1.52 years, with normal or corrected to normal vision took part in the study. All gave informed written consent and received course credits for their participation. The study was covered by the ethical protocol No PB\_2017-00295 of the ethics commissions of the cantons of Vaud and Geneva, Switzerland and complied with the standards of the Declaration of Helsinki.
 

 

 Experimental paradigm: 
 Each trial started with the presentation of a gray central fixation point at 0.5° visual angle and subjects were instructed to neither move nor blink until the trial was over. After 1 to 2 s (random duration), a cue—corresponding to the task to perform—was presented for 100 ms: half a circle (line width 0.1°, radius 2°) to the left or to the right indicated the side to attend to, a full circle around the fixation point indicated a central fixation trial (no covert attention shift). This was followed by the sustained attention phase—1 to 5 s—where subjects were instructed to covertly attend to the target placeholder indicated by the cue. Target placeholders were circles with an inscribed cross (line width 0.2°, radius 2°, centered at 12° extremity from the center point and at a downward angle of 30° from the horizontal midline; Fig. 1b). The target placeholder at the non-cued side is also called a distractor. To be consistent with the real-time feedback runs where color represented the decoded ?-LI (see below), the color of both target placeholders varied randomly between isoluminant red and green (L*a*b color space (CIELAB), L and b constant, a varied between – 80 and 80). A trial ended when the inscribed cross disappeared in the to-attend target (valid cue) or on the opposite side (invalid cue). Subjects were instructed to react to the trial end as fast as possible with a button press using the right index finger. The inter-trial interval was 2–3 s long. In online runs, the min. and max. duration of the sustained attention period was between 2 and 20 s and inter-trial intervals ranged from 4-5 seconds.
 

 

 Recordings:
 The EEG was recorded with an active 64 channel HIamp EEG amplifier (g.tec, Schiedlberg, Austria) at 512Hz and referenced to the linked ears. The electrodes were positioned according to the international 10-10 system with the ground electrode on FCz. For more details please refer to the paper.
 The study involved recordings (sessions) on three different days. One recording session lasted approximately 90 min, including the technical setup. Time on task was less than 40 min per session, with breaks after each run (every 9–10 min). On the first recording day subjects practiced for one run to familiarize with the task. Then they performed four offline runs (no feedback, 80 trials each) to calibrate their individual decoder for the real-time feedback (Fig. 1a, ""Offline Paradigm""). On day two and three the ?-power lateralization index (?-LI) feedback was administered in a single-blinded crossover design. Subjects were randomly assigned to either receive real or sham ?-LI feedback on day two and then switched the other feedback group on day three (""Real-time feedback paradigm"" and ""Sham feedback""). Therefore, both days had the same run structure: they started and ended with one offline run (80 trials each, including catch trials), while the real-time feedback was given during two middle runs (40 long trials each).",1,1,0
ds002094,2019-08-02 12:56:34,Sara Hussain,1.0.0,Single-pulse open-loop TMS-EEG dataset,2019-08-02 15:03:21,1,1,42339800000,0.042 TB,282,20,0,0,1.3.0,CC0,,,,,,,,"rest, tmseeg1, tmseeg2",,"channels, eeg, events",,1,1,0
ds002158,2019-09-04 15:28:46,Michael Pereira,1.0.2,Simultaneous eeg-fmri for a speeded discrimination task with confidence,2020-03-19 13:15:51,1,1,82185100000,0.082 TB,949,20,20,32,1.1.1,CC0,"Michael Pereira, Nathan Faivre, Inaki Iturrate, Marco Wirthlin, Luana Serafini, Stephanie Martin, Arnaud Desvachez, Olaf Blanke, Dimitri Van de Ville, Jose del R. Millan","ACKNOWLEDGMENTS. O.B. is supported by the Bertarelli Foundation, the Swiss National Science Foundation, and the European Science Foundation. D.V.D.V. is supported by the Bertarelli Foundation and the Swiss National Science Foundation. N.F. has received funding from the European Research Council under the European Union’s Horizon 2020 research and innova- tion programme grant 803122. We thank Roberto Martuzzi, Loan Mattera, Gwénaël Birot, Gisong Kim, and Léa Vidal for their help during data acqui- sition and Elisa Filevich, Roy Salomon, and two anonymous reviewers for constructive comments on the manuscript.","If you reference this dataset in your publications, please acknowledge its authors by citing: 
 Pereira, M., Faivre, N., Iturrate, I., Wirthlin, M., Serafini, L., Martin, S., Desvachez, A., Blanke, O., Van De Ville, D., Millan, JdR. (2018). Disentangling the origins of confidence in speeded perceptual judgments through multimodal imaging (2020). Proceedings of the National Academy of Science, 117 (15) pp 8382-8390
 https://doi.org/10.1073/pnas.1918335117
 

 Also, please include the following message: 'This data was obtained from the Open Neuro database.'",Bertarelli Foundation ===NEMAR-SEP=== Swiss National Science Foundation,"Pereira, M., Faivre, N., Iturrate, I., Wirthlin, M., Serafini, L., Martin, S., Desvachez, A., Blanke, O., Van De Ville, D., Millan, JdR. (2018). Disentangling the origins of confidence in speeded perceptual judgments through multimodal imaging (2020). Proceedings of the National Academy of Science, 117 (15) pp 8382-8390 https://doi.org/10.1073/pnas.1918335117",10.18112/openneuro.ds002158.v1.0.2,,"main, rest",,"MRI, EEG","This dataset contains the data in  
 

 Pereira, M., Faivre, N., Iturrate, I., Wirthlin, M., Serafini, L., Martin, S., Desvachez, A., Blanke, O., Van De Ville, D., Millan, JdR. (2020). Disentangling the origins of confidence in speeded perceptual judgments through multimodal imaging. Proceedings of the National Academy of Science, 117 (15) pp. 8382-8390
 https://doi.org/10.1073/pnas.1918335117
 

 Preprint: https://www.biorxiv.org/content/10.1101/496877v1
 

 ABSTRACT
 The human capacity to compute the likelihood that a decision is correct—known as metacognition—has proven difficult to study in isolation as it usually cooccurs with decision making. Here, we isolated postdecisional from decisional contributions to metacognition by analyzing neural correlates of confidence with multimodal imaging. Healthy volunteers reported their confidence in the accuracy of decisions they made or decisions they observed. We found better metacognitive performance for committed vs. observed decisions, indicating that committing to a decision may improve confidence. Relying on concurrent electroencephalography and hemodynamic recordings, we found a common correlate of confidence following committed and observed decisions in the inferior frontal gyrus and a dissociation in the anterior prefrontal cortex and anterior insula. We discuss these results in light of decisional and postdecisional accounts of confidence and propose a computational model of confidence in which metacognitive performance naturally improves when evidence accumulation is constrained upon committing a decision.
 

 preregistration: https://osf.io/a5qmv/
 

 The dataset contains raw fMRI scans, raw EEG in BrainVision format as well as anatomical scans (T1) and field mapping. We also included preprocessed EEG and fMRI data in derivatives/eegprep and derivatives/fmriprep.
 

 EEG PREPROCESSING
 MR-gradient artifacts were removed using sliding window average template subtraction. TP10 electrode on the right mastoid was used to detect heartbeats for ballistocardiogram artifact (BCG) removal using a semi-automatic procedure in BrainVision Analyzer 2. Data were then filtered using a Butterworth, 4th order zero-phase (two-pass) bandpass filter between 1 and 10 Hz, epoched [-0.2, 0.6 s] around the response onset (i.e. the button press in the active condition or the appearance of the virtual hand for in observation condition), re-referenced to a common average, and input to independent component analysis (ICA) to remove residual BCG and ocular artifacts. In order to ensure numerical stability when estimating the independent components, we retained 99\% of the variance from the electrode space, leading to an average of 19 (SD = 6) components estimated for each participant and condition. Independent components (ICs) were then fitted with a dipolar source localization method (66). ICs whose dipole lied outside the brain, or resembled muscular or ocular artifacts were eliminated. A total of 8 (SD = 3) components were finally kept. All preprocessing steps were performed using EEGLAB and in-house scripts under Matlab (The MathWorks, Inc., Natick, Massachusetts, United States).
  
 FMRI PREPROCESSING
 We modeled the BOLD signal using a general linear model (GLM) with two separate regressors (stick functions at stimulus onset) for the active and observation condition as well as their spatial and temporal derivatives. We then parametrically modulated the regressors with three behavioral variables: the confidence ratings, the response times, and the numerosity difference between the two arrays of dots (i.e., perceptual evidence). Empirical cross-correlation between regressors confirmed limited collinearity for the active (resp. observation) condition (max(abs(R)) = 0.26 ± 0.02 resp., max(abs(R)) = 0.25 ± 0.02). Bad trials as defined in the behavioral analysis section were modeled by two separate regressors (one for active and one for observation) and their spatial and temporal derivatives. We added six realignments parameters as regressors of no interest. All second-level (group-level) results are reported at a significance-level of p < 0.05 using cluster-extent family-wise error (FWE) correction with a voxel-height threshold of p < 0.001. We used the anatomical automatic labelling (AAL) atlas for brain parcellation (Tzourio-Mazoyer et al., 2002).",1,1,0
ds002181,2019-09-15 18:43:27,Wanze Xie,1.0.0,CRYPTO and PROVIDE EEG Baseline Data,2019-09-16 0:33:06,0,1,158222000,0 TB,1134,226,6,36,1.2,CC0,"Wanze Xie, Sarah Jensen, Mark Wade, Swapna Kumar, Alissa Westerlund, Shahria Kakon, Rashidul Haque, William A Petri, Charles A Nelson",,,,,mockDOI,,Baseline,,"channels, eeg, events","These are the EEG baseline data used in the study on the association between stunting and EEG brain functional connectivity in Bangladeshi children (https://doi.org/10.1101/447722).
 

 Data with an ID < 2000 were collected for a cohort of 36-month-old toddlers, and those with an ID > 2000 were collected for a cohort of 6-month-old infants. The children were watching screen savers for 2 minutes.",1,1,0
ds002218,2019-10-03 3:55:56,Daniel C Comstock,2.0.0,Auditory and Visual Rhythm Omission EEG,2019-10-03 5:24:27,1,1,2089140000,0.002 TB,133,18,0,0,1.1.1,CC0,"Daniel C Comstock, Ramesh Balasubramaniam",,"If you reference this dataset in your publications, please acknowledge its authors and cite the paper specified under 'ReferenceAndLinks'. Also, please include the following message: 'This data was obtained from the Open Neuro database.'",,,mockDOI,,Experiment,,EEG,"This EEG dataset was recorded as part of a study of the predictive mechanisms of rhythm perception by using an omission paradigm to separate out predictive neural activity from sensory evoked neural activity. The study had 18 participants listen to auditory rhythms and watch visual flashing rhythms separately. The stimulus trains of both kinds of rhythms contained occasional omissions. Code for preprocessing, time/freq computation, frequency band extraction and statistics is provided. Cluster formation was performed using the EEGLAB Study function.",1,1,0
ds002336,2019-12-02 15:14:38,Claire Cury,2.0.2,"An open access, multi-modal human neuroimaging dataset for data integration: simultaneous EEG and MRI acquisition during a motor imagery neurofeedback task: XP1",2019-12-04 14:48:34,1,1,18046600000,0.018 TB,325,10,19,39,1.2.0,CC0,"Giulia Lioi, Claire Cury, Lorraine Perronnet, Marsel Mano, Elise Bannier, Anatole Lecuyer, Christian Barillot","MRI data acquisition was supported by the Neurinfo MRI research facility at the University Hospital of Rennes, which is granted by the European Union and the French Gouvernment",,This work was supported by the French Natinal Research Agency under the reference ANR-10-LABX-07-01,"Perronnet, Lorraine, L Anatole, Marsel Mano, Elise Bannier, Maureen Clerc, Christian Barillot, Lorraine Perronnet, et al. 2017. “Unimodal Versus Bimodal EEG-FMRI Neurofeedback of a Motor Imagery Task.” Frontiers in Human Neuroscience 11 (193). https://doi.org/10.3389/fnhum.2017.00193. ===NEMAR-SEP=== Giulia Lioi, Claire Cury, Lorraine Perronnet, Marsel Mano, Elise Bannier, Anatole Lecuyer, Christian Barillot. Simultaneous MRI-EEG during a motor imagery neurofeedback task: an open access brain imaging dataset for multi-modal data integration. bioRxiv 862375; doi: https://doi.org/10.1101/862375",10.18112/openneuro.ds002336.v2.0.2,,"fmriNF, eegfmriNF, MIpost, eegNF, motorloc, MIpre",,"EEG, MRI","————————————————————————————————
 ORIGINAL PAPERS
 ————————————————————————————————
 

 Lioi, G., Cury, C., Perronnet, L., Mano, M., Bannier, E., Lécuyer, A., & Barillot, C. (2019). Simultaneous MRI-EEG during a motor imagery neurofeedback task: an open access brain imaging dataset for multi-modal data integration Authors. BioRxiv. https://doi.org/https://doi.org/10.1101/862375
 

 Mano, Marsel, Anatole Lécuyer, Elise Bannier, Lorraine Perronnet, Saman Noorzadeh, and Christian Barillot. 2017. “How to Build a Hybrid Neurofeedback Platform Combining EEG and FMRI.” Frontiers in Neuroscience 11 (140). https://doi.org/10.3389/fnins.2017.00140
 Perronnet, Lorraine, L Anatole, Marsel Mano, Elise Bannier, Maureen Clerc, Christian Barillot, Lorraine Perronnet, et al. 2017. “Unimodal Versus Bimodal EEG-FMRI Neurofeedback of a Motor Imagery Task.” Frontiers in Human Neuroscience 11 (193). https://doi.org/10.3389/fnhum.2017.00193.
 

 This dataset named XP1 can be pull together with the dataset XP2, available here : https://openneuro.org/datasets/ds002338.
 Data acquisition methods have been described in Perronnet et al. (2017, Frontiers in Human Neuroscience).
 Simultaneous 64 channels EEG and fMRI during right-hand motor imagery and neurofeedback (NF) were acquired in this study (as well as in XP2). For this study, 10 subjects performed three types of NF runs (bimodal EEG-fMRI NF, unimodal EEG-NF and fMRI-NF). 
 

 ————————————————————————————————
 EXPERIMENTAL PARADIGM
 ———————————————————————————————— 
 Subjects were instructed to perform a kinaesthetic motor imagery of the right hand and to find their own strategy to control and bring the ball to the target.
 The experimental protocol consisted of 6 EEG-fMRI runs with a 20s block design alternating rest and task
 motor localizer run (task-motorloc) - 8 blocks X (20s rest+20 s task)
 motor imagery run without NF (task-MIpre) -5 blocks X (20s rest+20 s task)
 three NF runs with different NF conditions (task-eegNF, task-fmriNF, task-eegfmriNF) occurring in random order- 10 blocks X (20s rest+20 s task)
 motor imagery run without NF (task-MIpost) - 5 blocks X (20s rest+20 s task)
 

 

 ————————————————————————————————
 EEG DATA
 ————————————————————————————————
 EEG data was recorded using a 64-channel MR compatible solution from Brain Products (Brain Products GmbH, Gilching, Germany).
 

 RAW EEG DATA
 

 EEG was sampled at 5kHz with FCz as the reference electrode and AFz as the ground electrode, and a resolution of 0.5 microV. Following the BIDs arborescence, raw eeg data for each task can be found for each subject in
 

 XP1/sub-xp1*/eeg
 

 in Brain Vision Recorder format (File Version 1.0). Each raw EEG recording includes three files: the data file (*.eeg), the header file (*.vhdr) and the marker file (*.vmrk). 
 The header file contains information about acquisition parameters and amplifier setup. For each electrode, the impedance at the beginning of the recording is also specified. For all subjects, channel 32 is the ECG channel. The 63 other channels are EEG channels.
 

 The marker file contains the list of markers assigned to the EEG recordings and their properties (marker type, marker ID and position in data points). Three type of markers are relevant for the EEG processing:
 R128 (Response): is the fMRI volume marker to correct for the gradient artifact
 S 99 (Stimulus): is the protocol marker indicating the start of the Rest block
 S 2 (Stimulus): is the protocol marker indicating the start of the Task (Motor Execution Motor Imagery or Neurofeedback)  
 Warning : in few EEG data, the first S99 marker might be missing, but can be easily “added” 20 s before the first S 2.  
 

 PREPROCESSED EEG DATA
 

 Following the BIDs arborescence, processed eeg data for each task and subject in the pre-processed data folder :
 

 XP1/derivatives/sub-xp1*/eeg\_pp/*eeg\_pp.*
 

 and following the Brain Analyzer format. Each processed EEG recording includes three files: the data file (*.dat), the header file (*.vhdr) and the marker file (*.vmrk), containing information similar to those described for raw data. In the header file of preprocessed data channels location are also specified. In the marker file the location in data points of the identified heart pulse (R marker) are specified as well. 
 

 EEG data were pre-processed using BrainVision Analyzer II Software, with the following steps:
 Automatic gradient artifact correction using the artifact template subtraction method (Sliding average calculation with 21 intervals for sliding average and all channels enabled for correction.
 Downsampling with factor: 25 (200 Hz)
 Low Pass FIR Filter:Cut-off Frequency: 50 Hz.
 Ballistocardiogram (pulse) artifact correction using a semiautomatic procedure (Pulse Template searched between 40 s and 240 s in the ECG channel with the following parameters:Coherence Trigger = 0.5, Minimal Amplitude = 0.5, Maximal Amplitude = 1.3. The identified pulses were marked with R.
 Segmentation relative to the first block marker (S 99) for all the length of the training protocol (las S 2 + 20 s).
 

 EEG NF SCORES
 

 Neurofeedback scores can be found in the .mat structures in
 

 XP1/derivatives/sub-xp1*/NF\_eeg/d\_sub*NFeeg\_scores.mat
 

 Structures names NF\_eeg are composed of the following subfields:
 

 NF\_eeg
 -> .nf\_laterality (NF score computed as for real-time calculation - equation (1))  
 -> .filteegpow\_left (Bandpower of the filtered eeg signal in C1)
 -> .filteegpow\_right (Bandpower of the filtered eeg signal in C2)
 -> .nf (vector of NF scores -4 per s- computed as in eq 3) for comparison with XP2
 -> .smoothed
 -> .eegdata (64 X 200 X 400 matrix, with the pre-processed EEG signals according to the steps described above)
 -> .method
 

 Where the subfield method contains information about the laplacian filtered used and the frequency band of interest.
 

 

 ————————————————————————————————
 BOLD fMRI DATA
 ————————————————————————————————
 All DICOM files were converted to Nifti-1 and then in BIDs format (version 2.1.4) using the software dcm2niix (version v1.0.20190720 GVV7.4.0)
 

 fMRI acquisitions were performed using echo- planar imaging (EPI) and covering the entire brain with the following parameters 
 

 3T Siemens Verio
 EPI sequence
 TR=2 s
 TE=23 ms
 Resolution 2x2x4 mm3
 FOV = 210×210mm2
 N of slices: 32
 No slice gap
 

 

 As specified in the relative task event files in XP1\ *events.tsv files onset, the scanner began the EPI pulse sequence two seconds prior to the start of the protocol (first rest block), so the the first two TRs should be discarded. 
 The useful TRs for the runs are therefore
 

 task-motorloc: 320 s (2 to 322)
 task-MIpre and task-MIpost: 200 s (2 to 202)
 task-eegNF, task-fmriNF, task-eegfmriNF: 400 s (2 to 402)
 

 In task events files for the different tasks, each column represents:
 

 - 'onset': onset time (sec) of an event
 - 'duration': duration (sec) of the event
 - 'trial\_type': trial (block) type: rest or task (Rest, Task-ME, Task-MI, Task-NF)
 - ''stim\_file’: image presented in a stimulus block: during Rest, Motor Imagery (Task-MI) or Motor execution (Task-ME) instructions were presented. On the other hand, during Neurofeedback blocks (Task-NF) the image presented was a ball moving in a square that the subject could control self-regulating his EEG and/or fMRI brain activity.
 

 Following the BIDs arborescence, the functional data and relative metadata are found for each subject in the following directory
 

 XP1/sub-xp1*/func
 

 BOLD-NF SCORES
 

 For each subject and NF session, a matlab structure with BOLD-NF features can be found in 
 

 XP1/derivatives/sub-xp1*/NF\_bold/
 

 For each subject and NF session, a Matlab structure with BOLD-NF features can be found in
  
 XP1/derivatives/sub-xp1*/NF\_bold/
  
 In view of BOLD-NF scores computation, fMRI data were preprocessed using SPM8 and with the following steps: slice-time correction, spatial realignment and coregistration with the anatomical scan, spatial smoothing with a 6 mm Gaussian kernel and normalization to the Montreal Neurological Institute (MNI) template.
 For each session, a first level general linear model analysis was then performed. The resulting activation maps (voxel-wise Family-Wise error corrected at p < 0.05) were used to define two ROIs (9x9x3 voxels) around the maximum of activation in the left and right motor cortex. The BOLD-NF scores (fMRI laterality index) were calculated as the difference between percentage signal change in the left and right motor ROIs as for the online NF calculation. A smoothed and normalized version of the NF scores over the precedent three volumes was also computed. To allow for comparison and aggregation of the two datasets XP1 and XP2 we also computed NF scores considering the left motor cortex and a background as for online NF calculation in XP2.
  
 In the NF\_bold folder, the Matlab files sub-xp1*\_task-*\_NFbold\_scores.mat have therefore the following structure :
  
 NF\_bold
 -> .nf\_laterality (calculated as for online NF calculation)
 -> .smoothnf\_laterality
 -> .normnf\_laterality
 -> .nf (calculated as for online NF calculation in XP2)
 -> .roimean\_left (averaged BOLD signal in the left motor ROI)
 -> .roimean\_right (averaged BOLD signal in the right motor ROI)
 -> .bgmean (averaged BOLD signal in the background slice)
 -> .method
  
 Where the subfield "".method"" contains information about the ROI size (.roisize), the background mask (.bgmask) and ROI masks (.roimask\_left,.roimask\_right ). More details about signal processing and NF calculation can be found in Perronnet et al. 2017 and Perronnet et al. 2018.
 

 

 ————————————————————————————————
 ANATOMICAL MRI DATA
 ————————————————————————————————
 As a structural reference for the fMRI analysis, a high resolution 3D T1 MPRAGE sequence was acquired with the following parameters
 

 3T Siemens Verio
 3D T1 MPRAGE
 TR=1.9 s
 TE=22.6 ms
 Resolution 1x1x1 mm3
 FOV = 256×256 mm2
 N of slices: 176
 

 Defacing of MPRAGE T1 images was performed by the submitter using pydeface (https://github.com/poldracklab/pydeface) 
 

 Following the BIDs arborescence, the functional data and relative metadata are found for each subject in the following directory
 

 XP1/sub-xp1*/anat",1,1,0
ds002338,2019-12-03 10:06:04,Claire Cury,2.0.2,"An open access, multi-modal human neuroimaging dataset for data integration: simultaneous EEG and MRI acquisition during a motor imagery neurofeedback task: XP2",2019-12-04 14:40:17,1,1,26024200000,0.026 TB,484,17,18,66,1.2.0,CC0,"Giulia Lioi, Claire Cury, Lorraine Perronnet, Marsel Mano, Elise Bannier, Anatole Lecuyer, Christian Barillot",,,This work was supported by the French Natinal Research Agency under the reference ANR-10-LABX-07-01,"Perronnet, Lorraine, L Anatole, Marsel Mano, Maureen Clerc, Fabien Lotte, and Christian Barillot. 2018. “Learning 2-in-1 : Towards Integrated EEG-FMRI-Neurofeedback.” BioRxiv, no. 397729. https://doi.org/10.1101/397729.",doi:10.18112/openneuro.ds002338.v2.0.2,,"1dNF, MIpost, MIpre, 2dNF, 1dNF_run-01, 2dNF_run-02",,"EEG, MRI","————————————————————————————————
 ORIGINAL PAPERS
 ————————————————————————————————
 Lioi, G., Cury, C., Perronnet, L., Mano, M., Bannier, E., Lécuyer, A., & Barillot, C. (2019). Simultaneous MRI-EEG during a motor imagery neurofeedback task: an open access brain imaging dataset for multi-modal data integration Authors. Accepted for publication in Scientific Data. https://doi.org/https://doi.org/10.1101/862375
 Mano, Marsel, Anatole Lécuyer, Elise Bannier, Lorraine Perronnet, Saman Noorzadeh, and Christian Barillot. 2017. “How to Build a Hybrid Neurofeedback Platform Combining EEG and FMRI.” Frontiers in Neuroscience 11 (140). https://doi.org/10.3389/fnins.2017.00140
 Lorraine Perronnet, Anatole Lecuyer, Marsel Mano, Mathis Fleury, Giulia Lioi, Claire Cury, Maureen Clerc, Fabien Lotte, and Christian Barillot. 2018. “Learning 2-in-1 : Towards Integrated EEG-FMRI-Neurofeedback.” BioRxiv, no. 397729. https://doi.org/10.1101/397729.
 

 ————————————————————————————————
 OVERVIEW
 ————————————————————————————————
 This dataset XP2 can be pull together with the dataset XP1, available here : https://openneuro.org/datasets/ds002336.
 Data acquisition methods have been described in Perronnet et al. (2017, Frontiers in Human Neuroscience).
 Simultaneous 64 channel EEG and fMRI during right-hand motor imagery and neurofeedback (NF) were acquired in this study (as well as in XP1). This study involved 16 subjects randomly assigned to two groups: in a first group they performed bimodal EEG-fMRI NF with a bi-dimensional feedback metaphor, in the second group the same task was executed with a mono-dimensional feedback.
 

 ————————————————————————————————
 EXPERIMENTAL PARADIGM
 ————————————————————————————————
 

 The experimental protocol consisted of 5 EEG-fMRI runs with a 20s block design alternating rest and task. 1 block = 20s rest + 20s task. Task description :
 \_task-MIpre : motor imagery run without NF. 8 blocks.
 \_task-1dNF or \_task-2dNF : bimodal neurofeedback, with either a mono-dimensional neurofeedback display (mean of EEG NF and fMRI NF scores), either a bi-dimensional display (one modality per dimension). The list of subjects with 1d or 2d is given above.
 Each subjects had 3 runs. 8 blocks per run.
 \_task-MIpost : motor imagery run without NF. 8 blocks.
 Subjects with mono-dimensional feedback display :
 xp201 : 1D
 xp202 : 1D
 xp203 : 1D
 xp206 : 1D
 xp211 : 1D
 xp218 : 1D
 xp219 : 1D
 xp220 : 1D
 xp222 : 1D
 

 Subjects with bi-dimensional feedback display :
 xp204 : 2D
 xp205 : 2D
 xp207 : 2D
 xp210: 2D
 xp213 : 2D
 xp216 : 2D
 xp217 : 2D
 xp221 : 2D
 

 ————————————————————————————————
 EEG DATA
 ————————————————————————————————
 EEG data was recorded using a 64-channel MR compatible solution from Brain Products (Brain Products GmbH, Gilching, Germany).
 

 RAW EEG DATA
 

 EEG was sampled at 5kHz with FCz as the reference electrode and AFz as the ground electrode, and a resolution of 0.5 microV. Following the BIDs arborescence, raw eeg data for each task can be found for each subject in
 

 XP2/sub-xp2*/eeg
 

 in Brain Vision Recorder format (File Version 1.0). Each raw EEG recording includes three files: the data file (*.eeg), the header file (*.vhdr) and the marker file (*.vmrk). 
 The header file contains information about acquisition parameters and amplifier setup. For each electrode, the impedance at the beginning of the recording is also specified. For all subjects, channel 32 is the ECG channel. The 63 other channels are EEG channels.
 

 The marker file contains the list of markers assigned to the EEG recordings and their properties (marker type, marker ID and position in data points). Three type of markers are relevant for the EEG processing:
 R128 (Response): is the fMRI volume marker to correct for the gradient artifact
 S 99 (Stimulus): is the protocol marker indicating the start of the Rest block
 S 2 (Stimulus): is the protocol marker indicating the start of the Task (Motor Execution Motor Imagery or Neurofeedback)  
 Warning : in few EEG data, the first S99 marker might be missing, but can be easily “added” 20 s before the first S 2.  
 

 PREPROCESSED EEG DATA
 

 Following the BIDs arborescence, processed eeg data for each task can be found for each subject in
 

 XP2/derivatives/sub-xp2*/eeg\_pp/*eeg\_pp.*
 

 and following the Brain Analyzer format. Each processed EEG recording includes three files: the data file (*.dat), the header file (*.vhdr) and the marker file (*.vmrk), containing information similar to those described for raw data. In the header file of preprocessed data channels location are also specified. In the marker file the location in data points of the identified heart pulse (R marker) are specified as well. 
 

 EEG data were pre-processed using BrainVision Analyzer II Software, with the following steps:
 Automatic gradient artifact correction using the artifact template subtraction method (Sliding average calculation with 21 intervals for sliding average and all channels enabled for correction.
 Downsampling with factor: 25 (200 Hz)
 Low Pass FIR Filter:Cut-off Frequency: 50 Hz.
 Ballistocardiogram (pulse) artifact correction using a semiautomatic procedure (Pulse Template searched between 40 s and 240 s in the ECG channel with the following parameters:Coherence Trigger = 0.5, Minimal Amplitude = 0.5, Maximal Amplitude = 1.3). A Pulse Artifact marker R was associated to each identified pulse.
 Segmentation relative to the first block marker (S 99) for all the length of the training protocol (las S 2 + 20 s).
 

 EEG-NF SCORES
 

 Neurofeedback scores can be found in the .mat structures in
 

 XP2/derivatives/sub-xp2*/NF\_eeg/d\_sub*NFeeg\_scores.mat
 

 Structures names NF\_eeg are composed by the following subfields:
 ID : Subject ID, for example sub-xp201
 lapC3\_ERD : a 1x1280 vector of neurofeedback scores. 4 scores per secondes, for the whole session.
 eeg : a 64x80200 matrix, with the pre-processed EEG signals with the step described above, filtered between 8 and 30 Hz.
 lapC3\_bandpower\_8Hz\_30Hz : 1x1280 vector. Bandpower of the filtered signal with a laplacian centred on C3, used to estimate the lapC3\_ERD.
 lapC3\_filter : 1x64 vector. Laplacian filter centred above C3 channel.
 ————————————————————————————————
 BOLD fMRI DATA
 ————————————————————————————————
 All DICOM files were converted to Nifti-1 and then in BIDs format (version 2.1.4) using the software dcm2niix (version v1.0.20190720 GVV7.4.0)
 

 fMRI acquisitions were performed using echo- planar imaging (EPI) and covered the superior half of the brain with the following parameters 
 3T Siemens Verio
 EPI sequence
 TR=1 s
 TE=23 ms
 Resolution 2x2x4 mm
 N of slices: 16
 No slice gap
 

 

 As specified in the relative task event files in XP2\ *events.tsv files onset, the scanner began the EPI pulse sequence two seconds prior to the start of the protocol (first rest block), so the the first two TRs should be discarded. 
 

 The useful TRs for the runs are therefore
 

 -task-MIpre and task-MIpost: 320 s (2 to 302)
 -task-1dNF and task-2dNF: 320 s (2 to 302)
 

 In task events files for the different tasks, each column represents:
 

 - 'onset': onset time (sec) of an event
 - 'duration': duration (sec) of the event
 - 'trial\_type': trial (block) type: rest or task (Rest, Task-MI, Task-NF)
 - 'stim\_file': image presented in a stimulus block. During Rest or Motor Imagery (Task-MI) instructions were presented to the subject. On the other hand, during Neurofeedback blocks (Task-NF) the image presented was a ball moving in a square for the bidimensional NF (task-2dNF) or a ball moving along a gauge for the unidimensional NF (task-1dNF) that the subject could control self-regulating his EEG and fMRI brain activity.
 

 Following the BIDs arborescence, the functional data and relative metadata are found for each subject in the following directory
 

 XP2/sub-xp2*/func
 

 BOLD-NF SCORES
 

 For each subject and NF session, a matlab structure with BOLD-NF features can be found in 
 

 XP2/derivatives/sub-xp2*/NF\_bold/
 

 In view of BOLD-NF scores computation, fMRI data were preprocessed using AutoMRI, a software based on spm8 and with the following steps: slice-time correction, spatial realignment and coregistration with the anatomical scan, spatial smoothing with a 8 mm Gaussian kernel and normalization to the Montreal Neurological Institute template 
 For each session, a first level general linear model analysis modeling was then performed. The resulting activation maps (voxel-wise Family-Wise error corrected at p < 0.05) were used to define two ROIs (9x9x3 voxels) around the maximum of activation in the ipsilesional primary motor area (M1) and supplementary motor area (SMA) respectively. 
 

 The BOLD-NF scores were calculated as the difference between percentage signal change in the two ROIs (SMA and M1) and a large deep background region (slice 3 out of 16) whose activity is not correlated with the NF task. A smoothed version of the NF scores over the precedent three volumes was also computed. 
 

 The NF\_boldi structure has the following structure
 

 NF\_bold
  -> .m1-> .nf 
  -> .smoothnf  
  -> .roimean  (averaged BOLD signal in the ROI) 
  -> .bgmean (averaged BOLD signal in the background slice)
  -> .method  
 NFscores.fmri
  -> .sma-> .nf
  -> .smoothnf  
  -> .roimean  (averaged BOLD signal in the ROI) 
  -> .bgmean (averaged BOLD signal in the background slice)
  -> .method  
 

 

 Where the subfield method contains information about the ROI size (.roisize), the background mask (.bgmask) and ROI mask (.roimask).
 

 More details about signal processing and NF calculation can be found in Perronnet et al. 2017 and Perronnet et al. 2018.
 

 

 ————————————————————————————————
 ANATOMICAL MRI DATA
 ————————————————————————————————
 As a structural reference for the fMRI analysis, a high resolution 3D T1 MPRAGE sequence was acquired with the following parameters
 

 3T Siemens Verio
 3D T1 MPRAGE
 TR=1.9 s
 TE=22.6 ms
 Resolution 1x1x1 mm3
 FOV = 256×256 mm2
 N of slices: 176
 

 Defacing of MPRAGE T1 images was performed by the submitter using pydeface (https://github.com/poldracklab/pydeface) 
 

 Following the BIDs arborescence, the anatomical data and relative metadata are found for each subject in the following directory
 

 XP2/sub-xp2*/anat",1,1,0
ds002550,2020-02-12 20:34:30,romain quentin,1.0.1,Differential brain mechanisms of selection and maintenance of information during working memory (MEG data),2020-02-18 20:21:07,0,3,179849000000,167.5 GB,10745,22,0,0,1.1.1,CC0,"Romain Quentin, Jean-Remi King, Etienne Sallard, Nathan Fishman, Ryan Thompson, Ethan Buch, Leonardo Cohen","The intramural NINDS, DIR, NIH as well as the NIMH MEG and FMRIF facilities on the NIH campus in Bethesda (MD) contributed to this research. This project received funding from the FYSSEN foundation, the Bettencourt Schueller Foundation and the Philippe Foundation. We thank Dr. R. Coppola and Dr. T. Holroyd for their help in managing the NIH MEG facility","If you reference this dataset in your publications, please acknowledge its authors. Also, please include the following message: 'This data was obtained from the Open Neuro database.'",NINDS/HCPS ===NEMAR-SEP=== FYSSEN foundation ===NEMAR-SEP=== Bettencourt Schueller Foundation,,10.18112/openneuro.ds002550.v1.0.1,,"WorkingMemory, LocalizerControl",,"MEG, MRI","OpenNeuro curator note: This dataset was previously accessible at ds001750. The dataset was reuploaded due to privacy considerations. 
 

 # Data folder corresponding to [this manuscript](https://www.biorxiv.org/content/early/2018/03/16/283234)
 #### Differential brain mechanisms of selection and maintenance of information during working memory
 

 Note: One participant didn't sign a sharing data agreement so data of 22 participants are available here (vs. 23 in the manuscript). Results and conclusion are not different with only 22 participants.
 

 Participant folder are organized as:
 - ##### 'ses-mri/anat':
 contains T1 MRI of the participant
 - ##### ses-01:
 contains MEG data in BIDS format, behavioral data and HPI position in surface RAS MRI coordinates for session 1
 - ##### ses-02:
 contains MEG data in BIDS format, behavioral data and HPI position in surface RAS MRI coordinates for session 2
 

 ##### Description of non-MEG files:
 - ##### behavioral task scripts:
 Matlab (psychtoolbox 3) script for Working Memory (WorkMem) and one-back control task (LocaCue)
 - ##### hpi_mri_surf.txt:
 Contains the X, Y Z coordinates of the nasion, left and right HPI (head position indicator) in surface MRI coordinates. Names of the electrode are NEC (nasion), LEC (left) and REC (right). Others coordinates are for co-registration during the session (not useful here). These HPI coordinates have been acquired from the neuronavigation system brainsight (https://www.rogue-research.com/tms/brainsight-tms/)
 - ##### WorkMem+subNumber+date.csv:
 ###### Contains behavioral results:
 - NbTrial: trial number
 - FixNbTrial: trial number with good eye fixation
 - isFixed: whether the participant fixed the central dot during the trial (1:correct fixation, 0:broke fixation)
 - GaborLeft: left gabor (25 possible, 5 spatial frequency* 5 orientations)
 - GaborRight: right gabor (25 possible, 5 spatial frequency* 5 orientations)
 - Cue: cue (4 possible, 1: left dotted, 2: left solid, 3: right dotted, 4: left solid)
 - Change: whether the cued stimulus attribute is different from the corresponding probe attribute (1: different, 0: same)
 - sfLeft: spatial frequency of the left gabor (5 possible)
 - orientLeft: line orientation of the left gabor (5 possible)
 - phaseLeft: phase of the left gabor (5 possible)
 - sfRight: spatial frequency of the right gabor (5 possible)
 - orientRight: line orientation of the right gabor (5 possible)
 - phaseRight: phase of the right gabor (5 possible)
 - randomSF: probe spatial frequency if change=1
 - randomOrient: line orientation if change=1
 - phaseResp: phase of the probe
 - Response: response of the participant (1: different, 0: similar)
 - isCorrect: correctness of the response (1: correct, 0: uncorrect)
 - reactionTime: reaction time from probe onset to participant response
 - TrialTime: total trial duration
 - runningTime: running time
 - fixcrossTime: duration of the fixation dot presentation before stimulus onset (should be between 0.350 and 0.450 s)
 - gaborTime: duration of stimulus presentation (should be 0.1 s)
 - precueTime: duration between stimulus offset and cue onset (should be between 0.75 and 0.85 s)
 - cueTime: duration of the cue presentation (should be 0.1 s)
 - postcueTime: duration between cue offset and probe onset (should be between 1.45 and 1.55 s)
 - feedbackTime: duration of the feedback (green or red dot, should be 0.1 s)
 - triggGabor: trigger sent to MEG acquisition at the stimulus onset
 - triggCue: trigger sent to MEG acquisition at the cue onset
 - triggProbe: trigger sent to MEG acquisition at the probe onset
 - ##### locacue+subNumber+date.csv:
 - NbTrial: trial number
 - FixNbTrial: trial number with good eye fixation
 - isFixed: whether the participant fixed the central dot during the trial (1:correct fixation, 0:broke fixation)
 - same: whether 2 consecutive lines are similar (1: similar, 0: different)
 - Cue: cue (4 possible, 1: left dotted, 2: left solid, 3: right dotted, 4: left solid)
 - Side: side of the cue (1: right, 0: left)
 - Press: whether the participant press the button (1: press, 0: no press)
 - isCorrect: correctness of the response (1: correct, 0: uncorrect)
 - ReactionTime: reaction time when a button is pressed
 - TrialTime: total trial duration
 - runningTime: running time
 - fixcrossTime: duration of the fixation dot
 - cueTime: duration of the cue presentation (should be 0.1 s)
 - postcueTime: duration between the cue offset and the beginning of the next trial (should be 1.2 s)",1,0,0
ds002578,2020-02-19 15:14:13,Arnaud Delorme,1.1.0,P3 study,2020-02-21 19:41:52,1,1,1429240000,0.001 TB,22,2,30,30,v1.2.1,CC0,"Arnaud Delorme, Scott Makeig",,,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6673364/,10.18112/openneuro.ds002578.v1.1.0,,attention,8.0.0,"EEG, MRI","Data for this selective attention task was collected in 2004
 at the Swartz Center for Computational Neuroscience at UCSD.
 These datasets are part of a larger corpus of 32-channel data
 collected a few years prior. The experiment is identical
 although the number of channel is larger (256), the electrode
 positions are scanned and the anatomical MRI is provided
 (allowing for precise source localization). See publication
 for more details.
 

 Raw data manipulation before export:
 - Fuse all BDF BIOSEMI files and reference to electrode 135 (see loadallbdf\_2020.m)
 - Fuse with presentation file information (see loadallbdf\_2020.m)
 - Remove spurious events of type 'condition' and '201' (see clean\_events.m)
 - Add HED tags (see addHEDTags.m)
 - Convert MRI to NIFTI format (MRIcron) and reorient (MRIcrogl) (see convert\_nifti.m)",1,1,1
ds002680,2020-04-02 20:23:47,Arnaud Delorme,1.2.0,Go-nogo categorization and detection task,2020-04-09 0:54:52,1,2,9902130000,0.01 TB,4977,14,0,0,v1.2.1,CC0,Arnaud Delorme,,,,"https://www.ncbi.nlm.nih.gov/pubmed/11244543, https://www.ncbi.nlm.nih.gov/pubmed/15019707, https://papers.cnl.salk.edu/PDFs/From\%20Single-Trial\%20EEG\%20to\%20Brain\%20Area\%20Dynamics\%202002-3661.pdf",10.18112/openneuro.ds002680.v1.2.0,,gonogo,8.0.0,EEG,"Participants seated in a dimly lit room at 110 cm from a computer screen piloted from a PC computer. Two tasks alternated: a categorization task and a recognition task. In both tasks, target images and non-target images were equally likely presented. Participants were tested in two recording phases. The first day was composed of 13 series, the second day of 12 series, with 100 images per series (see details of the series below). To start a series, subjects had to press a touch-sensitive button. A small fixation point (smaller than 0.1 degree of visual angle) was drawn in the middle of a black screen. Then, an 8 bit color vertical photograph (256 pixels wide by 384 pixels high which roughly correspond to 4.5 degree of visual angle in width and 6.5 degree in height) was flashed for 20 ms (2 frames of a 100 Hz SVGA screen) using a programmable graphic board (VSG 2.1, Cambridge Research Systems). This short presentation time avoid that subjects use exploratory eye movement to respond. Participants gave their responses following a go/nogo paradigm. For each target, they had to lift their finger from the button as quickly and accurately as possible (releasing the button restored a focused light beam between an optic fiber led and its receiver; the response latency of this apparatus was under 1 ms). Participants were given 1000 ms to respond, after what any response was considered as a nogo response. The stimulus onset asynchrony (SOA) was 2000 ms plus or minus a random delay of 200 ms. For each distractor, participants had to keep pressing the button during at least 1000 ms (nogo response).
 

 More specifically, in the animal categorization task, participants had to respond whenever there was an animal in the picture. In the recognition task, the session started with a learning phase. A probe image was flashed 15 times during 20 ms intermixed with two presentations of 1000 ms after the fifth and the tenth flashes, allowing an ocular exploration of the image; with an inter-stimulus of 1000 ms. Participants were instructed to carefully examine and learn the probe image in order to recognize it in the following series. The test phase started immediately after the learning phase. The probe image constituted the unique target of the series. Both tasks were organized in series of 100 images; 50 targets images were mixed with 50 non-targets in the animal categorization task; 50 copies of an unique photographs were mixed at random with 50 non-targets in the recognition task.",1,1,1
ds002691,2020-04-08 23:44:40,Arnaud Delorme,1.1.0,Internal attention study,2020-04-09 0:50:39,1,1,814481000,0.001 TB,146,20,23,66,v1.2.1,CC0,"Arnaud Delorme, Dean Radin",,,,"Radin, D., Michel, L., Pierce, A., Delorme, A. (2015) Psychophysical interactions with a single-photon double-slit optical system. Quantum Biosystems, Vol 6, Issue 1, Page 82-98. https://www.semanticscholar.org/paper/Psychophysical-interactions-with-a-single-photon-Radin-Michel/8095890b463b7d373054b9da40a04356cc63bcf2",10.18112/openneuro.ds002691.v1.1.0,,internalattention,8.0.0,EEG,"This experiment has 20 subjects. Subjects asked to mentally concentrate on
 a target (see published article for more information) for periods of about 
 15 seconds.
  
 There are 4 verbal instructions given to subject by an automated computer
 program connected to a speakerphone:
 - The instruction is to wait until the experiment starts
 - The instruction is to relax
 - The instruction is to get ready as the trial is about to start
 - The instruction is to mentally concentrate on the target
  
 All the experiment is performed eye's closed. Relax periods last for about
 9 seconds, are then followed by a period of 6 seconds where the participants
 is asked to ""get ready"" for the trial, followed by a period of 15 seconds of
 concentration. This sequence is repeated 20 times for each participant.",1,1,1
ds002712,2020-04-16 7:54:58,Nicola Molinaro,1.0.1,Numbers and Letters,2020-04-17 17:27:31,1,1,109279000000,0.109 TB,482,25,0,0,1.1.1,CC0,"Sara Aurtenetxe, Nicola Molinaro, Doug Davidson, Manuel Carreiras",,,,https://www.ncbi.nlm.nih.gov/pubmed/XXXXXXXXX,10.18112/openneuro.ds002712.v1.0.1,,NumbersLetters,,"T1w, channels, events, meg","OpenNeuro curator note: This dataset was previously accessible at ds001985. The dataset was reuploaded due to privacy considerations. 
 

 The experiment is composed by two runs
 We here report the code triggers for each run:
 

 Run 1: single item
 10 = single numbers
 15 = single letters
 20 & 25 = single false fonts
 

 Run 2: strings
 35 = strings numbers
 40 = strings letters
 45 & 50 = strings false fonts
 

 raw files could be split into two files 
 (e.g., run-1 + run-11)",1,1,0
ds002718,2020-04-21 20:15:53,Dung Truong,1.0.5,Face processing EEG dataset for EEGLAB,2020-04-21 23:09:57,1,1,4623870000,0.005 TB,582,18,23,31,v1.2.0,CC0,"Daniel G. Wakeman, Richard N Henson",,,This work was supported by the UK Medical Research Council (MC\_A060\_5PR10) and Elekta Ltd.,"Wakeman, D., Henson, R. A multi-subject, multi-modal human neuroimaging dataset. Sci Data 2, 150001 (2015). https://doi.org/10.1038/sdata.2015.1",10.18112/openneuro.ds002718.v1.0.5,,FaceRecognition,7.1.2,"EEG, MRI","Multi-subject, multi-modal (sMRI+EEG) neuroimaging dataset
 on face processing. Original data described at https://www.nature.com/articles/sdata20151
 This is repackaged version of the EEG data in EEGLAB format. The data has gone through
 minimal preprocessing including (see wh\_extracteeg\_BIDS.m):
 - Ignoring fMRI and MEG data (sMRI preserved for EEG source localization)
 - Extracting EEG channels out of the MEG/EEG fif data
 - Adding fiducials
 - Renaming EOG and EKG channels
 - Extracting events from event channel
 - Removing spurious events 5, 6, 7, 13, 14, 15, 17, 18 and 19
 - Removing spurious event 24 for subject 3 run 4
 - Renaming events taking into account button assigned to each subject
 - Correcting event latencies (events have a shift of 34 ms)
 - Resampling data to 250 Hz (this is a step that is done because
  this dataset is used as tutorial for EEGLAB and need to be lightweight)
 - Merging run 1 to 6
 - Removing event fields urevent and duration 
 - Filling up empty fields for events boundary and stim\_file.
 - Saving as EEGLAB .set format
 

 Ramon Martinez, Dung Truong, Scott Makeig, Arnaud Delorme (UCSD, La Jolla, CA, USA)",1,1,0
ds002720,2020-04-22 21:02:35,Ian Daly,1.0.1,BCItempo,2020-04-24 15:25:17,1,1,2563670000,0.003 TB,828,18,18,28,1.0.2,CC0,"Ian Daly, Nicoletta Nicolaou, Duncan Williams, Faustina Hwang, Alexis Kirke, Eduardo Miranda, Slawomir J. Nasuto",,,,,10.18112/openneuro.ds002720.v1.0.1,,Run,,EEG,,1,1,0
ds002721,2020-04-22 21:09:02,Ian Daly,1.0.2,filmClips,2020-04-24 15:27:04,1,1,3598850000,0.004 TB,929,31,18,66,1.0.2,CC0,"Ian Daly, Nicoletta Nicolaou, Duncan Williams, Faustina Hwang, Alexis Kirke, Eduardo Miranda, Slawomir J. Nasuto",,,,,10.18112/openneuro.ds002721.v1.0.2,,Run,,EEG,"0. Sections
 ------------
 1. Project
 2. Dataset
 3. Terms of Use
 4. Contents
 5. Method and Processing
 1. PROJECT
 ------------
 

 Title: Brain-Computer Music Interface for Monitoring and Inducing Affective States (BCMI-MIdAS)
 

 Dates: 2012-2017
 

 Funding organisation: Engineering and Physical Sciences Research Council (EPSRC)
 

 Grant no.: EP/J003077/1 and EP/J002135/1.
 

 2. DATASET
 ------------
 Title: EEG data investigating neural correlates of music-induced emotion.
 

 Description: This dataset accompanies the publication by Daly et al. (2018) and has been analysed in Daly et al. (2014; 2015a; 2015b) (please see Section 5 for full references). The purpose of the research activity in which the data were collected was to investigate the EEG neural correlates of music-induced emotion. For this purpose 31 healthy adult participants listened to 40 music clips of 12 s duration each, targeting a range of emotional states. The music clips comprised excerpts from film scores spanning a range of styles and rated on induced emotion. 
 The dataset contains unprocessed EEG data from all 31 participants (age range 18-66, 18 female) while listening to the music clips, together with the reported induced emotional responses . The paradigm involved 6 runs of EEG recordings. The first and last runs were resting state runs, during which participants were instructed to sit still and rest for 300 s. The other 4 runs each contained 10 music listening trials.
 

 Publication Year: 2018
 

 Creator: Nicoletta Nicolaou, Ian Daly.
 

 Contributors: Isil Poyraz Bilgin, James Weaver, Asad Malik. 
 

 Principal Investigator: Slawomir Nasuto (EP/J003077/1).
 

 Co-Investigator: Eduardo Miranda (EP/J002135/1).
 

 Organisation: University of Reading
 

 Rights-holders: University of Reading
 

 Source: The musical stimuli were taken from Eerola & Vuoskoski, “A comparison of the discrete and dimensional models of emotion in music”, Psychol. Music, 39:18-49, 2010 (doi: 10.1177/0305735610362821). Stimuli set 1 was used (https://www.jyu.fi/hytk/fi/laitokset/mutku/en/research/projects2/past-projects/coe/materials/emotion/soundtracks/set1/view)
 

 System: The data is prepared for use on Windows systems and no garanantee is made that the datasets can be opened correctly on other systems.
 

 3. TERMS OF USE
 -----------------
 

 Copyright University of Reading, 2018. This dataset is licensed by the rights-holder(s) under a Creative Commons Attribution 4.0 International Licence: https://creativecommons.org/licenses/by/4.0/.
 

 4. CONTENTS
 ------------
 

 BIDS File listing:
 The dataset comprises data from 31 participants, named using the convention:
 sub\_s\_number
 where: s\_number is a random participant number from 1 to 31. For example: ‘sub-08’ contains data obtained from participant 8. 
 

 The data is BIDS format and contains EEG and associated meta data. The sampling rate is 1 kHz and the EEG corresponding to a music clip is 20 s long (the duration of the clips).
 

 Each data folder contains the following data (please note that the number of runs varies between participants):
 

 5. METHOD and PROCESSING
 --------------------------
 

 This information is available in the following publications:
 

 [1] Daly, I., Nicolaou, N., Williams, D., Hwang, F., Kirke, A., Miranda, E., Nasuto, S.J., ?eural and physiological data from participants listening to affective music? Scientific Data, 2018.
 [2] Daly, I., Malik, A., Hwang, F., Roesch, E., Weaver, J., Kirke, A., Williams, D., Miranda, E. R., Nasuto, S. J., ?eural correlates of emotional responses to music: an EEG study? Neuroscience Letters, 573: 52-7, 2014; doi: 10.1016/j.neulet.2014.05.003.
 [3] Daly, I., Hallowell, J., Hwang, F., Kirke, A., Malik, A., Roesch, E., Weaver, J., Williams, D., Miranda, E., Nasuto, S.J., ?hanges in music tempo entrain movement related brain activity? Proc. IEEE EMBC 2014, pp.4595-8; doi: 10.1109/EMBC.2014.6944647
 [4] Daly, I., Williams, D., Hallowell, J., Hwang, F., Kirke, A., Malik, A., Weaver, J., Miranda, E., Nasuto, S.J., ?usic-induced emotions can be predicted from a combination of brain activity and acoustic features? Brain and Cognition, 101:1-11, 2015b; doi: 10.1016/j.bandc.2015.08.003
 

 Please cite these references if you use this dataset in your study.
 

 Thank you for your interest in our work.",1,1,0
ds002722,2020-04-22 21:30:40,Ian Daly,1.0.1,phBCMIcalibration,2020-04-24 15:28:26,1,1,6545790000,0.007 TB,582,19,19,30,1.0.2,CC0,"Ian Daly, Nicoletta Nicolaou, Duncan Williams, Faustina Hwang, Alexis Kirke, Eduardo Miranda, Slawomir J. Nasuto",,,,,10.18112/openneuro.ds002722.v1.0.1,,Run,,"channels, eeg, events",,1,1,0
ds002723,2020-04-22 21:34:22,Ian Daly,1.1.0,phBCMItesting,2020-04-24 15:29:36,1,1,2789430000,0.003 TB,204,8,19,30,1.0.2,CC0,"Ian Daly, Nicoletta Nicolaou, Duncan Williams, Faustina Hwang, Alexis Kirke, Eduardo Miranda, Slawomir J. Nasuto",,,,,10.18112/openneuro.ds002723.v1.1.0,,Run,,"channels, eeg, events","0. Sections
 ------------
 

 1. Project
 2. Dataset
 3. Terms of Use
 4. Contents
 5. Method and Processing
 

 1. PROJECT
 ------------
 

 Title: Brain-Computer Music Interface for Monitoring and Inducing Affective States (BCMI-MIdAS)
 Dates: 2012-2017
 Funding organisation: Engineering and Physical Sciences Research Council (EPSRC)
 Grant no.: EP/J003077/1
 

 2. DATASET
 ------------
 

 EEG data from an affective Music Brain-Computer: online real-time control.
 

 Description: This dataset accompanies the publication by Daly et al. (2018) and has been analysed in Daly et al. (2016) (please see Section 5 for full references). The purpose of the research activity in which the data were collected was to investigate the performance of a real-time and online brain-computer interface that identified the user’s emotional state and modified music on-the-fly in order to induce a target emotional state. For this purpose, participants listened to 60 s music clips targeting different affective states, as defined by valence and arousal. The music clips were generated using a synthetic music generator. The dataset contains the EEG data from 8 healthy adult participants during real-time control of the system while listening to the music clips, together with the reported affective state (valence and arousal values).
 

 This dataset is connected to 2 additional datasets:
  
 1. EEG data from an affective Music Brain-Computer Interface: system calibration. doi:
 2. EEG data from an affective Music Brain-Computer: offline training data to induce target emotional states. doi:
 

 Please note that the number of participants varies between datasets; however, participant codes are the same across all three datasets.
 

 Publication Year: 2018
 

 Creators: Nicoletta Nicolaou, Ian Daly.
 Contributors: Isil Poyraz Bilgin, James Weaver, Asad Malik, Alexis Kirke, Duncan Williams.
 Principal Investigator: Slawomir Nasuto (EP/J003077/1).
 Co-Investigator: Eduardo Miranda (EP/J002135/1).
 Organisation: University of Reading
 Rights-holders: University of Reading
 Source: The synthetic generator used to generate the music clips was presented in Williams et al., “Affective Calibration of Musical Feature Sets in an Emotionally Intelligent Music Composition System”, ACM Trans. Appl. Percept. 14, 3, Article 17 (May 2017), 13 pages. DOI: https://doi.org/10.1145/3059005. 
 

 3. TERMS OF USE
 -----------------
 

 Copyright University of Reading, 2018. This dataset is licensed by the rights-holder(s) under a Creative Commons Attribution 4.0 International Licence: https://creativecommons.org/licenses/by/4.0/.
 

 4. CONTENTS
 ------------
 

 The dataset comprises data from 8 subjects. The sampling rate is 1 kHz and the music listening task corresponding to a music clip is 60 s long (clip duration). During the first 20 s, the music clip places the listener in emotional state A, while for the remaining 40 s the music clip targets the affective trajectory from emotional state B to C.
 Within a 60s music listening epoch there are two target affective states. In the first 20s the music is generated to target one affective state (target A), for the next 20s the BCMI attempts to (a) work out what affective state the participant is actually in, and (b) generate music to move them from this affective state to the next targetted affective state (target B), which is targetted for the last 20s of the 60s music listening epoch.
 

 5. METHOD and PROCESSING
 --------------------------
 

 This information is available in the following publications:
 

 [1] Daly, I., Nicolaou, N., Williams, D., Hwang, F., Kirke, A., Miranda, E., Nasuto, S.J., “Neural and physiological data from participants listening to affective music”, Scientific Data, 2018.
 [2] Daly, I., Williams, D., Hwang, F., Kirke, A., Malik, A., Weaver, J., Miranda, E. R., Nasuto, S. J., “Affective Brain-Computer Music Interfacing”, Journal of Neural Engineering, 13:4, July 2016. http://dx.doi.org/10.1088/1741-2560/13/4/046022
 If you use this dataset in your study please cite these references, as well as the following reference:
 [3] Williams, D., Kirke, A., Miranda, E.R., Daly, I., Hwang, F., Weaver, J., Nasuto, S.J., “Affective Calibration of Musical Feature Sets in an Emotionally Intelligent Music Composition System”, ACM Trans. Appl. Percept. 14, 3, Article 17 (May 2017), 13 pages. DOI: https://doi.org/10.1145/3059005
 

 Thank you for your interest in our work.",1,1,0
ds002724,2020-04-22 21:35:49,Ian Daly,1.0.1,phBCMItraining,2020-04-24 15:31:22,1,3,9147070000,0.009 TB,700,10,19,24,1.0.2,CC0,"Ian Daly, Nicoletta Nicolaou, Duncan Williams, Faustina Hwang, Alexis Kirke, Eduardo Miranda, Slawomir J. Nasuto",,,,,10.18112/openneuro.ds002724.v1.0.1,,Run,,"channels, eeg, events",,1,1,0
ds002725,2020-04-22 21:42:23,Ian Daly,1.0.0,A dataset recording joint EEG-fMRI during affective music listening,2020-04-24 18:49:36,1,1,16445700000,0.016 TB,1098,21,20,29,1.0.2,CC0,"Ian Daly, Nicoletta Nicolaou, Duncan Williams, Faustina Hwang, Alexis Kirke, Eduardo Miranda, Slawomir J. Nasuto",,,,,10.18112/openneuro.ds002725.v1.0.0,,"GeneratedMusic, classicalMusic, genMusic01, genMusic02, washout, genMusic03",,"T1w, channels, eeg, events, bold","Dataset: Joint EEG-fMRI recording during affective music listening.
 

 This dataset was recorded from 21 healthy adult participants viia a joint EEG-fMRI modality while they
 listened to a set of music stimuli chosen and generated to produce different affective (emotional)
 reponses. Participants self-reported their felt affective states as they listened to the music.
 

 The full experiment description can be found in our paper (Daly et.al., 2019).
 

 Data recorded in 2016
 

 Published in 2019
 

 [1] Daly, I., Williams, D., Hwang, F., Kirke, A., Miranda, E. R., & Nasuto, S. J. (2019). Electroencephalography reflects the activity of sub-cortical brain regions during approach-withdrawal behaviour while listening to music. Scientific Reports, 9(1), 9415. https://doi.org/10.1038/s41598-019-45105-2",1,1,0
ds002778,2020-05-05 17:44:34,Alexander Rockhill,1.0.5,UC San Diego Resting State EEG Data from Patients with Parkinson's Disease,2020-05-06 5:35:53,1,3,571471000,0.001 TB,328,31,47,82,1.2.2,CC0,"Alexander P. Rockhill, Nicko Jackson, Jobi George, Adam Aron, Nicole C. Swann",,,,,doi:10.18112/openneuro.ds002778.v1.0.5,,rest,,EEG,"Welcome to the resting state EEG dataset collected at the University of San Diego and curated by Alex Rockhill at the University of Oregon.
 

 Please email arockhil@uoregon.edu before submitting a manuscript to be published in a peer-reviewed journal using this data, we wish to ensure that the data to be analyzed and interpreted with scientific integrity so as not to mislead the public about findings that may have clinical relevance. The purpose of this is to be responsible stewards of the data without an ""available upon reasonable request"" clause that we feel doesn't fully represent the open-source, reproducible ethos. The data is freely available to download so we cannot stop your publication if we don't support your methods and interpretation of findings, however, in being good data stewards, we would like to offer suggestions in the pre-publication stage so as to reduce conflict in published scientific literature. As far as credit, there is precedent for receiving a mention in the acknowledgements section for reading and providing feedback on the paper or, for more involved consulting, being included as an author may be warranted. The purpose of asking for this is not to inflate our number of authorships; we take ethical considerations of the best way to handle intellectual property in the form of manuscripts very seriously, and, again, sharing is at the discretion of the author although we strongly recommend it. Please be ethical and considerate in your use of this data and all open-source data and be sure to credit authors by citing them.
 

 An example of an analysis that we could consider problematic and would strongly advice to be corrected before submission to a publication would be using machine learning to classify Parkinson's patients from healthy controls using this dataset. This is because there are far too few patients for proper statistics. Parkinson's disease presents heterogeneously across patients, and, with a proper test-training split, there would be fewer than 8 patients in the testing set. Statistics on 8 or fewer patients for such a complicated diease would be inaccurate due to having too small of a sample size. Furthermore, if multiple machine learning algorithms were desired to be tested, a third split would be required to choose the best method, further lowering the number of patients in the testing set. We strongly advise against using any such approach because it would mislead patients and people who are interested in knowing if they have Parkinson's disease.
 

 Note that UPDRS rating scales were collected by laboratory personnel who had completed online training and not a board-certified neurologist. Results should be interpreted accordingly, especially that analyses based largely on these ratings should be taken with the appropriate amount of uncertainty.
 

 In addition to contacting the aforementioned email, please cite the following papers:
 

 Nicko Jackson, Scott R. Cole, Bradley Voytek, Nicole C. Swann. Characteristics of Waveform Shape in Parkinson's Disease Detected with Scalp Electroencephalography. eNeuro 20 May 2019, 6 (3) ENEURO.0151-19.2019; DOI: 10.1523/ENEURO.0151-19.2019.
 

 Swann NC, de Hemptinne C, Aron AR, Ostrem JL, Knight RT, Starr PA. Elevated synchrony in Parkinson disease detected with electroencephalography. Ann Neurol. 2015 Nov;78(5):742-50. doi: 10.1002/ana.24507. Epub 2015 Sep 2. PMID: 26290353; PMCID: PMC4623949.
 

 George JS, Strunk J, Mak-McCully R, Houser M, Poizner H, Aron AR. Dopaminergic therapy in Parkinson's disease decreases cortical beta band coherence in the resting state and increases cortical beta band power during executive control. Neuroimage Clin. 2013 Aug 8;3:261-70. doi: 10.1016/j.nicl.2013.07.013. PMID: 24273711; PMCID: PMC3814961.
 

 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896).
 

 Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G.,
 Phillips, C., Delorme, A., Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Scientific Data, 6, 103. https://doi.org/10.1038/s41597-019-0104-8.
 

 Note: see this discussion on the structure of the json files that is sufficient but not optimal and will hopefully be changed in future versions of BIDS: https://neurostars.org/t/behavior-metadata-without-tsv-event-data-related-to-a-neuroimaging-data/6768/25.",1,1,0
ds002791,2020-05-13 12:50:22,Ahmad Mheich,1.0.0,DataSet1,2020-07-17 20:53:39,1,2,50562500000,0.051 TB,647,23,19,40,2.1,CC0,"Ahmad Mheich, Olivier Dufor, Sahar Yassine, Aya Kabbara, Fabrice Wendling, Mahmoud Hassan",,,,,10.18112/openneuro.ds002791.v1.0.0,,,,"channels, eeg, electrodes, events",,1,1,0
ds002799,2020-05-16 0:33:07,Remya Nair,1.0.4,Human es-fMRI Resource: Concurrent deep-brain stimulation and whole-brain functional MRI,2020-05-18 16:49:05,1,2,19940300000,0.02 TB,1103,26,13,59,1.0.0,CC0,"Thompson WH*, Nair R*, Oya H*, Esteban O, Shine JM, Petkov CI, Poldrack RA, Howard M, Adolphs R†, *equally contributing, †corresponding author",,,"Funded by NIH grant U01NS103780 (RP, RA and MH) ===NEMAR-SEP=== The Simons Foundation Collaboration on the Global Brain (RA) ===NEMAR-SEP=== Knut and Alice Wallenberg Foundation grant 2016.0473 (WHT) ===NEMAR-SEP=== UK Wellcome Trust (CIP) and European Research Council (CIP) ===NEMAR-SEP=== The OpenNeuro repository is funded by NIH Grant R24MH117179 (RP)",https://rdcu.be/b57kz,10.18112/openneuro.ds002799.v1.0.4,,"es, rest",,"MRI, iEEG","Link to published paper for this data resource: https://rdcu.be/b57kz
 

 This collection contains data from 26 human patients who underwent electrical stimulation during functional magnetic resonance imaging (es-fMRI). The patients had medically refractory epilepsy requiring surgically implanted intracranial electrodes in cortical and subcortical locations. One or multiple contacts on these electrodes were stimulated while simultaneously recording BOLD-fMRI activity in a block design. Multiple runs exist for patients with different stimulation sites. 
 Data is organized in two sessions : Pre-op (pre electrode implantation) and Post-op (post electrode implantation). Raw data is provided in BIDS format and consists of T1s, T2s, resting state scans (pre-op), es-fMRI scans(post-op) , any associated field-maps and stimulation electrode coordinates and stimulation parameters. Pre-processed data (fMRIprep and Freesurfer) is present in the ‘derivatives’ folder. 
 

 Notes:
 1. Subject IDs 339, 369 and 394 do not have stimulation electrode location data available.
 2. Electrodes are in chA-chB format (chA gets leading positive phase of the stimulation). This information is stored in the ""channel"" file for each stimulation run.
 3. In some cases, two distant sites were stimulated simultaneously as indicated by the electrode listed under the appropriate run IDs within the ieeg folders.",1,1,0
ds002814,2020-05-20 12:33:06,morteza mahdiani,1.3.0,TODO: name of the dataset,2020-11-22 9:06:42,1,2,29779100000,0.03 TB,1139,21,19,32,1.0.1,CC0,"Fatemeh Ebrahiminia, Morteza Mahdiani, Seyed-Mahdi Khaligh-Razavi","Please note that the article is currently under review. For further information, please contact the corresponding authors 
 Seyed-Mahdi Khaligh-Razavi (seyed@cognetivity.com)
 Fatemeh Ebrahiminia (ebrahiminia.fatemeh@gmail.com)","TODO: describe how to acknowledge -- either cite a corresponding paper, or just in acknowledgement section",TODO ===NEMAR-SEP=== GRANT #1 ===NEMAR-SEP=== GRANT #2,TODO ===NEMAR-SEP=== List of papers or websites,doi:10.18112/openneuro.ds002814.v1.3.0,,"task-categorySelectivity, categorySelectivity, TODO: full task name for categorySelectivity",,"MRI, EEG","TODO: Provide description for the dataset -- basic details about the study, possibly pointing to pre-registration (if public or embargoed)",1,1,0
ds002833,2020-05-21 19:21:20,Ahmad Mheich,1.0.0,DataSet2,2020-07-17 20:54:59,1,4,42698200000,0.043 TB,623,20,20,40,2.1,CC0,"Ahmad Mheich, Olivier Dufor, Sahar Yassine, Aya Kabbara, Fabrice Wendling, Mahmoud Hassan",,,,,10.18112/openneuro.ds002833.v1.0.0,,PicturesNaming,,"channels, eeg, electrodes, events",,1,1,0
ds002893,2020-06-15 6:04:46,Dung Truong,2.0.0,Auditory-Visual Shift Study,2020-06-29 23:47:19,1,1,8262780000,7.7 GB,370,49,20,78,1.7.0,CC0,"Marissa Westerfield (data, curation), Scott Makeig (data, curation), Dung Truong (curation), Kay Robbins (curation), Arno Delorme (curation)",,Cite this paper: https://pubmed.ncbi.nlm.nih.gov/18482717/ and consider including the following message: 'This data was obtained from the OpenNeuro database. Its accession number is ds002893',National Institutes of Health National Institute on Aging (R01-AG18030).,"Ceponiene R., Westerfield M., Torki M., and Townsend J.(2008). Modality-specificity of sensory aging in vision and audition: Evidence from event-related potentials, Brain Research, vol. 1215, 53-68. DOI: 10.1016/j.brainres.2008.02.010.",doi:10.18112/openneuro.ds002893.v2.0.0,,AuditoryVisualShift,8.0.0,EEG,"## Audio-Visual Attention Shift Experiment 
 

 **Project name:** [Sensory processing in aging]
 

 **Years the project ran:** 2007-2008
 

 **Brief overview of experiment task:** 
 The purpose of this Auditory-Visual Attention Shift study was to explore the effects of aging on selective 
 attending and responding to auditory and visual stimulus differences using an interleaved dual-oddball 
 audio-visual task design. EEG and EOG channels were acquired.
 

 **Data collection.** Scalp EEG data were collected from 33 scalp electrode channels, each referred to a 
 right mastoid electrode, within an analogue passband of 0.1 to 60 Hz.
 

 **Contact person:**  Scott Makeig <smakeig@ucsd.edu>, ORCID: 0000-0002-9048-8438.
 

 **Access information:** Contributed to OpenNeuro.org and NEMAR.org in BIDS format following annotation using HED 8.0.0 in April, 2022.
 

 **Independent variables:** Stimulus stream (visual, auditory, cue); stimulus stream identity (target, standard); task condition (FA, FV, SH)
 

 **Dependent variables:** Participant response (correct/incorrect). Button press response attributes (task time window and post-target latency).
 

 

 **Participant pool:** The dataset includes data collected from 19 younger adult subjects
 (8 male, 11 female, ages 20?40 years) and 30 older adult subjects (11 male, 19 female, ages 49-73 years).
 The subjects were cognitively intact and had normal or adjusted to normal hearing and vision.
 

 **Initial setup:** EEG data were collected from 33 EEG channels using the 10-20 placement and referenced to the right mastoid.
 The left mastoid and two EOG channels were also included in the collection. The data was acquired at a sampling rate of 250 Hz 
 with an analog pass band of 0.01 to 60 Hz (SA Instrumentation, San Diego). Input impedances were brought under 5 kilo-ohms 
 by careful scalp preparation.
 

 **Task conditions:**
 

 - **Focus Visual (FV):** participants pressed the response button only in response to target visual stimuli.
 - **Focus Auditory (FA):** participants pressed the same button only in response to target auditory stimuli.
 - **Shift Focus (SF):** participants shifted between performing the FV and FA tasks as cued by the preceding
 (Look/Hear) cue stimulus.
 

 **Task organization:** The stimuli were presented in blocks of 264 for a duration of 2.64.
 In each block there were 12 ""Hear"" and 12 ""Look"" cues. A total of 20 blocks were presented for each session.
 Each experiment began with two non-shift blocks (one each of auditory focus FA and visual focus FV counter-balanced across sessions).
 These were followed by 12 SF shift blocks. Finally an auditory focus FA group (3 blocks) and a visual focus FV group (3 blocks) were
 presented. The order of these groups was counter-balanced across experiments. Brief rest periods occur between task blocks. The task condition in the next block was given verbally to the participant during the pre-block rest period.
 

 **Task details:** Participants respond by finger button press selectively to auditory (brief tones) and 
 visual (colored squares) stimuli constituting distinct, interleaved auditory and visual oddball stimulus streams 
 whose stimuli are presented in randomly interleaved order with stimulus-onset asynchronies (SOAs) varying randomly 
 between 200 and 800 ms.  
 

 - **Visual stimuli:** were (infrequent, 10%) dark blue target or (frequent, 90%) light blue standard 8.4-cm2 squares presented for 100 ms.
 - **Auditory stimuli:** were (infrequent, 10%) 550-Hz target or (frequent, 90%) 500-Hz tones with 100 msec duration and 63 dB SPL intensity.
 - **Task cue stimuli:** interspersed in the stimulus sequence at mean 5-sec intervals and consisting of the simultaneous spoken and printed display of one of the words *Look* or *Hear* each presented for 200 msec.
 

 **Additional data acquired:** Participants had no history of major neurological, psychiatric, or medical disorders. All had normal or adjusted to normal vision and hearing (none wore hearing aids). Verbal and performance IQ were assessed using the WASI-III (Wechsler, 1997). There were no significant differences between the groups in IQ measures or years of education. Participants in the Older group received a battery of neuropsychological tests to assure normal cognitive functioning, including the Mini Mental State Exam (MMSE) (Folstein et al., 1975), Dementia Rating Scale (DRS) (Mattis, 1988), Wechsler Memory Scale.
 

 **Experiment location:** Department of Psychiatry laboratory of Jeanne Townsend, University of California San Diego, La Jolla CA (USA).
 

 **Note 1**: ERP measure results for the FA and FV conditions only were presented in Ceponiene, R., Westerfield, M., Torki, M. and Townsend, J., 2008. Modality-specificity of sensory aging in vision and audition: evidence from event-related potentials.?Brain research,?1215, pp.53-68. Some unpublished results by Christian Kothe and Scott Makeig on the SH condition may be available from the authors <christiankothe@gmail.com> <smakeig@ucsd.edu>.
 

 **Note 2**: The code subdirectory has several auxilliary files that were produced during
 the curation process. The curation was done using a series of Jupyter notebooks
 that are available as run in the code/curation_notebooks subdirectory.
 

 During the running of these curation notebooks information about the status was logged
 using the HEDLogger. The output of the logging process is in code/curation_logs.
 

 Updated versions of the curation notebooks can be found at:
 https://github.com/hed-standard/hed-examples/tree/main/hedcode/jupyter_notebooks/dataset_specific_processing/attention_shift.",1,0,1
ds003004,2020-07-09 23:36:39,Dung Truong,1.1.1,Emotion Study,2020-07-10 0:05:52,1,1,38704500000,0.039 TB,277,34,0,0,1.1.1,CC0,"Julie Onton, Scott Makeig",,,,"Onton J.A. and Makeig S. (2009). High-frequency broadband modulations of electroencephalographic spectra. Front. Hum. Neurosci. 3,:61. DOI: https://doi.org/10.3389/neuro.09.061.2009 ===NEMAR-SEP=== Onton J.A. and Makeig S. (2009). Independent modulators of regional EEG alpha sub-band power during a working memory task. Poster session presented to Cognitive Neuroscience Society; San Francisco, CA. https://sccn.ucsd.edu/~julie/AlphaIMposter.pdf ===NEMAR-SEP=== Kothe C.A., Makeig S., and Onton J.A. (2013). Emotion Recognition from EEG during Self-Paced Emotional Imagery. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. IEEE. 855-858, DOI: https://doi.org/10.1109/ACII.2013.160 ===NEMAR-SEP=== Hsu S.H., Lin Y., Onton J.A., Jung T.P., and Makeig S. (2022). Unsupervised learning of brain state dynamics during emotion imagination using high-density EEG. NeuroImage. 249:118873. DOI: https://doi.org/10.1016/j.neuroimage.2022.118873",doi:10.18112/openneuro.ds003004.v1.1.1,,ImaginedEmotion,,EEG,"**PARADIGM:** The study uses the method of guided imagery to induce resting, eyes-closed participants using voice-guided imagination to enter distinct 15 emotion states during acquisition of high-density EEG data. 
 

 During the study, participants listen to 15 voice recordings that each suggest imagining a scenario in which they have experienced -- or would experience the named target emotion. Some target emotions have positive valence (e.g., joy, happiness), others negative valence (e.g., sadness, anger). Before and between the 15 emotion imagination periods, participants hear relaxation suggestions ('Now return to a neutral state by ...').
 

 **PROCEDURE:** When the participant first begins to feel the target emotion, they are asked to indicate this by pressing a handheld button. Participants are asked to continue feeling the emotion as long as possible. To intensify and lengthen the periods of experienced emotion, participants are asked to interoceptively perceive and attend relevant somatosensory sensations. When the target feeling wanes (typically after 1 and 5 minutes), participants push the button again to leave the emotion imagination period and cue the relaxation instructions.
 

 **DATA HANDLING:** The raw data have been preprocessed to fix confusing event codes and to remove excessively noisy channels. In addition, a 1-Hz high pass filter was applied to ready the data for ICA decomposition. Note: Unfortunately, the unfiltered data are no longer available.
 

 **NOTE:** Sub22 was a repeat subject, hence was removed from the dataset.",1,1,0
ds003029,2020-07-26 19:33:04,Adam Li,1.0.6,iEEG Fragility Epileptogenic Zone,2020-12-02 21:03:31,0,1,11083800000,0.011 TB,679,35,13,59,1.4.0,CC0,"Adam Li, Sara Inati, Kareem Zaghloul, Nathan Crone, William Anderson, Emily Johnson, Iahn Cajigas, Damian Brusko, Jonathan Jagid, Angel Claudio, Andres Kanner, Jennifer Hopp, Stephanie Chen, Jennifer Haagensen, Sridevi Sarma",Maryland Advanced Research Computing Center (MARCC),"Please cite: https://www.biorxiv.org/content/10.1101/862797v3
 

 doi: https://doi.org/10.1101/862797",NIH T32 EB003383 ===NEMAR-SEP=== NSF GRFP (DGE-1746891) ===NEMAR-SEP=== ARCS Scholarship ===NEMAR-SEP=== Whitaker Fellowship ===NEMAR-SEP=== Chateaubriand Fellowship ===NEMAR-SEP=== NIH R21 NS103113 ===NEMAR-SEP=== Coulter Foundation ===NEMAR-SEP=== Maryland Innovation Initiative ===NEMAR-SEP=== US NSF Career Award 1055560 ===NEMAR-SEP=== Burroughs Well CASI Award 1007274,,doi:10.18112/openneuro.ds003029.v1.0.6,"IRBs at JHH, UMMC, UMF, NIH",ictal,,iEEG,"Fragility Multi-Center Retrospective Study
 ------------------------------------------
 

 This dataset was updated and prepared for release as part of a manuscript by Bernabei & Li et al. (in preparation). A subset of the data has been featured in [1].
 

 Summary
 -------------
 

 iEEG and EEG data from 5 centers is organized in our study with a total of 100 subjects. We publish 4 centers' dataset here due to data sharing issues. 
 

 Acquisitions include ECoG and SEEG. Each run specifies a different snapshot of EEG data from that specific subject's session. For seizure sessions, this means that each run is a EEG snapshot around a different seizure event.
 

 For additional clinical metadata about each subject, refer to the clinical Excel table in the publication.
 

 Data Availability
 ----------------------
 

 NIH, JHH, UMMC, and UMF agreed to share. Cleveland Clinic did not, so requires an additional DUA.
 

 All data, except for Cleveland Clinic was approved by their centers to be de-identified and shared. All data in this dataset have no PHI, or other identifiers associated with patient. In order to access Cleveland Clinic data, please forward all requests to Amber Sours, SOURSA@ccf.org:
 

 Amber Sours, MPH
 Research Supervisor | Epilepsy Center
 Cleveland Clinic | 9500 Euclid Ave. S3-399 | Cleveland, OH 44195
 (216) 444-8638
 

 You will need to sign a data use agreement (DUA).
 

 Sourcedata
 ----------------
 

 For each subject, there was a raw EDF file, which was converted into the BrainVision format with `mne\_bids`.
 Each subject with SEEG implantation, also has an Excel table, called `electrode\_layout.xlsx`, which outlines where the clinicians marked each electrode anatomically. Note that there is no rigorous atlas applied, so the main points of interest are: `WM`, `GM`, `VENTRICLE`, `CSF`, and `OUT`, which represent white-matter, gray-matter, ventricle, cerebrospinal fluid and outside the brain. WM, Ventricle, CSF and OUT were removed channels from further analysis. These were labeled in the corresponding BIDS `channels.tsv` sidecar file as `status=bad`.
 The dataset uploaded to `openneuro.org` does not contain the `sourcedata` since there was an extra
 anonymization step that occurred when fully converting to BIDS.
 

 Derivatives
 ----------------
 

 Derivatives include:
 * fragility analysis
 * frequency analysis
 * graph metrics analysis
 * figures
 

 These can be computed by following the following paper:
 [Neural Fragility as an EEG Marker for the Seizure Onset Zone](https://www.biorxiv.org/content/10.1101/862797v3)
 

 Channel locations in (x,y,z) coordinates
 --------------------------------------------------------
 

 Unfortunately, the necessary T1 MRI, and CT scans to estimate these were not collected/processed, and exact channel locations are not available for any subject in this dataset as of 09/05/2023.
 

 Events and Descriptions
 -----------------------------------
 

 Within each EDF file, there contain event markers that are annotated by clinicians, which may inform you of specific clinical events that are occuring in time, or of when they saw seizures onset and offset (clinical and electrographic). 
 

 During a seizure event, specifically event markers may follow this time course:
 

  * eeg onset, or clinical onset - the onset of a seizure that is either marked electrographically, or by clinical behavior. Note that the clinical onset may not always be present, since some seizures manifest without clinical behavioral changes.
  * Marker/Mark On - these are usually annotations within some cases, where a health practitioner injects a chemical marker for use in ICTAL SPECT imaging after a seizure occurs. This is commonly done to see which portions of the brain are active metabolically.
  * Marker/Mark Off - This is when the ICTAL SPECT stops imaging.
  * eeg offset, or clinical offset - this is the offset of the seizure, as determined either electrographically, or by clinical symptoms.
 

 Other events included may be beneficial for you to understand the time-course of each seizure. Note that ICTAL SPECT occurs in all Cleveland Clinic data. Note that seizure markers are not consistent in their description naming, so one might encode some specific regular-expression rules to consistently capture seizure onset/offset markers across all dataset. In the case of UMMC data, all onset and offset markers were provided by the clinicians on an Excel sheet instead of via the EDF file. So we went in and added the annotations manually to each EDF file. 
 

 Seizure Electrographic and Clinical Onset Annotations
 -----------------------------------------------------------------------------
 

 For various datasets, there are seizures present within the dataset. Generally there is only one seizure per EDF file. When seizures are present, they are marked electrographically (and clinically if present) via standard approaches in the epilepsy clinical workflow.
 

 Clinical onset are just manifestation of the seizures with clinical syndromes. Sometimes the maker may not be present.
 

 Seizure Onset Zone Annotations
 ------------------------------
 What is actually important in the evaluation of datasets is the clinical annotations of their localization hypotheses of the seizure onset zone.
 

 These generally include:
 

  * early onset: the earliest onset electrodes participating in the seizure that clinicians saw
  * early/late spread (optional): the electrodes that showed epileptic spread activity after seizure onset. Not all seizures has spread contacts annotated.
 

 

 Surgical Zone (Resection or Ablation) Annotations
 -----------------------------------------------------------------------
 

 For patients with the post-surgical MRI available, then the segmentation process outlined above tells us which electrodes were within the surgical removed brain region.
 

 Otherwise, clinicians give us their best estimate, of which electrodes were resected/ablated based on their surgical notes.
 

 For surgical patients whose postoperative medical records did not explicitly indicate specific resected or ablated contacts, manual visual inspection was performed to determine the approximate contacts that were located in later resected/ablated tissue. Postoperative T1 MRI scans were compared against post-SEEG implantation CT scans or CURRY coregistrations of preoperative MRI/post SEEG CT scans. Contacts of interest in and around the area of the reported resection were selected individually and the corresponding slice was navigated to on the CT scan or CURRY coregistration. After identifying landmarks of that slice (e.g. skull shape, skull features, shape of prominent brain structures like the ventricles, central sulcus, superior temporal gyrus, etc.), the location of a given contact in relation to these landmarks, and the location of the slice along the axial plane, the corresponding slice in the postoperative MRI scan was navigated to. The resected tissue within the slice was then visually inspected and compared against the distinct landmarks identified in the CT scans, if brain tissue was not present in the corresponding location of the contact, then the contact was marked as resected/ablated. This process was repeated for each contact of interest.
 

 References
 -----------------
 

 [1] Adam Li, Chester Huynh, Zachary Fitzgerald, Iahn Cajigas, Damian Brusko, Jonathan Jagid, Angel Claudio, Andres Kanner, Jennifer Hopp, Stephanie Chen, Jennifer Haagensen, Emily Johnson, William Anderson, Nathan Crone, Sara Inati, Kareem Zaghloul, Juan Bulacio, Jorge Gonzalez-Martinez, Sridevi V. Sarma. Neural Fragility as an EEG Marker of the Seizure Onset Zone. bioRxiv 862797; doi: https://doi.org/10.1101/862797
 

 [2] Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 [3] Holdgraf, C., Appelhoff, S., Bickel, S., Bouchard, K., D'Ambrosio, S., David, O., … Hermes, D. (2019). iEEG-BIDS, extending the Brain Imaging Data Structure specification to human intracranial electrophysiology. Scientific Data, 6, 102. https://doi.org/10.1038/s41597-019-0105-7
 

 [4] Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G., Phillips, C., Delorme, A., Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Scientific Data, 6, 103. https://doi.org/10.1038/s41597-019-0104-8",1,1,0
ds003039,2020-07-29 9:46:15,Nadine Jacobsen,1.0.2,free walking study,2022-05-06 12:42:43,0,1,8401130000,0.008 TB,157,19,19,32,v2.0,CC0,"Nadine Jacobsen, Sarah Blum, Karsten Witt, Stefan Debener, Joanna Scanlon",,,,"Jacobsen, N. S. J., Blum, S., Witt, K., & Debener, S. (2020). A walk in the park? Characterizing gait-related artifacts in mobile EEG recordings. European Journal of Neuroscience. https://doi.org/10.1111/ejn.14965",doi:10.18112/openneuro.ds003039.v1.0.2,,neurCorrYoung,,EEG,"This free walking experiment consists of 19 participants.
 Subjects walked two different routes outdoors
 Each route was walked twice. Once while pressing buttons in a self-paced manner.
 The article (see Reference) contains all methodological details
 

 - Nadine Jacobsen (March, 2020)",1,1,0
ds003061,2020-08-08 7:05:52,Arnaud Delorme,1.1.2,P300 sound task,2020-08-19 0:37:07,1,1,2421800000,0.002 TB,282,13,24,58,v1.2.1,CC0,Arnaud Delorme,,"Delorme, A. (2020) EEG data from an auditory oddball task. Data archive openneuro.org. 10.18112/openneuro.ds003061.v1.0.0.",,No bibliographic reference other than the DOI for this dataset,doi:10.18112/openneuro.ds003061.v1.1.2,,P300,8.0.0,EEG,"Data collection took place at the Meditation Research Institute (MRI) in Rishikesh, India under the supervision of Arnaud Delorme, PhD. The project was approved by the local MRI Indian ethical committee and the ethical committee of the University of California San Diego (IRB project # 090731).
 

 Participants sat either on a blanket on the floor or on a chair for both experimental periods depending on their personal preference. They were asked to keep their eyes closed and all lighting in the room was turned off during data collection. An intercom allowed communication between the experimental and the recording room.
 

 Participants performed three identical sessions of 13 minutes each. 750 stimuli were presented with 70\% of them being standard (500 Hz pure tone lasting 60 milliseconds), 15\% being oddball (1000 Hz pure tone lasting 60 ms) and 15\% being distractors (1000 Hz white noise lasting 60 ms). All sounds took 5 milliseconds to ramp up and 5 milliseconds to ramp down. Sounds were presented at a rate of 1 per second with a random gaussian jitter of standard deviation 25 ms. Participants were instructed to respond to oddball by pressing a key on a keypad that was resting on their lap.
 

 Data collection was performed with an Active Two Biosemi system (Biosemi, Inc.) at 1024Hz and 10-20 standard caps from the same company tailored to the subject’s head size. Stimuli were presented with the psychophysics MATLAB toolbox. The code for presenting stimuli and all the data is made available (see code and stimuli folder). Before making the data public, the raw data were resampled at 256 Hz using the standalone tool provided by Biosemi, then converted to the EEGLAB data format. No further data manipulation was performed.",1,1,1
ds003078,2020-08-16 19:12:06,philippe Domenech,1.0.0,PROBE iEEG,2020-08-17 6:46:03,1,1,11761400000,0.012 TB,159,6,0,0,1.0.0,CC0,"Philippe DOMENECH, Sylvain RHEIMS, Etienne KOECHLIN",We thank Valentin Wyart and Jan Drugowitsch for their help.,,European Research Council Grant ERC-AdG #250106 ===NEMAR-SEP=== IHU CESAME program “Investissements d’Avenir” ANR-10-IBHU-0003,,10.18112/openneuro.ds003078.v1.0.0,"The study (DSI-SEEG protocol, NCT02869698) was approved by the Institutional Review Board (ANSM #2009-A00239-48) and National French science ethic committee (CPP 09-CHUG-12, #0907)",,,"iEEG, MRI","--- version 1.0.0 ---
 

 initial release
 

 Raw iEEG data + pre and post surgery MRI 
 No DOI to data
 No reference to the published study (still in press)",1,1,0
ds004011,2022-01-26 23:26:00,Lina Teichmann,1.0.3,,2022-02-08 2:58:36,1,1,212730000000,198.1 GB,4578,22,22,41,1.6.0,CC0,"Lina Teichmann, Denise Moerel, Anina Rich, Chris Baker",,,,"Teichmann, L., Moerel, D., Rich, A. N., & Baker, C. I. (2021, April 24). The nature of neural object representations during dynamic occlusion. Retrieved from osf.io/tp2ad ===NEMAR-SEP=== Teichmann, L., Moerel, D., Rich, A. N., & Baker, C. I. (2022, January 24). The nature of neural object representations during dynamic occlusion . Retrieved from osf.io/hc25w",doi:10.18112/openneuro.ds004011.v1.0.3,,occlusion,,MEG,"The main folder contains the raw MEG data for all participants in standard bids format. See references.
  
 

 The ‘sourcedata’ folder contains the behavioural data collected during the MEG session as well as the eyetracking data. The data in this folder follows the following trial structure:
  
 

 - sourcedata
  - beh
   - sub-[participant number]
    - sub-[participant number]_task-occlusion_run-[run number]_events.csv: contains all the events for each trial in the MEG session, detailing what was shown on the screen.
    - sub-[participant number]_task-occlusion_run-[run number]_occframes.csv: contains all the stimulus positions for each occlusion trial in the MEG session.
    - sub-[participant number]_task-occlusion_run-[run number]_disframes.csv: contains all the stimulus positions for each disappearance trial in the MEG session.
  - eyetracking
   - sub-[participant number]_Occ.edf: edf file containing the eye positions during the MEG session.
 

 The ‘derivatives’ folder contains the pre-processed MEG data for each participant. The data in this folder follows the following trial structure:
 

 

 - derivatives
  - preprocessed
   - cosmo_p[participant number].mat: cosmomvpa formatted file with the pre-processed data, epoched for each trial, containing the following variables:
    - ds_diss: cosmo data struct containing the disappearance trials epoched relative to stimulus onset (MEG channels)
    - ds_occ: cosmo data struct containing the disappearance trials epoched relative to stimulus onset (MEG channels)
    - ds_loc: cosmo data struct containing the unpredictable position stream trials epoched relative to stimulus onset (MEG channels)
    - ds_eyes_diss: cosmo data struct containing the disappearance trials epoched relative to stimulus onset (eye-x, eye-y, pupil size)
    - ds_eyes_occ: cosmo data struct containing the disappearance trials epoched relative to stimulus onset (eye-x, eye-y, pupil size)
    - ds_eyes_loc: cosmo data struct containing the unpredictable position stream trials epoched relative to stimulus onset (eye-x, eye-y, pupil size)
   - cosmo_p[participant number]_position_epochs.mat: cosmomvpa formatted file with the pre-processed data, epoched relative to each position change, containing the following variables:
    - ds_tiny: cell with two entries. First entry contains the disappearance trials epoched relative to position change. Second entry contains the occlusion trials epoched relative to position change. (MEG channels)
    - ds_tiny_eyes: cell with two entries. First entry contains the disappearance trials epoched relative to position change. Second entry contains the occlusion trials epoched relative to position change. (eye-x, eye-y, pupil size)
 

 

 ------------
 

 References:
 

 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., Moreau, J., Oostenveld, R., Schoffelen, J., Tadel, F., Wexler, J., Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5, 180110. https://doi.org/10.1038/sdata.2018.110",1,0,0
ds003104,2020-08-31 7:20:15,Richard Höchenberger,1.0.0,MNE-somato-data-bids (anonymized),2020-08-31 7:30:43,1,1,349945000,0 TB,12,1,0,0,1.4.0,CC0,"Lauri Parkkonen, Stefan Appelhoff, Alexandre Gramfort, Mainak Jas, Richard Höchenberger",,,,https://mne.tools/stable/overview/datasets\_index.html#somatosensory,10.18112/openneuro.ds003104.v1.0.0,,somato,,"MEG, MRI","MNE-somato-data-bids
 ====================
 

 This dataset contains the MNE-somato-data in BIDS format.
 

 The conversion can be reproduced through the Python script stored in the
 `/code` directory of this dataset. See the README in that directory.
 

 The `/derivatives` directory contains the outputs of running the FreeSurfer
 pipeline `recon-all` on the MRI data with no additional commandline options
 (only defaults were used):
 

 $ recon-all -i sub-01\_T1w.nii.gz -s 01 -all
 

 After the `recon-all` call, there were further FreeSurfer calls from the MNE
 API:
 

 $ mne make\_scalp\_surfaces -s 01 --force
 $ mne watershed\_bem -s 01
 

 The derivatives also contain the forward model `*-fwd.fif`, which was produced
 using the source space definition, a `*-trans.fif` file, and the boundary
 element model (=conductor model) that lives in
 `freesurfer/subjects/01/bem/*-bem-sol.fif`.
 

 The `*-trans.fif` file is not saved, but can be recovered from the anatomical
 landmarks in the `sub-01/anat/T1w.json` file and MNE-BIDS' function
 `get\_head\_mri\_transform`.
 

 See: https://github.com/mne-tools/mne-bids for more information.
 

 Notes on FreeSurfer
 ===================
 the FreeSurfer pipeline `recon-all` was run new for the sake of converting the
 somato data to BIDS format. This needed to be done to change the ""somato""
 subject name to the BIDS subject label ""01"". Note, that this is NOT ""sub-01"",
 because in BIDS, the ""sub-"" is just a prefix, whereas the ""01"" is the subject
 label.",1,1,0
ds003190,2020-09-25 22:01:52,Juan David Chailloux Peguero,1.0.1,Assesment of the visual stimuli properties in P300 paradigm,2020-10-01 5:10:17,1,3,1079430000,0.001 TB,1685,19,0,0,1.2,CC0,"Omar Mendoza-Montoya, Javier M. Antelis",,,,,10.18112/openneuro.ds003190.v1.0.1,,cnos,,"channels, eeg, events","Dataset description:
 

 The database consists of a total of 382 electroencephalographic files from 19 participants. All recordings were collected on channels Fz, Cz, P3, Pz,P4, PO7, PO8 and Oz, according to the 10-20 EEG electrode placement standard, grounded to AFz channel and referenced to right mastoid (M2).
 •Each participant (S1-S19) performed 3 experimental sessions (Session01-Session03) and in each session there are 7 data files.
 •The filenames for these data files are ’Training 4’, ’Training 5 - SF’, ’Training 5 - CF’, ’Training 6’, ’Training 7’, ’Training 8’, and ’Training 9’.
 •The number accompanying the filename indicates the number of stimuli, whereas letters SF and CF for data files with 5 stimuli indicate the type of flash, SF for Standard-Flash of the stimulus and CF for superimposing a yellow smiling Cartoon Face.
 •Note that filenames for data-files with 4, 6, 7, 8, and 9 stimuli do not have a letter and were recorded with the type of flash that provided the greater classification accuracy when using 5 stimuli.
 •Each data file contains the data stream in a 2D matrix where rows correspond to channels and columns correspond to time samples with sampling frequency of 256Hz.
 •There are 10 rows, 1 to 8 for each EEG electrode (in descending order Fz, Cz, P3, Pz, P4, PO7, PO8 and Oz), 9 for time stamps, and 10 for a marker that encode information about the execution of theexperiment.
 

 The marker encodes this information as follows:
 •(i)marker numbers 101, 200, 201, 202 and 203, indicate the beginning and end of the five phases in a block
 •(ii)marker numbers 1, 2, 3, 4, 5, 6, 7, 8 and 9, indicate the symbol that is activated on the screen
 •(iii)each phase of the experiment block is identified with a marker
 •(iv)the phases of one block of the experiment are: Fixation, Target Presentation, Preparation, Stimulation and Rest
 •(iv)in particular the Stimulation phase has a start marker and an end marker",1,1,0
ds003194,2020-09-28 10:00:15,Carlos López,1.0.4,Neuroepo multisession,2020-10-13 0:44:11,1,2,198333000,0 TB,149,15,0,0,1.2.0,CC0,"Maria Luisa Bringas Vega, Lilia Morales Chacon , Ivonne Pedroso Ibanez",The authors will like to thank Centro Internacional de Restauracion Neurologica for the recruitment and neurophysiological evaluation of the patients and the support of the Ministry of Public Health of the Republic of Cuba. The authors are in debt with all the PD patients and their caretakers who volunteered to participate in our study.,,"National Nature and Science Foundation of China (NSFC) No. 61871105, 61673090, 81330032  ===NEMAR-SEP=== CNS Program of UESTC (No. Y0301902610100201)",,doi:10.18112/openneuro.ds003194.v1.0.4,,"task pre nepo, task 6m nepo",,EEG,"The quest for neuroprotection in Parkinson’s disease (PD) has been for new compounds to slow disease progression and stable and non-invasive biomarkers to document their benefits. Neuroepo, a new formulation of EPO with low content of sialic acid reported good results in animal model and tolerance in healthy participants and PD patients. 
 In a double-blind randomized placebo (https://clinicaltrials.gov/ number NCT04110678) twenty-five PD patients were assigned randomly to Neuroepo (n=15) or placebo (n=10) groups we reported the tolerance of the drug. We recorded resting-state EEG before and six months after the administration of the drug. The qualitative analysis of the abnormalities of the EEG was evaluated by two experts using a Likert-type scale and a multivariate item response theory (MIRT) approach was employed to establish the differences between groups in the two times. The quantitative EEG (qEEG) analysis was performed at the sources looking for generators of the neural activity using software VARETA and co-registering the results using the Montreal Neurological Institute Atlas. The statistical analysis between the sources was conducted using a permutation test and later a contrast method using the surfstat software between groups and before vs after condition, with Bonferroni correction for multiple comparisons.
 Here in this repository, we placed the raw EEG in BIDS format (Pernet, C. R. et al. EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Sci. data 6, 103 (2019). For the use of VARETA the qEEG program you can use (Bosch-Bayard, J. et al. A Quantitative EEG Toolbox for the MNI Neuroinformatics Ecosystem: Normative SPM of EEG Source Spectra. Front. Neuroinform. 14, (2020).)
 The EEG dataset from the different stages of processing can be requested to the authors.",1,1,0
ds003195,2020-09-28 10:18:38,Carlos López,1.0.4,Placebo Neuroepo multisession,2020-10-13 0:43:48,1,2,126957000,0 TB,104,10,0,0,1.2.0,CC0,"Maria Luisa Bringas Vega, Lilia Morales Chacon , Ivonne Pedroso Ibanez",The authors will like to thank Centro Internacional de Restauracion Neurologica for the recruitment and neurophysiological evaluation of the patients and the support of the Ministry of Public Health of the Republic of Cuba. The authors are in debt with all the PD patients and their caretakers who volunteered to participate in our study.,,"National Nature and Science Foundation of China (NSFC) No. 61871105, 61673090, 81330032 ===NEMAR-SEP=== CNS Program of UESTC (No. Y0301902610100201)",,doi:10.18112/openneuro.ds003195.v1.0.4,,"task placebo 6m , task placebo pre",,EEG,"The quest for neuroprotection in Parkinson’s disease (PD) has been for new compounds to slow disease progression and stable and non-invasive biomarkers to document their benefits. Neuroepo, a new formulation of EPO with low content of sialic acid reported good results in animal model and tolerance in healthy participants and PD patients. 
 In a double-blind randomized placebo (https://clinicaltrials.gov/ number NCT04110678) twenty-five PD patients were assigned randomly to Neuroepo (n=15) or placebo (n=10) groups we reported the tolerance of the drug. We recorded resting-state EEG before and six months after the administration of the drug. The qualitative analysis of the abnormalities of the EEG was evaluated by two experts using a Likert-type scale and a multivariate item response theory (MIRT) approach was employed to stablish the differences between groups in the two times. The quantitative EEG (qEEG) analysis was performed at the sources looking for generators of the neural activity using software VARETA and co-registering the results using the Montreal Neurological Institute Atlas. The statistical analysis between the sources was conducted using a permutation test and later a contrast method using the surfstat software between groups and before vs after condition, with Bonferroni correction for multiple comparisons.
 Here in this repository we placed the raw EEG in BIDS format (Pernet, C. R. et al. EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Sci. data 6, 103 (2019). For the use of VARETA the qEEG program you can use (Bosch-Bayard, J. et al. A Quantitative EEG Toolbox for the MNI Neuroinformatics Ecosystem: Normative SPM of EEG Source Spectra. Front. Neuroinform. 14, (2020).)
 The EEG dataset from the different stages of processing can be requested to the authors.",1,1,0
ds003343,2020-10-25 1:44:08,Christoph Schneider,2.0.1,Disentangling the percepts of illusory movement and sensory stimulation during tendon vibration in the EEG,2020-11-12 7:14:08,1,1,695587000,0.001 TB,320,20,20,32,1.4.1,CC0,"Christoph Schneider, Renaud Marquis, Jane Johr, Marina Da Silva Lopes, Philippe Ryvlin, Andrea Serino, Marzia De Lucia, Karin Diserens",We want to thank Techno Concept and especially Frederic Albert for sharing their extensive knowledge on the topic and for aiding with the technical setup for the tendon vibration. Further we appreciate the work Sophie Aebischer and Elise Marenghi put into reviewing corresponding literature for their bachelor thesis at the Haute ecole de travail social et de la sante (EESP) Lausanne.,"If you reference this dataset in your publications, please acknowledge its authors and cite the paper specified under 'ReferenceAndLinks'. Also, please include the following message: 'This data was obtained from the Open Neuro database.'","This work was supported by the Fondation CHUV, Lausanne, Switzerland; the Loterie Romande, Lausanne, Switzerland; and Techno Concept, Mane, France.",Insert citation + paper doi,10.18112/openneuro.ds003343.v2.0.1,,fps,,EEG,"This dataset contains the EEG data used for the study: ""Disentangling the percepts of illusory movement and sensory stimulation during tendon vibration in the EEG"" (Schneider, C., Marquis, R., Jöhr, J., Da Lopes Silva, M., Ryvlin, P., Serino, A., De Lucia, M., Diserens, K. Unpublished [fill according to following pattern: Journal (Year). https://doi.org/....])
 

 Participants:
 Twenty healthy participants (twelve female, eight male), age 24.6 ± 3.2 years, all right-handed. All subjects participated voluntarily and consented in writing to the experiment. The study was covered by the ethical protocol No. 142/09 from the Commission cantonale d'éthique de la recherche sur l'être humain (CER -VD) and in agreement with the Declaration of Helsinki.
 

 Experimental setup: 
 The subjects sat comfortably in a chair facing towards their right side so to not see the stimulated left arm, which could have hampered the illusion of movement created during the tendon vibration. While their right arm rested comfortably in the lap, the left arm was supported by a movable forearm rest which allowed two degrees of freedom in the horizontal plane. The reason for this was that proprioceptive feedback of the arm touching an immobile object can prevent the motor illusion from forming.
 Subjects wore an EEG cap with built-in wireless amplifier (g.tec Nautilus, g.tec medical engineering, Graz, Austria) with 16 electrodes covering the sensorimotor cortex in the international 10-10 system at positions (Fz, FC3, FC2, FCz, FC2, FC4, C3, C1, Cz, C2, C4, CP3, CP1, CPz, CP2, CP4). The signals were recorded at 500Hz with a hardware-implemented bandpass filter between 0.1 and 100 Hz and sent to a computer in the same room. The reference electrode was placed on the right earlobe.
 Tendon vibration was achieved with electromechanical wireless vibrators set into a soft, elastic brace on the left elbow joint (Vibramoov, Techno Concept, Manosque, France). The left arm was chosen since it was demonstrated that illusions start faster and are more vivid in the non-dominant extremity. One vibrator was sitting against the distal biceps tendon and the other against the distal triceps tendon on the same arm. Time information about the beginning of each stimulation was sent via a cable link to the computer and stored with the EEG data.
 

 Study protocol:
 EEG was recorded continuously while delivering stimulation sequences consisting of two different vibration types. The first elicited an illusion of elbow extension and was produced by vibrating the distal biceps tendon at 90Hz and the distal triceps tendon at 50Hz. The second produced only a vibration sensation without any movement illusion and consisted of stimulating both tendons at 70Hz. So, the average frequency of stimulation delivered to the agonist-antagonist pair was the same between conditions, but one condition was designed to induce a clear illusion and the other no illusion at all (control).
 Each stimulation lasted three seconds and consisted of one second of linear frequency ramp-up, one second of a stable frequency interval and one second of linear frequency ramp-down. The linear ramps started and ended 10 Hz below the target frequency for each stimulation type. The amplitude of the vibration was 2-3 mm. These parameters were based on Romaiguère et al. (2003) and the perception of illusory movement across all subjects was ensured in a pre-screening procedure. This setting was kept constant throughout the whole recording session. 
 Each subject underwent three blocks of 72 vibrations (36 illusion, 36 control), arranged randomly and different for each block. The same stimulus sequences were employed for each participant. Inter stimulus intervals varied between one and three seconds and were randomized within and between blocks in order to minimize stimulus onset anticipation.",1,1,0
ds003352,2020-11-03 20:36:34,Conway Lab,1.0.0,,2020-11-03 21:07:28,1,2,230093000000,0.23 TB,629,18,17,26,1.2.2,CC0,"Katherine Hermann, Isabelle Rosenthal, Shridhar R. Singh, Dimitrios Pantazis, Bevil R. Conway",,,,"Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896 ===NEMAR-SEP=== Holdgraf, C., Appelhoff, S., Bickel, S., Bouchard, K., D'Ambrosio, S., David, O., … Hermes, D. (2019). iEEG-BIDS, extending the Brain Imaging Data Structure specification to human intracranial electrophysiology. Scientific Data, 6, 102. https://doi.org/10.1038/s41597-019-0105-7",10.18112/openneuro.ds003352.v1.0.0,,ColorSpirals,,MEG,"Stimuli include eight different square wave spiral gratings subtending 10 degrees of visual angle as well as the color words ""blue"" and ""green."" The color words appeared as white on a gray background. Each stimulus appeared on the screen for 116 ms. The triggers or event ID's of each stimulus are as follows:
 

 1 - Light Pink Spiral
 2 - Dark Pink Spiral
 3 - Light Blue Spiral
 4 - Dark Blue Spiral
 5 - Light Green Spiral
 6 - Dark Green Spiral
 7 - Light Orange Spiral
 8 - Dark Orange Spiral
 9 - ""green""
 10 - ""blue""",1,1,0
ds003374,2020-11-11 18:36:10,Ece Boran,1.1.1,Dataset of neurons and intracranial EEG from human amygdala during aversive dynamic visual stimulation,2020-11-11 18:39:48,0,1,175461000,0 TB,104,9,19,48,1.4.0,CC0,"Tommaso Fedele, Ece Boran, Valeri Chirkov, Peter Hilfiker, Thomas Grunwald, Lennart Stieglitz, Hennric Jokeit, Johannes Sarnthein","We thank the physicians and the staff at Schweizerische Epilepsie-Klinik for their assistance and the subjects for their participation. We acknowledge grants awarded by the Swiss National Science Foundation (SNSF 320030\_156029 to J.S.), Mach-Gaensslen Stiftung (to J.S.), Stiftung für wissenschaftliche Forschung an der Universität Zürich (to J.S.), Forschungskredit der Universität Zürich (to T.F.) and Russian Foundation for Basic Research (RFBR 20-015-00176 A to T.F.). The funders had no role in the design or analysis of the study.",Please cite this paper: https://doi.org/10.1016/j.neuroimage.2020.116705,"Swiss National Science Foundation, SNSF 320030\_156029 ===NEMAR-SEP=== Mach-Gaensslen Stiftung ===NEMAR-SEP=== Stiftung für wissenschaftliche Forschung an der Universität Zürich ===NEMAR-SEP=== Forschungskredit der Universität Zürich ===NEMAR-SEP=== Russian Federation for Basic Research, RFBR 20-015-00176 A","Fedele, T., Tzovara, A., Steiger, B., Hilfiker, P., Grunwald, T., Stieglitz, L., Jokeit, H., Sarnthein, J., 2020. The relation between neuronal firing, local field potentials and hemodynamic activity in the human amygdala in response to aversive dynamic visual stimuli. Neuroimage 213, 116705. https://doi.org/10.1016/j.neuroimage.2020.116705 ===NEMAR-SEP=== Fedele, T., Boran, E., Chirkov, V., Hilfiker, P., Grunwald, T., Stieglitz, L., Jokeit, H., Sarnthein, J., 2020. Dataset of neurons and intracranial EEG from human amygdala during aversive dynamic visual stimulation. G-node. https://gin.g-node.org/USZ\_NCH/Human\_Amygdala\_MUA\_sEEG\_FearVideo",10.18112/openneuro.ds003374.v1.1.1,Kantonale Ethikkommission Zürich (PB-2016-02055),jokeit,,iEEG,"# Dataset of neurons and intracranial EEG from human amygdala during aversive dynamic visual stimulation
 

 ## Summary
 We present an electrophysiological dataset collected from the amygdalae of nine subjects attending a visual dynamic stimulation of emotional aversive content. The subjects were patients affected by epilepsy who underwent preoperative invasive monitoring in the mesial temporal lobe. Subjects were presented with dynamic visual sequences of fearful faces (aversive condition), interleaved with sequences of neutral landscapes (neutral condition).
 

 We provide the recordings of intracranial EEG (iEEG) and metadata related to the task, subjects, sessions and electrodes in the BIDS standard.
 

 We also provide a more extended version of the dataset that includes neuronal spike times and waveforms in the NIX standard under the folder ""bidsignore/data\_NIX"". This extended dataset is also available in G-Node at https://gin.g-node.org/USZ\_NCH/Human\_Amygdala\_MUA\_sEEG\_FearVideo/.
 

 This dataset allows the investigation of amygdalar response to dynamic aversive stimuli at multiple spatial scales, from the macroscopic EEG to the neuronal firing in the human brain.
 

 ## Repository structure
 

 ### Main directory
 Contains metadata in the BIDS standard. 
 

 ### Directories sub-**
 Contains folders for each subject, named sub-<subject number>.
 

 ### Directory bidsignore
 Contains data in the NIX standard, and metadata files. Subject\_Characteristics.pdf describes subjects and NIX\_File\_Structure.pdf describes the structure of the NIX files.
 

 #### Directory code\_MATLAB
 Contains MATLAB code for loading the data and generating the publication figures. Main\_Load\_NIX\_Data.m contains code snippets for reading NIX data and task related information. Main\_Plot\_Figures.m uses the functions Figure\_2.m and Figure\_3.m to generate figures.
 

 Required dependencies to run the script Main\_Load\_NIX\_Data.m:
 

 * [Nix-mx v1.4.1](https://github.com/G-Node/nix-mx/)
 

 Required dependencies to run the script Main\_Plot\_Figures.m:
 

 * [Nix-mx v1.4.1](https://github.com/G-Node/nix-mx/)
 * [Gramm](https://github.com/piermorel/gramm/)
 * [FieldTrip](http://www.fieldtriptoolbox.org/download/)
 

 #### Directory data\_NIX
 Contains nix files for each session of the task. Each file is named with the format:  
 Data\\_Subject\\_\<subject number>\\_Session\\_\<session number>.h5
 

 ## Support
 For questions on the dataset or the task, contact Johannes Sarnthein at [johannes.sarnthein@usz.ch](johannes.sarnthein@usz.ch).",1,1,0
ds003380,2020-11-13 5:11:59,Martin Frasch,1.0.0,"Corticothalamic communication under analgesia, sedation and gradual ischemia: a multimodal model of controlled gradual cerebral ischemia in pig",2020-11-13 5:20:09,1,1,20614200,0 TB,5,1,0,0,1.1.1,CC0,"Martin G. Frasch, Bernd Walter, Chrstophe L. Herry, Reinhard Bauer",,,,arXiv:2002.09154,10.18112/openneuro.ds003380.v1.0.0,,,,EEG,"This sedation, ischemia, recovery experiment contains 11 animals (juvenile pigs).
 

 Animals were surgically instrumented, and then monitored under sedation states 1-5 (isoflurane, fentanyl, propofol), followed by 1 or 2 episodes of gradual ischemia (states 6 and 8) and recovery (recovery 1 = state 7, between state 6 and 8; recovery 2, after state 8, corresponding to states 9-12).
 

 Two crude groups are indicated: 
 1) sedation - animals had no ischemia and 
 2) ischemia - animals had sedation, followed by ischemia episodes and followed by recovery.
 

 The scientific article (see Reference) contains all methodological details.
 - Martin Frasch and Reinhard Bauer, October 2, 2020
 

 PS. Sub-12 folder is to be ignored. It was added to satisfy the BIDS validation algorithm.",1,1,0
ds003392,2020-11-20 19:39:16,Alexandre Gramfort,1.0.4,NeuroSpin hMT+ Localizer DATA (MEG & aMRI),2020-11-20 20:41:26,1,1,10818500000,0.011 TB,159,11,0,0,?,CC0,"Nicolas Zilber, Philippe Ciuciu, Alexandre Gramfort, Leila Azizi, Virginie van Wassenhove","We are grateful to the NeuroSpin nursing staff for their help in recruiting and preparing participants for MEG data acquisition, and to Antoine Grigis for help with the anonymization of the MRIs.","Please cite: 
 

 Zilber, N., Ciuciu, P., Gramfort, A., Azizi, L., & Van Wassenhove, V. (2014). Supramodal processing optimizes visual perceptual learning and plasticity. Neuroimage, 93, 32-46.",This work was supported by a Marie Curie IRG-249222 and an ERC-StG-263584 to V.vW and an ANR Schubert ANR-0909-JCJC-071 to P.C.,https://doi.org/10.1016/j.neuroimage.2014.02.017,10.18112/openneuro.ds003392.v1.0.4,,"localizer, noise",,"MEG, MRI","Dataset description: Magnetoencephalography (MEG) dataset recorded during a hMT+ (human visual motion area) localizer task
 

 Published in: 
 Zilber, N., Ciuciu, P., Gramfort, A., Azizi, L., & Van Wassenhove, V. (2014). Supramodal processing optimizes visual perceptual learning and plasticity. Neuroimage, 93, 32-46.
 

 Data curation: Sophie Herbst, Alexandre Gramfort
 

 This MEG dataset was prepared in the Brain Imaging Data Structure (MEG-BIDS, Niso et al. 2018) format using MNE-BIDS (Appelhoff et al. 2019).
 

 The dataset contains 10 of the 12 participants from the vision-only training group. 
 

 Two participants were removed, one due to problems with the trigger channel, and one due to different settings in the acquisition preventing us from processing the dataset without prior adjustment. 
 

 

 ## EXPERIMENT
 

 Participants were presented with a cloud of moving dots, always starting with incoherent movement (up or down result in equal display, due to the incoherence). 
 After 500 ms, the movement became coherent in 50\% of the trials (95\% coherence, up or down) and remained incoherent in the other 50\%, lasting for 1000 ms. Participants were instructed to passively view the stimuli for a total of 120 trials. 
 

 Events: 
 

 1: coherent / down
 2: coherent / up
 3: incoherent / down
 4: incoherent / up
 

 

 ## MEG
 

 Brain magnetic fields were recorded in a MSR using a 306 MEG system (Neuromag Elekta LTD, Helsinki). MEG recordings were sampled at
 2 kHz and band-pass filtered between 0.03 and 600 Hz. 
 

 Four head position coils (HPI) measured the head position of participants before each
 block; three fiducial markers (nasion and pre-auricular points) were
 used for digitization and anatomicalMRI (aMRI) immediately following
 MEG acquisition. 
 

 Electrooculograms (EOG, horizontal and vertical eye
 movements) and electrocardiogram (ECG) were simultaneously recorded.
 Prior to the session, 5 min of empty room recordings was acquired
 for the computation of the noise covariance matrix.
 

 Bad MEG channels were marked manually.
 

 

 ## MRI
 

 The T1 weighted aMRI was recorded using a 3-T Siemens Trio MRI
 scanner. Parameters of the sequence were: voxel size: 1.0 × 1.0 ×
 1.1 mm; acquisition time: 466 s; repetition time TR = 2300 ms; and
 echo time TE = 2.98 ms
 

 

 ## References
 

 Zilber, N., Ciuciu, P., Gramfort, A., Azizi, L., & Van Wassenhove, V. (2014). Supramodal processing optimizes visual perceptual learning and plasticity. Neuroimage, 93, 32-46.
 

 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., Moreau, J., Oostenveld, R., Schoffelen, J., Tadel, F., Wexler, J., Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5, 180110. http://doi.org/10.1038/sdata.2018.110",1,1,0
ds003420,2020-12-04 13:06:19,EBN Lab,1.0.2,Dataset 1,2020-12-08 22:53:32,1,2,50565100000,0.051 TB,875,23,19,40,1.2,CC0,"Ahmad Mheich, Olivier Dufor, Sahar Yassine, Aya Kabbara, Arnaud Biraben, Fabrice Wendling, Mahmoud Hassan","The dataset 1 has received a French government support granted to the CominLabs excellence laboratory and managed by the National Research Agency in the Investing for the Future program under reference ANR-10-LABX-07-01. We also thank the European Research Council for the ERC-2011-ADG - Grant Agreement N 290901 ? Acronym NEUCOD. Dataset 1 and dataset 2 were also supported by the Rennes University Hospital (dataset 1 COREC Project named conneXion, 2012-14; dataset 2: COREC Project named BrainGraph, 2015-17). The study was also funded by the National Council for Scientific Research (CNRS) in Lebanon. Authors would also like to thank the Lebanese Association for Scientific Research (LASER) for its support and the Institute of Clinical Neuroscience of Rennes (project named EEGCog).",,,"Kabbara, A., Falou, W. E., Khalil, M., Wendling, F. & Hassan, M. The dynamic functional core network of the human brain at rest. Sci. Rep. 7, 2936 (2017) ===NEMAR-SEP=== Hassan, M. et al. Dynamic reorganization of functional brain networks during picture naming. Cortex 73, 276?288 (2015). ===NEMAR-SEP=== Mheich, A. et al. Spatiotemporal analysis of brain functional connectivity. in 6th European Conference of the International Federation for Medical and Biological Engineering 934?937 (Springer, 2015). ===NEMAR-SEP=== Mheich, A. et al. SimiNet A Novel Method for Quantifying Brain Network Similarity. IEEE Trans. Pattern Anal. Mach. Intell. 40, 2238?2249 (2018). ===NEMAR-SEP=== Rizkallah, J. et al. Dynamic reshaping of functional brain networks during visual object recognition. J. Neural Eng. 15, 056022 (2018).",10.18112/openneuro.ds003420.v1.0.2,,,,EEG,"# Dataset 1
 

 ##  Presentation
 

  This dataset was collected between 2012 and 2013 in Rennes (France) during two conditions (visual naming and spelling tasks).
  The dataset consists of naming and spelling the names of visually presented objects. The data was collected in the Rennes University Hospital. This experiment was approved by an independent ethics committee and authorized by the French institutional review board (IRB): ""Comite de Protection des Personnes dans la Recherche Biomedicale Ouest V"" (CCPPRB-Ouest V).
  This study was registered under the name ""conneXion"" and the agreement number: 2012- A01227-36.
  
 

 ### Participants
 

  Twenty-three right-handed healthy volunteers of whom 12 females, with an age range between
  19 and 40 years (mean age 28 year),and 11 males with an age range between 19 and 33 years (mean age 23 years) participated in this study. (See participants.json and participants.tsv for more details)
 

 

 #### Experiment
  * The experiment begins with the verification of inclusion/exclusion criteria.
  * The participants read the information notice and the consent form. 
  * Then they sign two questionnaires. 
  * One subject -->Two conditions (naming and spelling)--> two runs for each condition.
  * Each run contains 74 stimuli.
  * The spelling task always follow the naming task and its instruction was not given before the naming task was completed to avoid any reminiscence of words orthographic structures
  * Each run contains balanced numbers of animals and objects as well as long and short words.
  * Pictures are presented on a screen using a computer and the experimental paradigm is presented using E-prime Psychology Software Tools. 
  * The responses produced by the participants were collected via a Logitech microphone and analyzed to detect onsets of speech using Praat v5.3.13(University of Amsterdam, 1012VT Amsterdam, The Netherlands).
 

 #### EEG acquisition
 

  * HD-EEG system (EGI, Electrical Geodesic Inc., 256 electrodes) 
  * Sampling frequency: 1000Hz
  * Impedances were kept below 5k
 

 

 ## Contact
  * If you have any questions or comments, please contact: 
  * Ahmad Mheich: mheich.ahmad@gmail.com",1,1,0
ds003421,2020-12-04 20:05:14,EBN Lab,1.0.2,Dataset 2,2020-12-08 23:04:00,1,4,42498500000,0.042 TB,938,20,20,40,1.2,CC0,"Ahmad Mheich, Olivier Dufor, Sahar Yassine, Aya Kabbara, Arnaud Biraben, Fabrice Wendling, Mahmoud Hassan","The dataset 2 has received a French government support granted to the CominLabs excellence laboratory and managed by the National Research Agency in the Investing for the Future program under reference ANR-10-LABX-07-01. We also thank the European Research Council for the ERC-2011-ADG - Grant Agreement N 290901 ? Acronym NEUCOD. Dataset 1 and dataset 2 were also supported by the Rennes University Hospital (dataset 1 COREC Project named conneXion, 2012-14; dataset 2: COREC Project named BrainGraph, 2015-17). The study was also funded by the National Council for Scientific Research (CNRS) in Lebanon. Authors would also like to thank the Lebanese Association for Scientific Research (LASER) for its support and the Institute of Clinical Neuroscience of Rennes (project named EEGCog).",,,"Kabbara, A., Falou, W. E., Khalil, M., Wendling, F. & Hassan, M. The dynamic functional core network of the human brain at rest. Sci. Rep. 7, 2936 (2017) ===NEMAR-SEP=== Hassan, M. et al. Dynamic reorganization of functional brain networks during picture naming. Cortex 73, 276?288 (2015). ===NEMAR-SEP=== Mheich, A. et al. Spatiotemporal analysis of brain functional connectivity. in 6th European Conference of the International Federation for Medical and Biological Engineering 934?937 (Springer, 2015). ===NEMAR-SEP=== Mheich, A. et al. SimiNet A Novel Method for Quantifying Brain Network Similarity. IEEE Trans. Pattern Anal. Mach. Intell. 40, 2238?2249 (2018). ===NEMAR-SEP=== Rizkallah, J. et al. Dynamic reshaping of functional brain networks during visual object recognition. J. Neural Eng. 15, 056022 (2018).",10.18112/openneuro.ds003421.v1.0.2,,PicturesNaming,,EEG,"# Dataset 2
 

 ##  Presentation
 

  This dataset was collected between 2014 and 2017 in Rennes (France) during four conditions (resting state, visual naming, auditory naming and working memory tasks).
  All participants provided a written informed consent to participate in this study which was approved
  by an independent ethics committee and authorized by the IRB ""Comite de Protection des Personnes
  dans la Recherche Biomedicale Ouest V"" (CCPPRB-Ouest V). 
  The study name was ""Braingraph"" and study agreement number was 2014-A01461-46.
  Its promoter was the Rennes University Hospital.
 

  
 ### Participants
 

  Twenty right-handed healthy volunteers (10 females, 10 males, mean age 23 years) participated 
  in this experiment. (See participants.json and participants.tsv for more details)
 

 

 #### Experiment
 

  * The experiment begins with the verification of inclusion/exclusion criteria.
  * The participants read the information notice and the consent form. 
  * Then they sign two questionnaires. 
  * One subject -->four conditions (resting state, visual naming, auditory naming and working memory).
  * Resting state--> subject asked to relax for 10 min with their eyes open.
  * Visual naming-->subject asked to name 80 pictures. 40 scrambled pictures were presented and participant? were asked to say nothing.
  * Auditory naming--> subject asked to name 80 different sounds.
  * Memory--> 80 pictures were displayed of which 40 have already been shown in the naming task. New pictures and already seen pictures randomly appeared on the screen and participants have to indicate if they have seen them before by pressing a button or not. 
 

 

 

 #### EEG acquisition
 

  * HD-EEG system (EGI, Electrical Geodesic Inc., 256 electrodes) 
  * Sampling frequency: 1000Hz
  * Impedances were kept below 5k
 

 

 ## Contact
  * If you have any questions or comments, please contact: 
  * Ahmad Mheich: mheich.ahmad@gmail.com",1,1,0
ds003458,2021-01-04 22:31:04,James F Cavanagh,1.1.0,ThreeArmedBandit,2021-01-04 22:45:12,1,1,5065150000,0.005 TB,201,23,18,24,1.1.1,CC0,James F Cavanagh jcavanagh@unm.edu,,,,PMID: 25676913,10.18112/openneuro.ds003458.v1.1.0,,ThreeArmedBandit,,EEG,"Healthy control college students. 23 subjects completed the 3-armed bandit task with oscillating probabilities. For example, the 'blue' stim would slowly move from 20\% reinforcing to 90\% then back to 20 over many trials. The other 'red' and 'green' stims would move similarly, but in different phase. See Fig 1 of the paper. This makes the task great for investigating reward processing & reward prediction error in the service of novel task set generation.
 

 Task included in Matlab programming language.  
 

 Data collected in 2014 in the Cognitive Rhythms and Computation Lab, University of New Mexico. 
 

 I also collected Corrugator EMG (may be labeled EKG) and Skin Conductance on most people. But quality was dubious so I never did much with it. Check .xls sheet under code folder.
 

 Some pre-processing scripts are included in code folder as well. 
 

 - James F Cavanagh 01/04/2021",1,1,0
ds003474,2021-01-15 19:36:31,James F Cavanagh,1.1.0,ProbabilisticSelection,2021-02-05 21:40:47,1,1,17867700000,0.018 TB,1014,122,18,24,1.1.1,CC0,James F Cavanagh jcavanagh@unm.edu,,PMID: 31149639,,PMID: 31149639,10.18112/openneuro.ds003474.v1.1.0,,ProbabilisticSelection,,EEG,"Probabilistic selection task with 122 college-age participants. Task included in DMDX programming language. Data collected circa 2008-2010 in John J.B. Allen lab at U Arizona. Subjects scored reliably high or low in Beck Depression Inventory. Some have been clinically interviewed. For some subjects (maybe all?), HEOG and VEOG may be mis-labeled as the other. Some files have had some channels interpolated already. There are no raw data to revert to instead... Note subj 544 is not used b/c they had unstable BDI from pre-assessment to test session. Code is included to re-create this paper: DOI: 10.1162/cpsy\_a\_00024
  - James F Cavanagh 01/11/2021",1,1,0
ds003478,2021-01-18 18:31:12,James F Cavanagh,1.1.0,Rest,2021-01-18 19:50:00,1,1,11430500000,0.011 TB,1959,122,18,24,1.1.1,CC0,James F Cavanagh jcavanagh@unm.edu,,,,PMID: 31149639,10.18112/openneuro.ds003478.v1.1.0,,Rest,,EEG,"Resting EEG data with 122 college-age participants. These are the same participants as the Openneuro prob selection task. Subjects have the same task IDs, so you could match them up if you like.  Task included in DMDX programming language, with instructions for eyes open & eyes closed Triggers included for instrucgted one minute spans for open or closed, e.g. : OCCOCO or COOCOC  Data collected circa 2008-2010 in John J.B. Allen lab at U Arizona. Subjects scored reliably high or low in Beck Depression Inventory. Some have been clinically interviewed. See .xls sheet. For some subjects (maybe all?), HEOG and VEOG may be mis-labeled as the other. Some files have had some channels interpolated already. There are no raw data to revert to instead... I have never even looked at the last rest run; no idea how it looks. First rest run was high quality though. The first 6 mins happened immedately after EEG hook-up. The second 6 minutes came after task performance (about 1 hour later) 516 has no rest2. 544 was unused in all anlayses due to unstable BDI between mass assessment and lab assessment (1-4 months)  - James F Cavanagh 01/18/2021",1,1,0
ds003483,2021-01-21 21:01:58,Luis Fernando Antón Toro,1.0.2,raz\_leon,2021-01-24 10:37:19,1,1,26285800000,0.026 TB,209,21,0,0,BIDS 1.2.0,CC0,"Cognitive and Computational Neuroscience Laboratory (UPM - UCM)., PI: Fernando Maestu., PI: Carmen Requena, PI: Francisco Salto Alemany",,,,,10.18112/openneuro.ds003483.v1.0.2,,"induction, deduction",,MEG,,1,1,0
ds003490,2021-01-27 17:56:02,James F Cavanagh,1.1.0,Rest,2021-01-27 18:22:50,1,2,6276600000,0.006 TB,607,50,48,84,1.1.1,CC0,James F Cavanagh jcavanagh@unm.edu,,,,PMID: 29294412,10.18112/openneuro.ds003490.v1.1.0,,Rest,,EEG,"Rest and 3 stimulus auditory oddball data with 25 Parkinson patients and 25 matched controls. Some more subjects are included in the .xls sheet that don't have EEG data in this task. C'est la vie. PD came in twice separated by a week, either ON or OFF medication. CTL only came in once. Task included in Matlab programming language, with instructions for eyes open & eyes closed Triggers included for instructed one minute spans for open or closed (OC or CO) before the task. Data collected circa 2015 in Cognitive Rhythms and Computation Lab at University of New Mexico. Subjs also had an acceleromter taped to their most tremor affected hand. X, Y, Z dimensions recorded throughout. Check the .xls sheet under code folder for more meta data.  Also code to re-create the paper. - James F Cavanagh 01/18/2021",1,1,0
ds003498,2021-02-01 13:44:55,Adam Li,1.1.1,iEEG Interictal Asleep HFO Dataset,2021-03-10 15:56:12,0,1,48030900000,0.048 TB,2335,20,0,0,1.4.0,CC0,"Fedele T, Krayenbühl N, Hilfiker P, Adam Li, Sarnthein J.","Adam Li (github: adam2392) converted the dataset from the original format into BIDS. He used mne-bids to do so, and uses the dataset to validate mne-hfo, a Python open-source implementation of HFO detection algorithms. See 10.5281/zenodo.4485036.",,,,doi:10.18112/openneuro.ds003498.v1.1.1,,,,iEEG,"Zurich iEEG HFO Dataset
 ====================
 

 This dataset was obtained from the publication [1].
 

 There are 20 subjects with HFO events. We converted the dataset into BIDS format. The channels that were included in the resected region and channels excluded from analysis are included in the clinical Excel file under the ``sourcedata/`` directory. The channels were extracted from the Supplementary table at: https://static-content.springer.com/esm/art\%3A10.1038\%2Fs41598-017-13064-1/MediaObjects/41598\_2017\_13064\_MOESM1\_ESM.pdf.
 

 The original uploader: adam2392 obtained explicit permission from the authors of the dataset to upload this to openneuro. Adam worked on an open-source Python implementation of HFO detection algorithms, and uses this dataset in validation. Even though the publication involves a ``Morphology`` HFO detector, we have implemented our interpretation of the RMS, LineLength and Hilbert detectors in the [mne-hfo repository] (https://github.com/mne-tools/mne-hfo) [2].For more information, visit: https://github.com/mne-tools/mne-hfo.
 

 # Note from the paper
 ""We excluded all electrode contacts where electrical stimulation evoked motor or language responses (Table S1).
 In TLE patients, we included only the 3 most mesial bipolar channels"".
 

 BIDS Conversion
 ------------------------
 

 MNE-BIDS was used to convert the dataset into BIDS format. The code inside `code/` was used to generate the
 data.
 

 HFO Events From Original Paper
 ----------------------------------------------
 

 The HFO events from the original paper that were validated and detected are stored in the `*events.tsv` file per dataset run. The format is similar to ``mne-hfo`` and can be easily read in using ``mne-bids`` and/or ``mne-python``.
 

 Each row in the events.tsv file corresponds to a HFO detected in the original source dataset. The ``trial\_type`` column stores the information pertaining type of HFO (e.g. ``ripple``, ``fr`` for fast ripple, or ``frandr`` for fast ripple and ripple). The channel name (possibly in bipolar reference) is `""-""` character delimited and appended to the type of HFO with a `""\_""` separating. For example: ``<hfo\_type>\_<channel\_name>`` is the form. 
 

 Reference Dataset
 ---------------------------
 

 The following website was where the original data was downloaded.
 

 http://crcns.org/data-sets/methods/ieeg-1
 

 References
 ----------------
 

 [1] Fedele T, Burnos S, Boran E, Krayenbühl N, Hilfiker P, Grunwald T, Sarnthein J.
 Resection of high frequency oscillations predicts seizure outcome in the individual patient.
 Scientific Reports. 2017;7(1):13836.
 https://www.nature.com/articles/s41598-017-13064-1
 doi:10.1038/s41598-017-13064-1
 

 [2] Dataset meta analysis with mne-hfo. 10.5281/zenodo.4485036
 

 [3] Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 [4] Holdgraf, C., Appelhoff, S., Bickel, S., Bouchard, K., D'Ambrosio, S., David, O., … Hermes, D. (2019). iEEG-BIDS, extending the Brain Imaging Data Structure specification to human intracranial electrophysiology. Scientific Data, 6, 102. https://doi.org/10.1038/s41597-019-0105-7",1,1,0
ds003505,2021-02-04 8:01:40,Sinergia Consortium,1.1.2,VEPCON: Source imaging of high-density visual evoked potentials with multi-scale brain parcellations and connectomes,2021-03-11 7:46:16,1,1,31106000000,0.031 TB,310,20,20,32,1.6.0,CC0,"David Pascucci, Sebastien Tourbier, Joan Rue-Queralt, Margherita Carboni, Patric Hagmann, Gijs Plomp",,"Please cite the associated scientific data paper: Pascucci, D., Tourbier, S., Rué-Queralt, J. et al. Source imaging of high-density visual evoked potentials with multi-scale brain parcellations and connectomes. Sci Data 9, 9 (2022). https://doi.org/10.1038/s41597-021-01116-1","This research was supported by Swiss National Science Foundation grants PP00P1\_183714, PP00P1\_190065 and CRSII5-170873.",https://doi.org/10.1038/s41597-021-01116-1 ===NEMAR-SEP=== https://www.biorxiv.org/content/10.1101/2021.03.16.435599v1,doi:10.18112/openneuro.ds003505.v1.1.2,,"faces, motion",,"EEG, MRI","# VEPCON: Source imaging of high-density visual evoked potentials with multi-scale brain parcellations and connectomes
 

 ## Overview
 

 The multimodal dataset VEPCON follows the BIDS standard and provides raw data of high-density EEG, structural MRI and diffusion weighted images (DWI) recorded in 20 participants.
 

 Visual evoked potentials were recorded while participants discriminated briefly presented faces from scrambled faces (`task-faces`), or coherently moving stimuli from incoherent ones (`task-motion`). Note that raw EEG data for `sub-05` (for both `task-faces` and `task-motion`) and for `sub-15` (for `task-motion`) were discarded because of excessive motion. MRI and DWI were recorded in a separate session from the same participants. 
 

 VEPCON also contains data derivatives that follow as close as possible the BIDS derivatives specifications. It includes in particular: pre-processed EEG of single trials in each condition, behavioral measures, structural MRIs, Freesurfer `7.1.1` outputs of defaced MRIs, individual brain parcellations at 5 spatial resolutions (83 to 1015 regions), and corresponding structural connectomes based on fiber count, fiber density, average fractional anisotropy and mean diffusivity maps. In addition, Freesurfer's outputs include a `bem/` folder that contains all files generated by MNE to describe the Boundary Element Model (BEM) based on Freesurfer's surfaces estimated from the original undefaced structural MRIs. Finally, VEPCON also provides EEG inverse solutions for source imaging based on individual anatomy, and Python and Matlab code for deriving time-series of activity in each brain region, at each parcellation level.
 

 We believe this dataset can contribute to multimodal methods development, studying structure-function relations, as well as unimodal optimization of source imaging and graph analysis, among many other possibilities.
 

 All code supporting the dataset can be found in the `code/` folder.",1,1,0
ds003506,2021-02-05 19:08:26,James F Cavanagh,1.1.0,ReinforcementLearning,2021-02-05 20:07:58,1,2,17399700000,0.017 TB,724,56,48,84,1.1.1,CC0,"James F Cavanagh, Darin Brown",,,,PMID: 31704082,10.18112/openneuro.ds003506.v1.1.0,,ReinforcementLearning,,EEG,"Reinforcement learning task with 28 Parkinson patients and 28 matched controls. Task with volitional and instucted choices.  Task adapted from here: https://doi.org/10.1016/j.neuron.2014.06.035. Beh data first published here: 10.1016/j.cortex.2017.02.021. EEG published here: 10.1016/j.brainres.2019.146541.  PD came in twice separated by a week, either ON or OFF medication. CTL only came in once. Task included in Matlab programming language.  Data collected circa 2015 in Cognitive Rhythms and Computation Lab at University of New Mexico. Subjs also had an acceleromter taped to their most tremor affected hand. X, Y, Z dimensions recorded throughout. Check the .xls sheet under code folder for more meta data. Some Matlab analytic scripts are included, but I didnt ensure that these are complete. Also behavioral files from the task, which contain more trial-specific information than the triggers.  
  - James F Cavanagh 02/05/2021",1,1,0
ds003509,2021-02-08 20:45:46,James F Cavanagh,1.1.0,SimonConflict,2021-02-08 22:26:43,1,2,23987500000,0.024 TB,1080,56,48,84,1.1.1,CC0,"James F Cavanagh, Arun Singh, Kumar Narayanan",,,,PMID: 29802866,10.18112/openneuro.ds003509.v1.1.0,,SimonConflict,,EEG,"Simon conflict task with cost of conflict reinforcement manipulation.  28 Parkinson patients and 28 matched controls. Task adapted from here: 10.1038/ncomms6394. Beh data first published here: 10.1016/j.cortex.2017.02.021. EEG published here: 10.1016/j.neuropsychologia.2018.05.020.  PD came in twice separated by a week, either ON or OFF medication. CTL only came in once. Task included in Matlab programming language.  Data collected circa 2015 in Cognitive Rhythms and Computation Lab at University of New Mexico. Subjs also had an acceleromter taped to their most tremor affected hand. X, Y, Z dimensions recorded throughout. Check the .xls sheet under code folder for more meta data.  Triggers are complicated. See CC\_Triggers.mat under code folder. Many analysis scripts are included; no idea how these hold up.  Many are old. - James F Cavanagh 02/08/2021",1,1,0
ds003516,2021-02-12 15:37:08,Björn Holtze,1.1.3,Attended speaker paradigm (own name in ignored),2021-02-12 17:59:37,1,1,8152000000,0.008 TB,219,25,18,50,v5.2,CC0,"Bjoern Holtze, Manuela Jaeger, Stefan Debener, Kamil Adiloglu, Bojana Mirkovic",,,,https://doi.org/10.3389/fnins.2021.643705,doi:10.18112/openneuro.ds003516.v1.1.3,,AttendedSpeakerParadigmOwnName,,EEG,"Within this experiment 25 participants performed a two-competing speaker paradigm. Participants were instructed to either attend to the left or right audio book. The paradigm consisted of five 10-minute blocks of audio book presentation. In each 10-minute block the participants own name was presented 10 times, embedded within the to-be-ignored audio book. A 10-minute block could either be presented in the omnidirectional condition (both audio books were presented equally loud) or within the beamforming condition (the to-be-attended audio book was louder than the to-be-ignored audio book). The first 10-minute block was always presented in the omnidirectional condition whereas the conditions were alternated for the later four blocks, with one half of the participants starting with the omnidirectonal condition and the other half starting with the beamforming condition. The article (https://doi.org/10.3389/fnins.2021.643705) contains all methodological details
 

 - Björn Holtze (January, 2021)",1,1,0
ds003517,2021-02-15 16:30:23,James F Cavanagh,1.1.0,ContinuousVideoGamePlay,2021-02-15 18:10:24,1,1,6262670000,0.006 TB,389,17,18,39,1.1.1,CC0,"James F Cavanagh, Joel Castellanos",,,,PMID: 26952196,10.18112/openneuro.ds003517.v1.1.0,,ContinuousVideoGamePlay,,EEG,"EEG during during continuous gameplay of an 8-bit style video game. EEG published here: 10.1016/j.neuroimage.2016.02.075.  N=17 participants. in addition to the video game, participants first completed a 2-stim visual oddball and a 2-doors gambling task. Tasks included in Java programming language. Its pretty fun... Each task sends triggers to the EEG file, and also outputs continuous data in a .csv log file. For the Escape from Asteroid Axon video game this has a wealth of movement, player position and action, antagonist position, loot box, etc info.  Data collected circa 2015 in Cognitive Rhythms and Computation Lab at University of New Mexico. Some analytic scripts are inlcuded, but I cant verify that these were what I used in the final analysis. Some (ExAAx\_Log.m) are clearly pilot analyses. Your best bet would be to play the game and record some triggers and examine how those line up with the .csv log, etc.  - James F Cavanagh 02/10/2021",1,1,0
ds003518,2021-02-15 21:02:23,James F Cavanagh,1.1.0,SimonConflict,2021-02-16 18:03:02,1,2,42423400000,0.042 TB,1265,110,18,30,1.1.1,CC0,"James F Cavanagh, Michael J Frank",,,,PMID: 25367437,10.18112/openneuro.ds003518.v1.1.0,,SimonConflict,,EEG,"Simon conflict task with cost of conflict reinforcement manipulation.  Study 1: 80 healthy participants (2 removed) + 5 placebo session from a pilot of the drug study. Total n=83. Study 2: 30 healthy participants (3 dropout) in a double-blind drug study. Total n=27.  Drug was Cabergoline 1.25 mg.  Study 1 subjects had IDs 101-180 and the 5 placebo were 301/401 - 305/405.  Study 2 subjects had IDs 305/405 - 330/430.  The dual numbers were for session: 300s were first session, 400s were second session. Here we have simply put them in as session 1 and session 2.  So Joe Smith would have been 305 on visit 1, then 405 on visit 2. If he got cab first we indicated that in the Sess1\_Drug column.  EEG published here: 10.1038/ncomms6394.  Task included in Matlab programming language.  Data collected circa 2012-2013 in Laboratory for Neural Computation & Cognition at Brown. Check the .xls sheet under code folder for more meta data.  Triggers are complicated. See CC\_Triggers.mat under code folder. A few old analysis scripts are included. - James F Cavanagh 02/15/2021",1,1,0
ds003519,2021-02-17 16:28:46,James F Cavanagh,1.1.0,VisualWorkingMemory,2021-02-17 17:12:37,1,2,9623060000,0.01 TB,439,27,18,26,1.1.1,CC0,"James F Cavanagh, Michael J Frank, James Broadway",,,,PMID: 29569219,10.18112/openneuro.ds003519.v1.1.0,,VisualWorkingMemory,,EEG,"Visual Working Memory. Mostly unpublished!  Beh data published here: 10.3758/s13415-018-0584-6.  EEG data never published.  Same sample as this published study: 10.1038/ncomms6394. 30 healthy participants (3 dropout) in a double-blind drug study. Total n=27.  Drug was Cabergoline 1.25 mg.  Subjects had IDs 305/405 - 330/430.  The dual numbers were for session: 300s were first session, 400s were second session. Here we have simply put them in as session 1 and session 2.  So Joe Smith would have been 305 on visit 1, then 405 on visit 2. If he got cab first we indicated that in the Sess1\_Drug column.  Task included in Matlab programming language.  Data collected circa 2012-2013 in Laboratory for Neural Computation & Cognition at Brown. Check the .xls sheet under code folder for more meta data, incl. OSpan etc.  - James F Cavanagh 02/15/2021",1,1,0
ds003522,2021-02-19 20:52:38,James F Cavanagh,1.1.0,ThreeStimAuditoryOddball,2021-02-19 22:38:34,1,3,27224800000,0.027 TB,1631,96,18,55,1.1.1,CC0,"James F Cavanagh, Davin Quinn",,,,PMID: 31228481,10.18112/openneuro.ds003522.v1.1.0,,ThreeStimAuditoryOddball,,EEG,"3 stimulus auditory oddball data in control, sub-acute mild TBI, and chronic TBI.  Rest data is also included.  3AOB data published here: 10.1016/j.neuropsychologia.2019.107125. FYI, same task as this different dataset: https://openneuro.org/datasets/ds003490/versions/1.1.0.  For CTL and sub-acute mTBI: Session 1 was from 3 to 14 days post-injury and was the only session with MRI. (MRI will be uploaded ...later). Session 2 was ~2 months (1.5 to 3) and Session 3 was ~4 months (3 to 5) following Session 1.  For Chronic TBI, there was only one session for this study.  There was A LOT of subject attrition over timepoints.  Same samples as reported here: https://psycnet.apa.org/record/2020-66677-001  https://pubmed.ncbi.nlm.nih.gov/31344589/  https://pubmed.ncbi.nlm.nih.gov/31368085/ Task included in Matlab programming language.  Data collected 2016-2018 in the Center for Brain Recovery and Repair at the UNM Health Sciences Center. Check the .xls sheet under code folder for *LOTS* more meta data.  Analysis scripts are included to re-create the paper. - James F Cavanagh 02/17/2021",1,1,0
ds003523,2021-02-22 17:53:30,James F Cavanagh,1.1.0,VisualWorkingMemory,2021-02-22 20:47:30,1,3,40304000000,0.04 TB,1802,91,18,55,1.1.1,CC0,James F Cavanagh,,,,,10.18112/openneuro.ds003523.v1.1.0,,VisualWorkingMemory,,EEG,"Visual working memory in control & sub-acute mild TBI.  Mind wandering probes were inserted between trials.  **DATA NEVER PUBLISHED!**  If youre interested in working together, I have 1/3 of the paper already done, including CONSORT diagrams, tables, behavioral analysis, etc.  All EEG data are even fully cleaned and pre-processed.  For CTL and sub-acute mTBI: Session 1 was from 3 to 14 days post-injury and was the only session with MRI. (MRI will be uploaded ...later). Session 2 was ~2 months (1.5 to 3) and Session 3 was ~4 months (3 to 5) following Session 1.  There was A LOT of subject attrition over timepoints.  Same samples as reported here: https://psycnet.apa.org/record/2020-66677-001  https://pubmed.ncbi.nlm.nih.gov/31344589/  https://pubmed.ncbi.nlm.nih.gov/31368085/ 10.1016/j.neuropsychologia.2019.107125  Same task as this one here: 10.3758/s13415-018-0584-6.  Task included in Matlab programming language.  Data collected 2016-2018 in the Center for Brain Recovery and Repair at the UNM Health Sciences Center. Check the .xls sheet under code folder for *LOTS* more meta data.  Analysis scripts are included. - James F Cavanagh 02/17/2021",1,1,0
ds003555,2021-03-05 19:41:18,Dorottya Cserpan,1.0.1,Dataset of EEG recordings of pediatric patients with epilepsy based on the 10-20 system,2021-03-06 1:41:25,1,1,16177400000,0.016 TB,96,30,0,17,1.4.0,CC0,"Dorottya Cserpan, Ece Boran, Richard Rosch, San Pietro Lo Biundo, Georgia Ramantani, Johannes Sarnthein","We thank C. Carosio, L. Glaser, P. Hieber, and G. Selmin for their assistance with EEG recordings and data analysis and V. Dimakopoulos for fruitful discussions.",Please cite this paper: to be filled when DOI confirmed,Swiss National Science Foundation (CRSK-3\_190895 to G.R. and J.S.).,TBD,10.18112/openneuro.ds003555.v1.0.1,Kantonale Ethikkommission Zürich ( KEK-ZH PB-2016-02055),hfo,,EEG,"# Dataset of EEG recordings containing HFO markings for 30 pediatric patients with epilepsy 
 

 ## Summary
 High-frequency oscillations in scalp EEG are promising non-invasive biomarkers of epileptogenicity. However, it is unclear how high-frequency oscillations are impacted by age in the pediatric population. 
 We recorded and processed the first 3 hours of sleep EEG data in 30 children and adolescents with focal or generalized epilepsy. We used an automated and clinically validated high-frequency oscillation detector to determine ripple rates (80-250 Hz) in bipolar channels. The software for the detection of HFOs is freely available at the GitHub repository (https://github.com/ZurichNCH/Automatic-High-Frequency-Oscillation-Detector). Furthermore HFO markings are also added in this database for the selected N3 intervals.
 

 

 ## Repository structure
 

 ### Main directory (hfo/)
 Contains metadata files in the BIDS standard about the participants and the study. Folders are explained below.
 

 ### Subfolders
 * hfo/sub-**/
 Contains folders for each subject, named sub-<subject number> and session information.
 * hfo/sub-**/ses-01/eeg
 Contains the raw eeg data in .edf format for each subject. The duration is typically 3 hours, that was recorded in the beginning of the sleep. Details about the channels are given in the corresponding .tsv file. 
 * hfo/derivatives
 Besides containingsubfolders for the raw data, there are two .json files. The events\_description.json explains the meaning of the columns of the event description tsv files (in the subfolders).
 The interval\_description.json explains the meaning of the columns of the interval description tsv files (in the subfolders).
 

 * hfo/derivatives/sub-**/ses-01/eeg/
 Contains processed data for each subject. Based on the sleep annotations, first we identified the sleep stages. Then we cut 5 minutes data intervals from the N3 sleep stages. We applied bipolar referencing by considering all nearest neighbour chanels, thus resulting in 52 bipolar channels. Each run corresponds to one 5 minute data interval. The DataIntervals.tsv file provides information about how the various runs are related to the raw data by providing the start and end indeces. Besides the .edf and channel descriptor .tsv files there is an other .tsv file containing the detected candidate event details. Eg. sub-26\_ses-01\_task-hfo\_run-01\_events.tsv contains for subject 26 for the first processed data interval the event markings as indeces with additional features of this event described in the abovementioned events\_description.json file.
 

 

 ## Related materials
 The code for HFO detection is available at https://github.com/ZurichNCH/Automatic-High-Frequency-Oscillation-Detector
 

 

 ## Support
 For questions on the dataset or the task, contact Johannes Sarnthein at [johannes.sarnthein@usz.ch](johannes.sarnthein@usz.ch).",1,1,0
ds004398,2023-01-11 14:18:04,Elliott Wimmer,1.0.0,planmemreplay,2023-01-23 14:29:41,0,1,1373060000,1.3 GB,18,1,0,0,,CC0,"G. Elliott Wimmer, Yunzhe Liu, Daniel C. McNamee, Raymond J. Dolan",,Please cite https://doi.org/10.1101/2021.11.08.467745 (update to PNAS),G.E.W. was supported by a fellowship from the Deutsche Forschungsgemeinschaft (DFG) and an MRC Career Development Award (MR/V032429/1) ===NEMAR-SEP=== R.D. is supported by Wellcome Trust Investigator Award 098362/ Z/12/Z. ===NEMAR-SEP=== Y.L. was supported by the Open Research Fund of the State Key Laboratory of Cognitive Neuroscience and Learning. ===NEMAR-SEP=== D.C.M. was supported by a Sir Henry Wellcome Trust Postdoctoral Research Fellowship Q: (110257/Z/15/Z). ===NEMAR-SEP=== The Wellcome Centre for Human Neuroimaging is supported by core funding from the Wellcome Trust (203147/Z/16/Z). ===NEMAR-SEP=== The Max Planck University College London Centre is a joint initiative supported by University College London and the Max Planck Society.,https://github.com/gewimmer-neuro/multistep-replay ===NEMAR-SEP=== https://osf.io/szjxp/ ===NEMAR-SEP=== https://doi.org/10.1101/2021.11.08.467745 (update to PNAS),doi:10.18112/openneuro.ds004398.v1.0.0,,Memory,,MEG,"The MEG files contain a channel with triggers necessary for event marking and timing. Separate event files with onsets are provided in the participant directories for completeness only; the MEG triggers should be used for actual onsets in analysis. The delay between the trigger and the visual onset of an on-screen event sent by the projector is approximately 20 ms, as estimated using a photodiode.
 

 Localizer phase triggers: [Info to be added]
 

 Struct and Rew phase triggers: [Info to be added]
 

 Post triggers: [Info to be added]",1,0,0
ds003570,2021-03-19 11:06:14,Josef Faller,1.0.0,EEG: Improvisation and Musical Structures,2021-03-20 8:07:03,1,1,51133400000,0.051 TB,323,40,18,42,1.1.1,CC0,"Andrew Goldman, Tyreek Jackson, Paul Sajda","Sincere thanks to Josef Faller, Raphael Gerraty, Daphna Shohamy, George Lewis, Peter Gordon, Lori Custodero, the LIINC Lab, and the Learning Lab for supporting this project. Thanks also to the Presidential Scholars in Society and Neuroscience program.",Please cite this paper: https://doi.org/10.1177/0305735618779444,Presidential Scholars in Society and Neuroscience program at Columbia University ===NEMAR-SEP=== Army Research Laboratory under Cooperative Agreement Number W911NF-15-2-0074,"Goldman, A., Jackson, T., & Sajda, P. (2020). Improvisation experience predicts how musicians categorize musical structures. Psychology of Music, 48(1), 18-34. https://doi.org/10.1177/0305735618779444 ===NEMAR-SEP=== Faller, J., Goldman, A. J., Lin, Y., McIntosh, J. R., & Sajda, P. (2021). Spatiospectral brain networks reflective of improvisational experience. bioRxiv. https://doi.org/10.1101/2021.02.25.432633",10.18112/openneuro.ds003570.v1.0.0,"Institutional Review Board of Columbia University, New York City, NY, USA",AuditoryOddballChords,,EEG,"The musicians were instructed to listen to chord progressions, that each consisted of three chords. We refer to one instance of such a progression in the recording as a trial. Every one of the three chords in one trial sounded in sequence, each for 400 ms in piano timbre, after which each trial ended with another 400 ms silence. This resulted in a fixed, total trial length of 1600 ms. The only progressions used in the experiment were ii-IV-I, ii-V-I, ii-IV6-I and ii-V6-I. Each experimental block consisted of 180 trials. For each such block one of the four aforementioned progressions were chosen as ""standard"", resulting in four types of blocks. These ""block types"" were used to counterbalance the effect of other features of the individual progressions such as intervallic content that may have been in themselves salient. An experimental block always started with at least eight ""standard"" trials for the purpose of allowing participants to learn what type of progression would be the standard for the current block. There were two types of deviant trials that each occurred at a probability of 7.5\% (in total 15\%). Every deviant trial was followed by at least three standard trials. Deviant trials only differed from standard trials in terms of the middle chord: (1) Exemplar deviants, where the middle chord was replaced with a chord of identical notes but different inversion. For example, if the middle chord for a standard trial in that experimental block was V then the middle chord for the exemplar deviant in that block would be V6. For (2) function deviants, the middle chord was replaced by a chord from a different functional class. For example, if the middle chord for a standard was again V, then the middle chord for the corresponding function deviant in that block would be IV. Importantly, the key for each trial''s chord progression was picked at random. This meant that musicians needed to examine the second chord of every trial relative to the first and/or third to identify whether the trial was a standard or deviant. The order of standards and deviants within every one of the four types of experimental blocks was generated once only, and was thus identical across subjects within these block types. For the experiment, every one of the block types occurred twice, thus resulting in a total of eight blocks per subject. The order of the eight blocks was shuffled for every subject. In total, there were 1440 trials per subject of which 222 were functional and 218 were exemplar deviants.",1,1,0
ds003574,2021-03-25 21:11:41,Virginie Sterpenich,1.0.2,Reward biases spontaneous neural reactivation during sleep,2021-03-26 13:31:27,1,1,19277900000,0.019 TB,315,18,18,26,1.4.1,CC0,"Virginie Sterpenich, Mojca KM van Schie, Maximilien Catsiyannis, Avinash Ramyead, Stephen Perrig, Hee-Deok Yang, Dimitri Van De Ville, Sophie Schwartz","This research was supported by the National Center of Competence in Research (NCCR) Affective Sciences financed by the Swiss National Science Foundation (grant number: 51NF40-104897) and hosted by the University of Geneva, and the Swiss National Science Foundation (grant numbers: 320030-159862 and 320030-135653), and the Mercier Foundation. Hee-Deok Yang work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2017R1A2B4005305) We also thank Ben Meuleman for statistical advice.",Please cite the paper,Swiss National Science Foundation (51NF40-104897 - 320030-159862 - 320030-135653) ===NEMAR-SEP=== Mercier Foundation ===NEMAR-SEP=== National Research Foundation of Korea (NRF - 2017R1A2B4005305),"Reward biases spontaneous neural reactivation during sleep, Virginie Sterpenich,Mojca KM van Schie,Maximilien Catsiyannis,Avinash Ramyead,Stephen Perrig,Hee-Deok Yang,Dimitri Van De Ville,Sophie Schwartz, Nat. Comm., 2021",10.18112/openneuro.ds003574.v1.0.2,,"rest, game, Face-Maze game, Rest during sleep with simultaneous EEG",,"MRI, EEG","The data included 18 participants that played at 2 different games during wakefulness in the 3T MRI: the FACE and the MAZE game, intermixed with periods of REST and period of preparation of each game (game session). The tasks were manipulated and at the end of the game session, one game was won (Reward game) and the second was lost (No Reward game), randomly assigned for each participant. Next, during the sleep session, 64 electrodes were placed on the head of the participants, before they slept in the MRI with EEG for 1-2 hours (sleep session). Participants can be separated according to the won game (face or maze) and according sleep depth (whether they reached N3 sleep in the MRI or only N2 sleep). A decoding classifier was trained on the data from the game session at wake and applied to the MRI data acquired during sleep (sleep session). Finally, a memory test was performed the next day on the 2 tasks (face and maze).
 For any question related to the methods, please see the manuscript or contact Virginie Sterpenich (Virginie.Sterpenich@unige.ch)
 

 Files includes are
 1) 2 EPI sessions for the tasks
 2) 1 EPI session during resting including wake and sleep (sleep session)
 3) 1 EEG file corresponding to the sleep session (including wake and sleep in the MRI)
 4) 1 T1 anatomical image",1,1,0
ds003602,2021-04-02 21:07:13,Arnaud Delorme,1.0.1,Childhood Sexual Abuse and problem drinking in women: Neurobehavioral mechanisms,2022-07-27 17:15:46,1,1,78609700000,0.079 TB,4247,118,32,43,v5.1,CC0,"Ozlem Korucuoglu, Andrey P. Anokhin",Clement Lee and Arnaud Delorme,,5R01AA025646-04 ===NEMAR-SEP=== 5R24MH120037,No bibliographic reference other than the DOI for this dataset,doi:10.18112/openneuro.ds003602.v1.0.1,,Experiment,,EEG,"Data collection took place at the Washington University School of Medicine, St. Louis, under the supervision of Dr. Andrey Anokhin (andrey@wustl.edu). The project was approved by the Washington University Institutional Review Board (IRB project # 201707051). Detailed task description and subject instructions can be found in a seperate PDF file under the folder stimuli. The task sequence file (stim program code) together with the visual stimuli used in the task are also provided in the stimulus folder. Participants were Monozygotic twin pairs, twin pairs have the same FamilyID (provided in participants.tsv)",1,1,0
ds003620,2021-04-15 18:46:27,Andrew W Corcoran,1.1.1,Runabout: A mobile EEG study of auditory oddball processing in laboratory and real-world conditions,2021-04-20 2:21:41,1,1,18299500000,0.018 TB,370,44,18,39,1.4,CC0,"Magnus Liebherr, Andrew W. Corcoran, Phillip M. Alday, Scott Coussens, Valeria Bellan, Caitlin A. Howlett, Maarten A. Immink, Mark Kohler, Matthias Schlesewsky, Ina Bornkessel-Schlesewsky",AWC and CAH are supported by Australian Government Research Training Program (RTP) scholarships. IBS acknowledges the support of an Australian Research Council Future Fellowship (FT160100437). This research was funded by the University of South Australia under the Research Themes Investment Scheme. We would also like to thank the undergraduate students in course BEHL 3021 ('Cognitive Neuroscience') at the University of South Australia who participated in this study and helped with data collection as part of their class project.,"In addition to citing this OpenNeuro dataset, please cite the accompanying manuscript (linked below).",FT160100437,"Liebherr, Corcoran, et al. (2021). EEG and behavioral correlates of attentional processing while walking and navigating naturalistic environments. Sci Rep 11, 22325. doi: 10.1038/s41598-021-01772-8",doi:10.18112/openneuro.ds003620.v1.1.1,34991,Oddball,,EEG,"### Overview
 

 This dataset contains raw and pre-processed EEG data from a mobile EEG study investigating the effects of cognitive task demands, motor demands, and environmental complexity on attentional processing (see below for experiment details).
 

 All preprocessing and analysis code is deposited in the `code` directory. The entire MATLAB pipeline can be reproduced by executing the `run\_pipeline.m` script. In order to run these scripts, you will need to ensure you have the required MATLAB toolboxes and R packages on your system. You will also need to adapt `def\_local.m` to specify local paths to MATLAB and EEGLAB. Descriptive statistics and mixed-effects models can be reproduced in R by running the `stat\_analysis.R` script.
 

 See below for software details.
 

 ### Citing this dataset
 

 In addition to citing this dataset, please cite the original manuscript reporting data collection and experimental procedures.
 For more information, see the `dataset\_description.json` file.
 

 ### License
 

 ODC Open Database License (ODbL). For more information, see the `LICENCE` file.
 

 ### Format
 

 Dataset is formatted according to the EEG-BIDS extension (Pernet et al., 2019) and the BIDS extension proposal for common electrophysiological derivatives (BEP021) v0.0.1, which can be found here:
 

 https://docs.google.com/document/d/1PmcVs7vg7Th-cGC-UrX8rAhKUHIzOI-uIOh69\_mvdlw/edit#heading=h.mqkmyp254xh6
 

 Note that BEP021 is still a work in progress as of 2021-03-01.
 

 Generally, you can find data in the .tsv files and descriptions in the
 accompanying .json files.
 

 An important BIDS definition to consider is the ""Inheritance Principle"" (see 3.5 in the BIDS specification: http://bids.neuroimaging.io/bids\_spec.pdf), which states:
 

 > Any metadata file (.json, .bvec, .tsv, etc.) may be defined at any directory level. The values from the top level are inherited by all lower levels unless they are overridden by a file at the lower level.
 

 ### Details about the experiment
 

 Forty-four healthy adults aged 18-40 performed an oddball task involving complex tone (piano and horn) stimuli in three settings: 
 (1) sitting in a quiet room in the lab (LAB);
 (2) walking around a sports field (FIELD);
 (3) navigating a route through a university campus (CAMPUS).
 

 Participants performed each environmental condition twice: once while attending to oddball stimuli (i.e. counting the number of presented deviant tones; COUNT), and once while disregarding or ignoring the tone stimuli (IGNORE).
 

 EEG signals were recorded from 32 active electrodes using a Brain Vision LiveAmp 32 amplifier. See manuscript for further details.
 

 ### MATLAB software details
 

 MATLAB Version: 9.7.0.1319299 (R2019b) Update 5
 MATLAB License Number: 678256
 Operating System: Microsoft Windows 10 Enterprise Version 10.0 (Build 18363)
 Java Version: Java 1.8.0\_202-b08 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode
 

 * MATLAB (v9.7)
 * Simulink (v10.0)
 * Curve Fitting Toolbox (v3.5.10)
 * DSP System Toolbox (v9.9)
 * Image Processing Toolbox (v11.0)
 * MATLAB Compiler (v7.1)
 * MATLAB Compiler SDK (v6.7)
 * Parallel Computing Toolbox (v7.1)
 * Signal Processing Toolbox (v8.3)
 * Statistics and Machine Learning Toolbox (v11.6)
 * Symbolic Math Toolbox (v8.4)
 * Wavelet Toolbox (v5.3)
 

 **The following toolboxes/helper functions were also used:**
 

 * EEGLAB (v2019.1)
 * ERPLAB (v8.10)
 * ICLabel (v1.3)
 * clean\_rawdata (v2.3)
 * bids-matlab-tools (v5.2)
 * dipfit (v3.4)
 * firfilt (v2.4)
 * export\_fig (v3.12)
 * ColorBrewer (v3.1.0)
 

 ### R software details
 

 **R version 3.6.2 (2019-12-12)**
 

 **Platform:** x86\_64-w64-mingw32/x64 (64-bit) 
 

 **locale:**
 \_LC\_COLLATE=English\_Australia.1252\_, \_LC\_CTYPE=English\_Australia.1252\_, \_LC\_MONETARY=English\_Australia.1252\_, \_LC\_NUMERIC=C\_ and \_LC\_TIME=English\_Australia.1252\_
 

 **attached base packages:** 
 

 * stats 
 * graphics 
 * grDevices 
 * utils 
 * datasets 
 * methods 
 * base 
 

 **other attached packages:** 
 

 * sjPlot(v.2.8.7) 
 * emmeans(v.1.5.1) 
 * car(v.3.0-10) 
 * carData(v.3.0-4) 
 * lme4(v.1.1-23) 
 * Matrix(v.1.2-18) 
 * data.table(v.1.13.0) 
 * forcats(v.0.5.0) 
 * stringr(v.1.4.0) 
 * dplyr(v.1.0.2) 
 * purrr(v.0.3.4) 
 * readr(v.1.4.0) 
 * tidyr(v.1.1.2) 
 * tibble(v.3.0.4) 
 * ggplot2(v.3.3.2) 
 * tidyverse(v.1.3.0) 
 

 **loaded via a namespace (and not attached):** 
 

 * nlme(v.3.1-149) 
 * pbkrtest(v.0.4-8.6) 
 * fs(v.1.5.0) 
 * lubridate(v.1.7.9) 
 * insight(v.0.12.0) 
 * httr(v.1.4.2) 
 * numDeriv(v.2016.8-1.1) 
 * tools(v.3.6.2) 
 * backports(v.1.1.10) 
 * utf8(v.1.1.4) 
 * R6(v.2.4.1) 
 * sjlabelled(v.1.1.7) 
 * DBI(v.1.1.0) 
 * colorspace(v.1.4-1) 
 * withr(v.2.3.0) 
 * tidyselect(v.1.1.0) 
 * curl(v.4.3) 
 * compiler(v.3.6.2) 
 * performance(v.0.5.0) 
 * cli(v.2.1.0) 
 * rvest(v.0.3.6) 
 * xml2(v.1.3.2) 
 * sandwich(v.3.0-0) 
 * labeling(v.0.3) 
 * bayestestR(v.0.7.2) 
 * scales(v.1.1.1) 
 * mvtnorm(v.1.1-1) 
 * digest(v.0.6.25) 
 * foreign(v.0.8-76) 
 * minqa(v.1.2.4) 
 * rio(v.0.5.16) 
 * pkgconfig(v.2.0.3) 
 * dbplyr(v.1.4.4) 
 * rlang(v.0.4.8) 
 * readxl(v.1.3.1) 
 * rstudioapi(v.0.11) 
 * farver(v.2.0.3) 
 * generics(v.0.0.2) 
 * zoo(v.1.8-8) 
 * jsonlite(v.1.7.1) 
 * zip(v.2.1.1) 
 * magrittr(v.1.5) 
 * parameters(v.0.8.6) 
 * Rcpp(v.1.0.5) 
 * munsell(v.0.5.0) 
 * fansi(v.0.4.1) 
 * abind(v.1.4-5) 
 * lifecycle(v.0.2.0) 
 * stringi(v.1.4.6) 
 * multcomp(v.1.4-14) 
 * MASS(v.7.3-53) 
 * plyr(v.1.8.6) 
 * grid(v.3.6.2) 
 * blob(v.1.2.1) 
 * parallel(v.3.6.2) 
 * sjmisc(v.2.8.6) 
 * crayon(v.1.3.4) 
 * lattice(v.0.20-41) 
 * ggeffects(v.0.16.0) 
 * haven(v.2.3.1) 
 * splines(v.3.6.2) 
 * pander(v.0.6.3) 
 * sjstats(v.0.18.1) 
 * hms(v.0.5.3) 
 * knitr(v.1.30) 
 * pillar(v.1.4.6) 
 * boot(v.1.3-25) 
 * estimability(v.1.3) 
 * effectsize(v.0.3.3) 
 * codetools(v.0.2-16) 
 * reprex(v.0.3.0) 
 * glue(v.1.4.2) 
 * modelr(v.0.1.8) 
 * vctrs(v.0.3.4) 
 * nloptr(v.1.2.2.2) 
 * cellranger(v.1.1.0) 
 * gtable(v.0.3.0) 
 * assertthat(v.0.2.1) 
 * xfun(v.0.18) 
 * openxlsx(v.4.2.2) 
 * xtable(v.1.8-4) 
 * broom(v.0.7.1) 
 * coda(v.0.19-4) 
 * survival(v.3.2-7) 
 * lmerTest(v.3.1-3) 
 * statmod(v.1.4.34) 
 * TH.data(v.1.0-10) 
 * ellipsis(v.0.3.1)",1,1,0
ds003626,2021-04-17 14:09:21,Nicolás Nieto,2.1.2,Inner Speech,2021-04-17 19:51:37,1,3,19624700000,0.02 TB,33,10,0,0,1.4,CC0,"Nicolas Nieto, Victoria Peterson, Hugo Rufiner, Juan Kamienkowski, Ruben Spies",,,,https://www.nature.com/articles/s41597-022-01147-2 ===NEMAR-SEP=== https://github.com/N-Nieto/Inner\_Speech\_Dataset,doi:10.18112/openneuro.ds003626.v2.1.2,"Comité Asesor de Ética y Seguridad en el Trabajo Experimental (CEySTE), CCT-CONICET, Santa Fe,",,,EEG,"Inner Speech Dataset.
 

 Author: Nicolás Nieto 
 

 Code available at: https://github.com/N-Nieto/Inner\_Speech\_Dataset
 

 Publication available at: https://www.nature.com/articles/s41597-022-01147-2
 

 Abstract:
 Surface electroencephalography is a standard and noninvasive way to measure electrical brain activity. Recent advances in artificial intelligence led to significant improvements in the automatic detection of brain patterns, allowing increasingly faster, more reliable and accessible Brain-Computer Interfaces.
 Different paradigms have been used to enable the human-machine interaction and the last few years have broad a mark increase in the interest for interpreting and characterizing the ""inner voice"" phenomenon. This paradigm, called inner speech, raises the possibility of executing an order just by thinking about it, allowing a “natural” way of controlling external devices. Unfortunately, the lack of publicly available electroencephalography datasets, restricts the development of new techniques for inner speech recognition. A ten-subjects dataset acquired under this and two others related paradigms, obtain with an acquisition systems of 136 channels, is presented. The main purpose of this work is to provide the scientific community with an open-access multiclass electroencephalography database of inner speech commands that could be used for better understanding of the related brain mechanisms.  
 

 Conditions = Inner Speech, Pronounced Speech, Visualized Condition
 

 Classes = ""Arriba/Up"", ""Abajo/Down"", ""Derecha/Right"", ""Izquierda/Left""
 

 Total Trials = 5640
 

 

 Please contact us at this e-mail address if you have any doubts: nnieto@sinc.unl.edu.ar",1,1,0
ds004278,2022-09-23 17:53:03,Lina Teichmann,1.0.1,Sustained Neural Representations of Personally Familiar People and Places During Cued Recall,2022-09-26 16:19:30,0,1,82360000000,76.7 GB,844,30,21,35,1.6.0,CC0,"Alexis Kidder(*), Anna Corriveau(*), Lina Teichmann, Susan Wardle, Chris Baker, [(*) = equal contribution]","We thank Adam Dickter, Wan Kwok, and Tom Holroyd for assistance with data acquisition, and Ed Silson and Adrian Gilmore for assistance with experimental design and comments on an earlier version of this manuscript. We also thank Karen for helpful discussions.","Kidder*, Corriveau* et al., 2022",Intramural Research Program of the National Institute of Mental Health (ZIAMH002909),,doi:10.18112/openneuro.ds004278.v1.0.1,"This research was conducted according to procedures approved the NIH Institutional Review Board as a part of the study protocol (93-M-0170, NCT00001360).",CuedRecall,,MEG,"References
 ----------
 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., Moreau, J., Oostenveld, R., Schoffelen, J., Tadel, F., Wexler, J., Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5, 180110. https://doi.org/10.1038/sdata.2018.110",1,0,0
ds002761,2020-04-29 12:47:53,Elliott Wimmer,1.1.2,memoryreplay,2020-05-15 7:08:56,0,1,1758240,0.002 GB,752,25,0,0,,CC0,"G. Elliott Wimmer, Yunzhe Liu, Neza Vehar, Timothy E.J. Behrens, Raymond J. Dolan",The authors thank Zeb Kurth-Nelson for helpful discussions.,Please cite https://doi.org/10.1101/758185,R.D. is supported by Wellcome Trust Investigator Award 098362/ Z/12/Z. ===NEMAR-SEP=== Y.L. is supported by a UCL Graduate Research Scholarship and an Overseas Research Scholarship. ===NEMAR-SEP=== The Wellcome Centre for Human Neuroimaging is supported by core funding from the Wellcome Trust (203147/Z/16/Z). ===NEMAR-SEP=== The Max Planck University College London Centre is a joint initiative supported by University College London and the Max Planck Society.,https://github.com/gewimmer-neuro/memory-sequences ===NEMAR-SEP=== https://osf.io/qaewv/ ===NEMAR-SEP=== https://doi.org/10.1101/758185,doi:10.18112/openneuro.ds002761.v1.1.2,,Memory,,MEG,"The MEG files contain a channel with triggers necessary for event marking and timing. Separate event files with onsets are provided in the participant directories for completeness only; the MEG triggers should be used for actual onsets in analysis. The delay between the trigger and the visual onset of an on-screen event sent by the projector is approximately 20 ms, as estimated using a photodiode.
 

 Memory phase triggers: At the onset of a trial, the first trigger represents the category (1-8) of the on-screen image. Categories 1-6 represent actual stimulus categories. Trigger values of 7 and 8 represent the 4 positive and 4 negative story-ending stimuli, respectively. The onset of the answer, approximately 5.5 sec later, is marked by a trigger value of 11.
 

 Localizer phase triggers: As in the memory phase, at the onset of a trial, the first trigger represents the category (1-8) of the on-screen image. Categories 1-6 represent true categories. Trigger values of 7 and 8 represent the 4 positive and 4 negative story-ending stimuli, respectively. For a baseline, note that for the 2 s prior to picture onset, a word naming that picture was presented on the screen; thus, baseline values should be taken from data more than 2 s before the trigger onset.
 

 Methods note: a sequenceness analysis step was omitted from the published 2020 Nature Neuroscience paper. The text should have read:  
 ""We next asked whether the ?i(?t) was consistent with a specified 6 × 6 transition matrix by taking the Frobenius inner product between these two matrices (the sum of element-wise products of the two matrices). This resulted in a single number Z?t, which pertained to lag ?t. For each trial, sequenceness results were then z-scored across lags. Finally, differential forward – backward sequenceness was defined as Zf?t – Zb?t.""",1,0,0
ds002001,2019-06-27 21:51:22,Beth Bock,1.0.0,Rivalry_Tagging,2019-06-28 18:20:34,1,7,87694100000,81.7 GB,1001,10,0,0,1.1.1,PD,"Janine Mendola, Elizabeth Bock",,,,,10.18112/openneuro.ds002001.v1.0.0,,"rivalry, noise",,"MEG, MRI",,1,0,0
ds003638,2021-04-26 15:21:15,James F Cavanagh,1.0.0,"EEG: Electrophysiological biomarkers of behavioral dimensions 
 from cross-species paradigms",2021-09-17 19:50:42,0,1,16397700000,0.016 TB,404,57,18,35,1.1.1,CC0,"James F Cavanagh, Greg Light, Neal Swerdlow, Jonathan Brigman, Jared Young",,,,https://www.nature.com/articles/s41398-021-01562-w,10.18112/openneuro.ds003638.v1.0.0,,5CCPTPSTPRBP,,EEG,"Three different tasks.  From: ""Electrophysiological biomarkers of behavioral dimensions from cross-species paradigms""  N=57 humans.  Also has mouse data in code folder.  Triggers were odd binary recombinations that were re-translated into 0-255 in Matlab.  See .m scripts and Trigger Translator.xlsData collected circa 2014-2016 in San Diego. Data analyzed circa 2015-2021 in New Mexico.  - James F Cavanagh 04/19/2021",1,1,0
ds003645,2021-05-04 8:12:20,Dung Truong,2.0.2,Face processing MEEG dataset with HED annotation,2021-05-20 17:51:13,1,1,114157000000,0.114 TB,1137,18,23,37,1.8.0,CC0,"Daniel G. Wakeman, Richard N Henson, Dung Truong (curation), Kay Robbins (curation), Scott Makeig (curation), Arno Delorme (curation)",,,"Experiment was supported by the UK Medical Research Council (MC\_A060\_5PR10) and Elekta Ltd. ===NEMAR-SEP=== Curation was supported by: Army Research Laboratory W911NF-10-2-0022, NIH R01 EB023297-03, NIH R01 NS047293-l4, and NIH R24 MH120037-01.","Wakeman, D., Henson, R. (2015). A multi-subject, multi-modal human neuroimaging dataset. Sci Data 2, 150001. https://doi.org/10.1038/sdata.2015.1 ===NEMAR-SEP=== Robbins, K., Truong, D., Appelhoff, S., Delorme, A., & Makeig, S. (2021). Capturing the nature of events and event context using Hierarchical Event Descriptors (HED). In press for NeuroImage Special Issue Practice in MEEG. NeuroImage 245 (2021) 118766. Online: https://www.sciencedirect.com/science/article/pii/S1053811921010387. ===NEMAR-SEP=== Robbins, K., Truong, D., Jones, A., Callanan, I., & Makeig, S. (2021). Building FAIR functionality: Annotating events in time series data using Hierarchical Event Descriptors (HED). Neuroinformatics Special Issue Building the NeuroCommons. Neuroinformatics https://doi.org/10.1007/s12021-021-09537-4. Online: https://link.springer.com/article/10.1007/s12021-021-09537-4.",doi:10.18112/openneuro.ds003645.v2.0.2,The study was approved by Cambridge University Psychological Ethics Committee. Written informed consent was obtained from each participant prior to and following each phase of the experiment.,FacePerception,8.1.0,"EEG, MEG, MRI","**Introduction:**
 This dataset consists of the MEEG (sMRI+MEG+EEG) portion of the multi-subject, multi-modal face processing dataset (ds000117). This dataset was originally acquired and shared by Daniel Wakeman and Richard Henson (https://pubmed.ncbi.nlm.nih.gov/25977808/). The MEG and EEG data were simultaneously recorded; the sMRI scans were preserved to support M/EEG source localization. Following event log augmentation, reorganization, and HED (v8.0.0) annotation, the EEG data have been repackaged in EEGLAB format.
 

 **Overview of the experiment:**
 Eighteen participants completed two recording sessions spaced three months apart – one session recorded fMRI and the other simultaneously recorded MEG and EEG data. During each session, participants performed the same simple perceptual task, responding to presented photographs of famous, unfamiliar, and scrambled faces by pressing one of two keyboard keys to indicate a subjective yes or no decision as to the relative spatial symmetry of the viewed face. Famous faces were feature-matched to unfamiliar faces; half the faces were female. The two sessions (MEEG, fMRI) had different organizations of event timing and presentation because of technological requirements of the respective imaging modalities. Each individual face was presented twice during the session. For half of the presented faces, the second presentation followed immediately after the first. For the other half, the second presentation was delayed by 5-15 face presentations.
 

 **Preprocessing:**
 The EEG preprocessing, which was performed using the `wh\_extracteeg\_BIDS.m` located in the code directory, includes the following steps:
 * Ignore MRI data except for sMRI.
 * Extract EEG channels out of the MEG/EEG fif data
 * Add fiducials
 * Rename EOG and EKG channels
 * Extract events from event channel
 * Add button press events!
 * Remove spurious event types 5, 6, 7, 13, 14, 15, 17, 18 and 19
 * Remove spurious event types 24 for subject 3 run 4
 * Correct event latencies (events have a shift of 34 ms)
 * Add HED (v8.0.0) event annotations -- see Robbins et al. (2021)
 * Remove event fields `urevent` and `duration`
 * Save as EEGLAB .set format
 

 **Data curators:**
 Dung Truong, Ramon Martinez, Scott Makeig, Arnaud Delorme (UCSD, La Jolla, CA, USA), Kay Robbins (UTSA, San Antonio, TX, USA)",1,1,1
ds003655,2021-05-17 18:09:21,Yuri Pavlov,1.0.2,VerbalWorkingMemory,2021-05-20 16:20:52,1,1,21756900000,0.022 TB,1253,156,0,0,1.1.1,CC0,Yuri G. Pavlov,,"Please cite these papers for the acknowledgement:
 Pavlov, Y. G., & Kotchoubey, B. (2021). Temporally distinct oscillatory codes of retention and manipulation of verbal working memory. European Journal Of Neuroscience. https://doi.org/10.1111/ejn.15457
 Pavlov, Y. G., & Kotchoubey, B. (2020). The electrophysiological underpinnings of variation in verbal working memory capacity. Scientific Reports, 10(1), 16090. https://doi.org/10.1038/s41598-020-72940-5",,https://doi.org/10.1038/s41598-020-72940-5 ===NEMAR-SEP=== https://doi.org/10.1111/ejn.15457,doi:10.18112/openneuro.ds003655.v1.0.2,,VerbalWorkingMemory,,EEG,"raw EEG in a modified Sternberg working memory paradigm with two types of task: with mental manipulations (alphabetization) and simple retention (TASK) and 3 levels of load: 5, 6, or 7 letter to memorize (LOAD).
 

 Events.tsv files in eeg folders contain event descriptions (see column trial\_type). 
 

 trial\_type = {  
  '500' 'start of the trial';
  '150' 'start of the baseline period: Retention of 5 letters (5R)';
  '151' 'start of the baseline period: Manipulation with 5 letters (5M)';
  '160' 'start of the baseline period: Retention of 6 letters (6R)';
  '161' 'start of the baseline period: Manipulation with 6 letters (6M)';
  '170' 'start of the baseline period: Retention of 7 letters (7R)';
  '171' 'start of the baseline period: Manipulation with 7 letters (7M)';
  '4' 'presentation of the task instruction';
  '250' 'encoding: 5R';
  '251' 'encoding: 5M';
  '260' 'encoding: 6R';
  '261' 'encoding: 6M';
  '270' 'encoding: 7R';
  '271' 'encoding: 7M';
  '350' 'delay period: 5R';
  '351' 'delay period: 5M';
  '360' 'delay period: 6R';
  '361' 'delay period: 6M';
  '370' 'delay period: 7R';
  '371' 'delay period: 7M';
  '50' 'probe: 5R';
  '51' 'probe: 5M';
  '60' 'probe: 6R';
  '61' 'probe: 6M';
  '70' 'probe: 7R';
  '71' 'probe: 7M';
  '4500' 'Response: 5R: error';
  '4501' 'Response: 5R: correct';
  '4510' 'Response: 5M: error';
  '4511' 'Response: 5M: correct';
  '4600' 'Response: 6R: error';
  '4601' 'Response: 6R: correct';
  '4610' 'Response: 6M: error';
  '4611' 'Response: 6M: correct';
  '4700' 'Response: 7R: error';
  '4701' 'Response: 7R: correct';
  '4710' 'Response: 7M: error';
  '4711' 'Response: 7M: correct'
  };",1,1,0
ds003670,2021-05-29 22:23:51,Nigel Gebodh,1.1.0,"Dataset of Concurrent EEG, ECG, and Behavior with Multiple Doses of transcranial Electrical Stimulation - BIDS",2021-07-10 22:21:41,1,6,77549600000,0.078 TB,512,25,19,43,1.1.1,CC0,"Nigel Gebodh, Zeinab Esmaeilpour, Abhishek Datta, Marom Bikson","Portions of this study were funded by X (formerly Google X), the Moonshot Factory. The funding source had no influence on study conduction or result evaluation. MB is further supported by grants from the National Institutes of Health: R01NS101362, R01NS095123, R01NS112996, R01MH111896, R01MH109289, and (to NG) NIH-G-RISE T32GM136499.
 

 We would like to thank Yuxin Xu and Michaela Chum for all their technical assistance.","**Raw Data Use**
 Gebodh, Nigel, Esmaeilpour, Zeinab, Datta, Abhishek, & Bikson, Marom. (2020). Dataset of Concurrent EEG, ECG, and Behavior with Multiple Doses of transcranial Electrical Stimulation (Version V2) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.3837212",X (formerly Google X) ===NEMAR-SEP=== NIH R01NS095123 ===NEMAR-SEP=== NIH R01NS101362 ===NEMAR-SEP=== NIH R01NS112996 ===NEMAR-SEP=== NIH R01MH111896 ===NEMAR-SEP=== NIH R01MH109289 ===NEMAR-SEP=== NIH-G-RISE T32GM136499,"Gebodh, Nigel, Esmaeilpour, Zeinab, Datta, Abhishek, & Bikson, Marom. (2020). Dataset of Concurrent EEG, ECG, and Behavior with Multiple Doses of transcranial Electrical Stimulation (Version All) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.3837212",10.18112/openneuro.ds003670.v1.1.0,,GXtESCTT,,EEG,"**Synopsis**
 This is the **GX dataset** formatted to comply with [BIDS](https://bids.neuroimaging.io/) standard format. 
 

 The tES/EEG/CTT/Vigilance experiment contains 19 unique participants (some repeated experiments).
 Over a 70 min period EEG/ECG/EOG were recorded concurrently with a [CTT](https://sccn.ucsd.edu/~scott/pdf/COMPTRACK.pdf)
 where participants maintained a ball at the center of the screen
 and were periodically stimulated (with low-intensity noninvasive brain stimulation) for 30 secs with combinations of 9 stimulation montages.
 

 For the **raw data** please see: https://zenodo.org/record/4456079 
 

 For methodological details please see corresponding article titled:
  **Dataset of concurrent EEG, ECG, and behavior with multiple doses of transcranial Electrical Stimulation**
 

 

 **Data Descriptor Abstract**
 We present a dataset combining human-participant high-density electroencephalography (EEG) with physiological and continuous behavioral metrics during transcranial electrical stimulation (tES). Data include within participant application of nine High-Definition tES (HD-tES) types, targeting three cortical regions (frontal, motor, parietal) with three stimulation waveforms (DC, 5 Hz, 30 Hz); more than 783 total stimulation trials over 62 sessions with EEG, physiological (ECG, EOG), and continuous behavioral vigilance/alertness metrics. Experiment 1 and 2 consisted of participants performing a continuous vigilance/alertness task over three 70-minute and two 70.5-minute sessions, respectively. Demographic data were collected, as well as self-reported wellness questionnaires before and after each session. Participants received all 9 stimulation types in Experiment 1, with each session including three stimulation types, with 4 trials per type. Participants received 2 stimulation types in Experiment 2, with 20 trials of a given stimulation type per session. Within-participant reliability was tested by repeating select sessions. This unique dataset supports a range of hypothesis testing including interactions of tDCS/tACS location and frequency, brain-state, physiology, fatigue, and cognitive performance.
 

 For more details please see the full data descriptor article.
 

 

 Code used to import and process this dataset can be found here:
 **GitHub** : https://github.com/ngebodh/GX\_tES\_EEG\_Physio\_Behavior
 

 For downsampled data please see:
 **Experiment 1** : https://doi.org/10.5281/zenodo.3840615
 **Experiment 2** : https://doi.org/10.5281/zenodo.3840617
 

 

 

 - Nigel Gebodh (May 26th, 2021)",1,1,0
ds003682,2021-06-05 16:53:45,Toby Wise,1.0.0,Model-based aversive learning in humans is supported by preferential task state reactivation,2021-06-05 23:01:30,1,1,227225000000,0.227 TB,1347,28,0,0,v1.5.0,CC0,"Toby Wise, Yunzhe Liu, Fatima Chowdhury, Raymond J. Dolan",,,,,10.18112/openneuro.ds003682.v1.0.0,,AversiveLearningReplay,,MEG,"This dataset contains raw and processed MEG data for the paper ""Model-based aversive learning in humans is supported by preferential task state reactivation"" by Toby Wise, Yunzhe Liu, Fatima Chowdhury & Ray Dolan.
 

 Raw data is provided as `.fif` files, although it was acquired on a CRF system.",1,1,0
ds003688,2021-06-08 21:48:50,Julia Berezutskaya,1.0.7,Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film,2021-06-13 20:10:30,1,3,16334700000,0.016 TB,1355,63,5,55,1.2.1,CC0,"Julia Berezutskaya, Mariska J. Vansteensel, Erik J. Aarnoutse, Zachary V. Freudenburg, Giovanni Piantoni, Mariana P. Branco, Nick F. Ramsey","We thank Frans Leijten, Cyrille Ferrier, Geert-Jan Huiskamp, Sandra van der Salm and Tineke Gebbink for help with collecting data; Peter Gosselaar and Peter van Rijen for implanting the electrodes; the technicians and staff of the clinical neurophysiology department and the patients for their time and effort; and the members of the UMC Utrecht ECoG research team for data collection. We also thank the Swedish Film Institute film company for their help and the provided materials.",Please cite this paper at https://www.nature.com/articles/s41597-022-01173-0,Advanced iConnect Project Grant ADV 320708 ===NEMAR-SEP=== the Netherlands Organisation for Scientific Research (Language in Interaction Project Gravitation Grant 024.001.006),https://www.biorxiv.org/content/10.1101/2021.06.09.447733,doi:10.18112/openneuro.ds003688.v1.0.7,The study was approved by the Medical Ethical Committee of the Utrecht University Medical Center in accordance with the Declaration of Helsinki (2013),"film, rest",,"iEEG, MRI","Open iEEG-fMRI dataset from stimulation with a short audiovisual film
 

 Full description of the data in our dataset paper: https://www.nature.com/articles/s41597-022-01173-0
 

 Video description of the dataset: https://www.youtube.com/watch?v=C14cWM1CvrE&t=13s
 

 UMC Utrecht Team
 https://www.nick-ramsey.eu/",1,1,0
ds003690,2021-06-09 12:14:21,Maria José Ribeiro,1.0.0,"EEG, ECG and pupil data from young and older adults: rest and auditory cued reaction time tasks",2021-06-10 8:35:24,0,1,23043400000,0.023 TB,2630,75,19,70,v1.2.1,CC0,"Maria J. Ribeiro, Miguel Castelo-Branco",,"For EEG and pupil data, please cite: Ribeiro M.J. and Castelo-Branco M. 2019 Age-related differences in event-related potentials and pupillary responses in cued reaction time tasks. Neurobiology of Aging. Vol. 73: 177-189 PMID: 30366291 DOI: 10.1016/j.neurobiolaging.2018.09.028. For ECG data, please cite: Ribeiro MJ, Castelo-Branco M. 2019. Neural correlates of anticipatory cardiac deceleration and its association with the speed of perceptual decision-making, in young and older adults. Neuroimage. Jun 5;199:521-533. PMID: 31173904 DOI: 10.1016/j.neuroimage.2019.06.004","This work was funded by Fundação para a Ciência e a Tecnologia (Grants: SFRH/BPD/102188/2014 and UID/NEU/04539/2013-COMPETE, POCI-01-0145-FEDER-00 7440, PACeMEDPERSYST, POCI-01-0145-FEDER-30852, POCI-01-0145-FEDER-016428, BIGDATIMAGE, CENTRO-01-0145-FEDER-000016, financed by Centro 2020 FEDER, COMPETE).","Ribeiro M.J. and Castelo-Branco M. 2019 Age-related differences in event-related potentials and pupillary responses in cued reaction time tasks. Neurobiology of Aging. Vol. 73: 177-189 PMID: 30366291 DOI: 10.1016/j.neurobiolaging.2018.09.028 ===NEMAR-SEP=== Ribeiro MJ, Castelo-Branco M. 2019. Neural correlates of anticipatory cardiac deceleration and its association with the speed of perceptual decision-making, in young and older adults. Neuroimage. Jun 5;199:521-533. PMID: 31173904 DOI: 10.1016/j.neuroimage.2019.06.004",10.18112/openneuro.ds003690.v1.0.0,,"gonogo, passive, simpleRT",,EEG,"Age-related differences in EEG, ECG and pupilography during auditory cued reaction time tasks
 

 In this study, we acquired the electroencephalogram (EEG), pupilogram and electrocardiogram (ECG) while a group of young (N = 36) and a group of older (N = 39) adults were engaged in auditory cued reaction time tasks (active tasks) or passively listening to the auditory stimulus used as temporal cue, presented with the same frequency as in the active tasks (passive task - 4 minutes acquired at the beginning of the session).
 

 The active tasks were a cued simple reaction time task and a cued go/no-go task. In the active tasks, 16\% of the trials were cue only trials (the cue was presented but no target followed).
 

 The order of the active tasks was counterbalanced across participants and were acquired in two runs of 8 minutes per task. In each task, we acquired 120 trials. In the simple reaction time task, 100 trials were cue-target trials and 20 trials were cue-only. In the go/no-go task, 80 trials were cue-go trials, 20 were cue-no-go trials, and 20 trials were cue-only trials.
 

 Participants were fixating a grey computer screen with a lighter grey fixation cross at the center. The auditory stimuli were single-frequency signals (pure tones) with duration 250 ms, with the following frequencies: cue 1500 Hz; go stimulus 1700 Hz; no-go stimulus 1300 Hz; and error feedback signal 1000 Hz.
 

 The sounds were played at around 67 dB(A) from a hi-fi speakers system. All stimuli were suprathreshold.
 

 EEG signal was recorded using a 64-channel Neuroscan system with scalp electrodes placed according to the International 10-20 electrode placement standard, with reference between the electrodes CPz and Cz and ground between FPz and Fz. Acquisition rate was 500 Hz. Vertical and horizontal electrooculograms were recorded to monitor eye movements and blinks. Bipolar electrocardiogram (ECG) electrodes were placed on the chest. During data acquisition, the participants head was stabilized with a chin and forehead rest. Consequently, the electrodes on the forehead, FP1, FPz, and FP2, displayed signal fluctuation artifacts due to the pressure on the forehead rest. These were excluded from the recordings.
 

 Electrode positions were measured using a 3D-digitizer Fastrak (Polhemus, VT, USA) and imported into the EEGLAB files.
 

 Pupil data was acquired with iView X Hi-Speed 1250 system from SMI with a sampling rate of 240 Hz. Pupil data was imported into the EEG dataset with the EYE-EEG EEGLAB plugin.
 

 Synchronized EEG, ECG and pupil data are included in separate channels in the EEGLAB .set files.",1,1,0
ds003694,2021-06-12 9:48:35,Benjamin James Griffiths,1.0.0,MEGMEM,2021-07-13 12:17:12,1,1,234582000000,0.235 TB,531,28,0,0,1.0.2,CC0,"Benjamin J. Griffiths, María Carmen Martín-Buro, Bernhard Staresina, Simon Hanslmayr",,"When making use of this data, please cite the associated publication [https://www.biorxiv.org/content/10.1101/2020.01.22.915330v3]",,,10.18112/openneuro.ds003694.v1.0.0,,MEM,,MEG,,1,1,0
ds003702,2021-06-15 15:03:49,Samantha Gregory,1.0.1,Social Memory cuing,2021-06-15 17:10:15,1,1,18784100000,0.019 TB,240,47,0,0,1.6.0,CC0,"Samantha Gregory, Hongfang Wang, Klaus Kessler",,,This work was supported by a Leverhulme Trust early career fellowship [ECF-2018-130] awarded to S.E.A. Gregory.).,"Gregory, S., Wang, H., & Kessler, K. (2021, June 17). EEG alpha and theta signatures of socially and non-socially cued working memory in virtual reality. https://doi.org/10.31234/osf.io/mkags",10.18112/openneuro.ds003702.v1.0.1,,Social Memory cuing,,EEG,"EEG Raw and processed data for a memory task presented in virtual reality, 
 The full task is on OSF: https://osf.io/s9xmu/files
 

 Derviatives:
 Behavioral data from the task in derivatives
 Processed data is in the derivatives 
 

 Code
 Event codes are listed in the code file
 Trial function, preprocessing, data cleaning and analysis are also in the code file. 
 

 

 Task:
 Wearing a head mounted display to display the task in virtual reality participants completed a visual working memory task 
 In the task they had to remember the status of and details about presented objects. 
 A person or a stick cued the items such that it could look left or right and items could appear on the left or right. Sometimes the cue was valid 
 (i.e. pointed where objects appeared) and sometimes it was invalid (pointed away from where objects appeared) 
 The objects were always a bowl, a cup, a plate and a teapot. Each could have a different status that needed to be remembered 
 praticipants were probed on memory for item location and then item status
 

 

 Preprint to be added
 

 

 Event codes within the data set are as follows. Trial function is included in code folder. 
 For the avatar cue
 s3021 Character shown - i.e. moment the avatar appears
 s3022 Objects shown - i.e. moment that the memory targets appear
 s3023 Maintenance interal - i.e. moment the memory objects leave the screen and the blank maintenance interval occurs
 s3024 Probe object shown - i.e. moment that participantis presented with location probe
 s3025 Resp 1 made - i.e. moment that the participants have responded to the location probe
 s3026 Q2 shown - i.e. moment that participants are asked a question about the status of the objects
 s3027 Resp 2 made - i.e. moment that the participants have responded to the status probe
 

 For the stick cue
 s3041 Stick shown - i.e. moment the stick appears
 s3042 Objects shown - i.e. moment that the memory targets appear
 s3043 Maintenance interal - i.e. moment the memory objects leave the screen and the blank maintenance interval occurs
 s3044 Probe object shown - i.e. moment that participantis presented with location probe
 s3045 Resp 1 made - i.e. moment that the participants have responded to the location probe
 s3046 Q2 shown - i.e. moment that participants are asked a question about the status of the objects
 s3047 Resp 2 made - i.e. moment that the participants have responded to the status probe
 

 

 Trial info EEG processed data
 

 1: Main condition (MainCon)
  1 = Stick Congruent 
  2 = Stick Incongruenct
  3 = Avatar Congruent
  4 = Avatar Incongruent
 2: Cue condition left or right (MConCueLR)
  1 = Stick Congruent Cue shifts left
  2 = Stick Congruent Cue shifts right
  3 = Stick Incongruent Cue shifts left
  4 = Stick Incongruent Cue shifts right
  5 = Avatar Congruent Cue shifts left
  6 = Avatar Congruent Cue shifts right
  7 = Avatar Incongruent Cue shifts left
  8 = Avatar Incongruent Cue shifts right
 3: Condition from experiment build (con) 
  1 = Congruent, cueshift L, items Left, same location 
  2 = Congruent, cueshift L, items Left, dif location 
  3 = Congruent, cueshift R, items Right, same location 
  4 = Congruent, cueshift R, items Right, dif location 
  5 = Incongruent, cueshift L, items Right, same location 
  6 = Incongruent, cueshift L, items Right, dif location 
  7 = Incongruent, cueshift R, items Left, same location 
  8 = Incongruent, cueshift R, items Left, dif location 
 4: Validity 
  1 = valid (congruent)
  2= invalid (incongruent)
 

 5: Location
  1 = Same (i.e. probe at same location as when initially presented)
  2 = Different (i.e. probe at different location as when initially presented)
 6 Cue 
  1 = Stick cue
  2 = Avatar cue
 7 Left or Right cue (LorR) specific to the cue shift
  1 = Left Stick
  2 = Right Stick
  3 = Left Avatar
  4 = Right Avatar
 8 Start: Sample time for start of the trial
 9 Cue: Sample time for cue onset
 10 Targets: Sample time for target onset
 11 Maintenance: Sample time for start of mainanance interval 
 12 Location probe: Sample time for location probe being shown
 13 Location response time: Sample time for response to location probe being made
 14 Status question: Sample time for status question being asked
 15 Status response time: Sample time for response to status question being made
 16 Accuracy for the location question
 17 Accuracy for the status question",1,1,0
ds003703,2021-06-18 3:26:19,Evgenii Kalenkovich,1.0.0,Frequency Tagging of Syntactic Structure or Lexical Properties,2021-06-23 7:43:39,1,1,99158100000,0.099 TB,448,34,18,38,1.4.0,CC0,"Evgenii Kalenkovich, Anna Shestakova, Nina Kazanina",,,"The study has been funded by the International Laboratory for Social Neuroscience of the Institute for Cognitive Neuroscience HSE, RF Government grant # 075-15-2019-1930.",,10.18112/openneuro.ds003703.v1.0.0,,"listeningToSpeech, rest",,MEG,"References
 ----------
 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., Moreau, J., Oostenveld, R., Schoffelen, J., Tadel, F., Wexler, J., Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5, 180110. http://doi.org/10.1038/sdata.2018.110",1,1,0
ds004457,2023-02-01 14:05:34,Harvey Huang,1.0.2,Electrical stimulation of temporal and limbic circuitry produces distinct responses in human ventral temporal cortex,2023-05-17 20:25:49,0,1,11705300000,10.9 GB,2751,5,13,46,v 1.9.9,CC0,"Harvey Huang, Nicholas M Gregg, Gabriela Ojeda Valencia, Benjamin H Brinkmann, Brian N Lundstrom, Gregory A Worrell, Kai J Miller, Dora Hermes","Cindy Nelson, Karla Crockett","This dataset is part of the paper on 'Electrical stimulation of temporal and limbic circuitry produces distinct responses in human ventral temporal cortex' by H Huang, NM Gregg, G Ojeda Valencia, BH Brinkmann, BN Lundstrom, GA Worrell, KJ Miller, and D Hermes. This project is funded by the National Institute Of Mental Health of the National Institutes of Health under Award Number R01MH122258 to Dora Hermes (Mayo Clinic) and by the National Institute of General Medical Sciences of the National Institutes of Health under Award Number T32GM065841 to the Medical Scientist Training Program at Mayo Clinic. The data was collected by Harvey Huang (Mayo Clinic), Dora Hermes (Mayo Clinic), Nick Gregg (Mayo Clinic), Brian Lundstrom (Mayo Clinic) and Cindy Nelson (Mayo Clinic). The BIDS formatting was performed by Harvey Huang (Mayo Clinic), Dora Hermes (Mayo Clinic) and Gabriela Ojeda Valencia (Mayo Clinic). The iEEG data collection was facilitated by Gregory Worrell and Kai J. Miller (Mayo Clinic)",R01 MH122258 CRCNS: Processing speed in the human connectome across the lifespan ===NEMAR-SEP=== T32 GM065841: Medical Scientist Training Program at Mayo Clinic,https://doi.org/10.1101/2022.07.06.498994,doi:10.18112/openneuro.ds004457.v1.0.2,,"ccep task with annotations, ccep and movement task with annotations, downsampled",,iEEG,"# Basis Profile Curve identification in the human ventral temporal cortex
 

 This dataset contains intracranial EEG recordings from five patients during single pulse electrical stimulation as described in:
  
 * Huang, H., Gregg, N. M., Valencia, G. O., Brinkmann, B. H., Lundstrom, B. N., Worrell, G. A., ... & Hermes, D. (2023). Electrical stimulation of temporal and limbic circuitry produces distinct responses in human ventral temporal cortex. Journal of Neuroscience.
 DOI: https://doi.org/10.1523/JNEUROSCI.1325-22.2023
 

 Please cite this work when using the data. These data were recorded at the Mayo Clinic in Rochester, MN, as part of the NIH Brain Initiative supported project R01 MH122258 ""CRCNS: Processing speed in the human connectome across the lifespan"". Research reported in this publication was supported by the National Institute Of Mental Health of the National Institutes of Health under Award Number R01MH122258 and by the National Institute of General Medical Sciences of the National Institutes of Health under Award Number T32GM065841. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. The data was collected by Harvey Huang, Dora Hermes, Nick Gregg, Brian Lundstrom, Cindy Nelson, Gregg Worrell and Kai J. Miller. The BIDS formatting was performed by Harvey Huang, Dora Hermes and Gabriela Ojeda Valencia. 
 

 Data can be analyzed using the Matlab code at: 
 * https://github.com/hharveygit/VTCBPC_JNS_Manu
 

 ## Format
 Data are formatted according to BIDS version 1.9.9
 

 ## Single pulse stimulation
 The patient were resting in the hospital bed, while single pulse stimulation was performed with a frequency of ~0.2 Hz. The stimulation had a duration of 200 microseconds, was biphasic and had an amplitude of 6mA.
 

 ## Contact
 Please contact Dora Hermes (hermes.dora@mayo.edu) for questions.",1,0,0
ds003710,2021-06-28 19:41:35,Meghan Puglia,1.0.2,APPLESEED Example Dataset,2021-07-12 19:38:41,1,4,10934700000,0.011 TB,293,13,0,0,v1.6.0,CC0,"Cabell L. Williams, Meghan H. Puglia",,"Please cite: Puglia, M.H., Slobin, J.S., & Williams, C.L., 2021. The Automated Preprocessing Pipe-Line for the Estimation of Scale-wise Entropy from EEG Data (APPLESEED): Development and validation for use in pediatric populations. bioRxiv. https://doi.org/10.1101/2021.07.10.450198., and Puglia, M.H., Krol, K.M., Missana, M., Williams, C.L., Lillard, T.S., Morris, J.P., Connelly, J.J., & Grossmann, T. (2020). Epigenetic tuning of brain signal entropy in emergent human social behavior. BMC medicine, 18(1), 1-24. https://doi.org/10.1186/s12916-020-01683-x",,,doi:10.18112/openneuro.ds003710.v1.0.2,,APPLESEED example,,EEG,"The APPLESEED Example Dataset
 

 This dataset consists of longitudinal EEG recordings from 13 infants at 4, 8, and 12 months of age. Test-retest reliability was assessed at 4 months of age via two appointments (session 1 & 2) that occurred within 1 week of each other. Session 3 data was recorded at 8 months of age and session 4 data was recorded at 12 months of age. Two participants did not return for longitudinal testing at sessions 3 & 4. Therefore, the complete dataset consists of 48 recording sessions, with reliability and longitudinal data (sessions 1-4) for 11 infants (6 F), and reliability data only (sessions 1 & 2) for an additional 2 infants. A channel location file and bin file for analysis are included in the ""code"" directory.
 

 This dataset was used to develop and validate the Automated Preprocessing Pipe-Line for the Estimation of Scale-wise Entropy from EEG Data (APPLESEED) and is provided as an example dataset to accompany Puglia, M.H., Slobin, J.S., & Williams, C.L., 2022. The Automated Preprocessing Pipe-Line for the Estimation of Scale-wise Entropy from EEG Data (APPLESEED): Development and validation for use in pediatric populations. Developmental Cognitive Neuroscience, 101163.
 

 APPLESEED code is available to download from https://github.com/mhpuglia/APPLESEED.
 

 This dataset is part of a larger, ongoing longitudinal study initially described in Puglia, M.H., Krol, K.M., Missana, M., Williams, C.L., Lillard, T.S., Morris, J.P., Connelly, J.J. and Grossmann, T., 2020. Epigenetic tuning of brain signal entropy in emergent human social behavior. BMC medicine, 18(1), pp.1-24. https://doi.org/10.1186/s12916-020-01683-x.",1,1,0
ds003739,2021-07-22 23:11:48,Steven Peterson,1.0.3,Perturbed beam-walking task,2021-07-23 0:06:01,1,4,11742600000,0.012 TB,965,30,18,34,v1.6.0,CC0,"Steven Peterson, Daniel Ferris",We thank Estefania Rios and Zachary Ohs for their assistance with data collections and processing the motion capture data. We would also like to thank members of the Human Neuromechanics Laboratory for input regarding the experiment design and data processing.,"Please cite:
 

 Peterson, S. M., & Ferris, D. P. (2021). Human electrocortical, electromyographical,
 ocular, and kinematic data during perturbed walking and standing.
 Data Brief, 39.
 DOI: 10.1016/j.dib.2021.107635
 

 Peterson SM & Ferris DP (2018). Differentiation in theta and beta electrocortical activity between visual and physical perturbations to walking and standing balance. eNeuro, 5(4): ENEURO.0207-18.2018.
 DOI: 10.1523/ENEURO.0207-18.2018",National Science Foundation (DGE 1256260) ===NEMAR-SEP=== Army Research Laboratory (W911NF-10-2-0022) ===NEMAR-SEP=== National Institutes of Health (R01 NS104772),https://doi.org/10.1016/j.neuroimage.2019.05.038 ===NEMAR-SEP=== https://doi.org/10.1523/ENEURO.0207-18.2018 ===NEMAR-SEP=== https://doi.org/10.1016/j.dib.2021.107635,doi:10.18112/openneuro.ds003739.v1.0.3,University of Michigan Health Sciences and Behavioral Sciences Institutional Review Board,mixed,,EEG,"Data was collected at the University of Michigan by Steven Peterson in the lab of Daniel Ferris. This study's protocol was approved by the University of Michigan Institutional Review Board and all participants provided written consent. Please see our data publication for a more comprehensive description of this dataset: https://doi.org/10.1016/j.dib.2021.107635.
 

 Each data file includes synchronized 128-channel EEG, lower leg EMG, neck EMG, EOG, and motion capture data. Participants performed four 10-minute, same-day sessions where they either stood or walked at 0.22 m/s on a treadmill-mounted balance beam that was 2.5 cm tall and 12.7 cm wide.
 

 During each session, participants were exposed to sensorimotor perturbations (either virtual-reality-induced visual field rotations or side-to-side waist pulls, lasting 0.5 seconds and 1 second in duration, respectively). Each session involved 150 perturbation events, balanced between rotation/pull directions.
 

 We have included the indices of all good channels for each participant in EEG.etc.good\_chans of each .set file (includes non-EEG channel indices). Criteria for determining good/bad EEG channels can be found in our eNeuro publication. EEG.etc also includes the resulting ICA sphere and weight matrices when run on only the EEG channels, along with the selected good IC's that were retained for our analyses.",1,1,0
ds003751,2021-07-30 10:57:42,SUDHAKAR MISHRA,1.0.6,Dataset on Emotion with Naturalistic Stimuli,2021-08-08 3:26:55,1,1,5057920000,0.005 TB,200,40,19,26,1.4,CC0,"Sudhakar Mishra, Md. Asif, Uma Shanker Tiwary, Narayanan Srinivasan","We wish to extend our sincere thanks to Prof. Narayanan Srinivasan, CBCS, University of Allahabad. He kindly gave the SM some very useful pieces of advice related to the ex- periment. Dr Sonia baloni ray, IIIT-Delhi, has introduced SM to the EEG technology and taught him how to use it. She demonstrated it to him by letting him participate in her attention study. Then, we would like to thank Mr Amit Ti- wary (then M.tech student), Mr Pravin Srivastav (then PhD candidate), and Ms Anandpreet Kaur (then PhD candidate) for helping us in conducting the validation study.","Mishra, Sudhakar, Mohammad Asif, and Uma Shankar Tiwary. ""Dataset on Emotions using Naturalistic Stimuli (DENS)."" bioRxiv (2021).
 

 Mishra, S., Asif, M., & Tiwary, U. S. (2021). Naturalistic paradigm reveals multi-component emotion dynamics in theta and beta bands using DENS dataset. bioRxiv, 2021-08.
 

 Asif, M., Mishra, S., Vinodbhai, M. T., & Tiwary, U. S. (2023). Emotion Recognition using Temporally Localized Emotional Events in EEG with Naturalistic Context: DENS# Dataset. IEEE Access.",,"Mishra, Sudhakar, Mohammad Asif, and Uma Shankar Tiwary. ""Dataset on Emotions using Naturalistic Stimuli (DENS)."" bioRxiv (2021). ===NEMAR-SEP=== Mishra, S., Asif, M., & Tiwary, U. S. (2021). Naturalistic paradigm reveals multi-component emotion dynamics in theta and beta bands using DENS dataset. bioRxiv, 2021-08.",doi:10.18112/openneuro.ds003751.v1.0.6,,Emotion,,EEG,"Overview
 --------
 This is the ""Emotion"" dataset. The Dataset is recorded with naturalistic paradigm
 

 In brief, it contains EEG, ECG and EMG data for 40 subjects emotionally stimulated using naturalistic emotion stimuli. The stimuli are multimedia videos providing context to understand the situated conceptualization of emotions.
 For details, see the `Details about the experiment` section.
 

 

 Citing this dataset
 -------------------
 Please cite as follows:
 Sudhakar Mishra and Md. Asif and Uma Shanker Tiwary and Narayanan Srinivasan(2023). Dataset on Emotion with Naturalistic Stimuli (DENS). OpenNeuro. [Dataset] doi: https://doi.org/10.18112/openneuro.ds003751.v1.0.5
 

 For more information, see the `dataset\_description.json` file.
 

 

 License
 -------
 This eeg\_emotion dataset is made available under the Open Database
 License: See the LICENSE file. A human readable information can be found at:
 

 https://opendatacommons.org/licenses/odbl/summary/
 

 Any rights in individual contents of the database are licensed under the
 Database Contents License: http://opendatacommons.org/licenses/dbcl/1.0/
 

 

 Dataset Description
 ------
 Dataset\\_description file described the metadata for the dataset. Participants related details are described in participants.json and participants.tsv files. Each subject directory contains two directories- beh and eeg. A tsv file inside beh folder having entries about the feedbacks given by subject on self-assessment scales-valence, arousal, dominance, liking, familiarity, relevance and emotion category. In addition, it contains the information about the time-stamp of mouse click and other details. The eeg folder inside subject directory contains the raw eeg data in .set \& .fdt format along with the information about task events in \\_task-emotion\\_events.tsv file. The stimuli directory contains stimuli which were used during the experiment. In addition, feedback excel sheet participant\\_details.xlsx filled by participants is also added. The code directory contains the python code for data collection, python code for data validation and matlab file for pre-processing the raw data.",1,1,0
ds003753,2021-07-30 16:12:01,James F Cavanagh,1.1.0,Probabilistic Learning with Affective Feedback: Exp #2,2021-09-29 17:02:08,0,1,4965120000,0.005 TB,589,25,18,27,1.1.1,CC0,"Darin R. Brown, Trevor Jackson, James F Cavanagh",,,1R01MH119382-01,pending,10.18112/openneuro.ds003753.v1.1.0,,ProbabilisticSelection,,EEG,"RL task in N=25 college age participants. Data collected circa 2019 in the CRCL at UNM. The paper [Brown, D.R., Jackson, T.J. & Cavanagh, J.F. The Reward Positivity is sensitive to affective liking] Should be coming out in Cognitive, Affective, & Behavioral Neuroscience. THIS IS EXPERIMENT #2. Your best bet for understanding this task would be to read that paper first.  Note we have since made minor adjustments to the task which really enhance the ability to resolve the RewP. I also have analytic scripts for it. If you are interetsted in running this task, contact me for the new version. - James F Cavanagh 07/02/2021",1,1,0
ds003754,2021-08-02 9:42:16,Sen Wan,1.0.2,Conscious-SEEG-Dataset,2021-08-06 2:20:39,0,1,17789600000,0.018 TB,213,44,1,35,1.2,CC0,"Sen Wan, Feng Bao, Chen Yao, Fengpeng Wang, Bohan Li, Shi Mao, Wendi Yu, Kexin Zhang, Youyong Kong, Xiaobin Zhang, Xiaoping Du, Xiaodong Cai, Lu Fang, Yue Deng, Yi Yao, Qionghai Dai",,,,,10.18112/openneuro.ds003754.v1.0.2,,"ana2con, con2ana",,iEEG,"Mapping brain dynamics in anesthesia-induced unconsciousness with a human stereoelectroencephalography resource
 

 

 Consciousness Study
 ------------------------------------------
 

 SEEG is organized in our study with a total of 44 subjects under the anesthesia-induced consciousness switches.  
 

 For additional clinical metadata about each subject, refer to the clinical Excel table in the publication.
 

 

 Data Availability
 -----------------
 Xiamen Humanity Hospital agreed to share.
 

 All data were approved to be de-identified and shared. All data in this dataset have no identifiers associated with the patient. 
 

 

 Sourcedata
 ----------
 

 For each subject, there was a raw EDF file, which was converted into the BrainVision format with `FieldTrip`.
 

 Each subject also has an 'anat' folder which contains 'anat\_t1.nii', 'postop\_ct.nii', 'glanat.nii' and 'glpostop\_ct.nii'. The first two are origin preoperative MRI and postoperative CT. The latter two are MRI and CT which were registered to the MNI template.
 Each subject also has 'SEEG\_COORDINATE.mat' in 'ieeg' folder. 'SEEG\_COORDINATE.mat' contains the coordinates in MNI space of each electrode contact. The corresponding index for AAL atlas was also included. 
 

 Subjects used in the article(Mapping brain dynamics in anesthesia-induced unconsciousness with a human stereoelectroencephalography resource) also contain ‘SEEG.mat’, which is the preprocessed data with 0.3~200Hz bandpass filtering and 50Hz notch filtering.
 

 

 

 Events and Descriptions
 -----------------------
 Within each EDF file, there contain event markers that are annotated by clinicians, which may inform you of specific clinical events that are occurring in time. The events were labeled for the consciousness state. Labels such as eyes open, spontaneous breathing, swallow, choking cough were regarded as the return of consciousness.",1,1,0
ds003766,2021-08-15 10:03:58,const,2.0.3,HD-EEG task with mouse tracking,2021-08-19 13:50:18,1,1,76566000000,0.077 TB,1028,31,18,33,1.6.0,CC0,"Kun Chen, Ruien Wang, Jiamin Huang, Fei Gao, Zhen Yuan, Yanyan Qi, Haiyan Wu",,,,"Chen, K., Wang, R., Huang, J., Gao, F., Yuan, Z., Qi, Y., & Wu, H. (2022). A resource for assessing dynamic binary choices in the adult brain using EEG and mouse-tracking. Scientific Data, 9(1), 416. https://doi.org/10.1038/s41597-022-01538-5",doi:10.18112/openneuro.ds003766.v2.0.3,,"foodchoice, imagechoice, resting, wordchoice",,EEG,"# A resource for assessing dynamic binary choices in the adult brain using EEG and mouse-tracking
 

 ## Description
 

 This dataset was collected in 2020, which combines high-density Electroencephalography (HD-EEG, 128 channels) and mouse-tracking intended as a resource for examining the dynamic decision process of semantics and preference choices in the human brain. The dataset includes high-density resting-state and task-related (food preference choices and semantic judgments) EEG acquired from 31 individuals (ages: 18-33).
 

 ## EEG acquisition
 

 The EEG data were acquired using a 128-channel cap based on the standard 10/20 System with Electrical Geodesics Inc (EGI, Eugene, Oregon) system. During recording, sampling rate was 1000Hz, and the E129 (Cz) electrode was used as reference. Electrode impedances were kept below 50kohm for each electrode during the experiment.
 

 ## Main files
 

 **`sub-*`**: EEG (`.set`) and behavior data with BIDS format.
 

 **`sourcedata/rawdata`**: Raw `.mff` EGI data and behavior data with subject information desensitization.
 

 **`sourcedata/psychopy`**: Stimuli and PsychoPy scripts for presentation.
 

 **`derivatives/eeglab-preproc`**: Preprocessed continuous EEG data with EEGLAB (Easy to set different epoch time windows for further analysis).
 

 ## Others
 

 Please refer to the [corresponding paper](https://doi.org/10.1038/s41597-022-01538-5) and [GitHub code](https://github.com/andlab-um/MT-EEG-dataset) to get more details.
 

 ## References
 

 Chen, K., Wang, R., Huang, J., Gao, F., Yuan, Z., Qi, Y., & Wu, H. (2022). A resource for assessing dynamic binary choices in the adult brain using EEG and mouse-tracking. Scientific Data, 9(1), 416. https://doi.org/10.1038/s41597-022-01538-5
 

 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G., Phillips, C., Delorme, A., Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Scientific Data, 6, 103. https://doi.org/10.1038/s41597-019-0104-8",1,1,0
ds003768,2021-08-16 22:59:17,yameng gu,1.0.11,Simultaneous EEG and fMRI signals during sleep from humans,2021-08-17 16:50:30,1,1,92938500000,0.093 TB,1344,33,0,0,1.4.1,CC0,"Yameng Gu, Feng Han, Lucas E. Sainburg, Margeaux M. Schade, Xiao Liu","We would like to express special thanks to Dr. Thomas Beck and Dr. Josef Pfeuffer, Siemens AG, Healthcare Sector for providing us with the Advanced fMRI package WIP package. We would also like to thank Dr. Xiaoxiao Bai, Jack W. Williams and Youyou Yu for their help during the experiment. Data analysis were conducted using the computing resources provided by the Institute for Computational and Data Sciences at the Pennsylvania State University (https://icds.psu.edu).","Please cite the following two papers: 
 

 An orderly sequence of autonomic and neural events at transient arousal changes, Yameng Gu, Feng Han, Lucas E. Sainburg, Margeaux M. Schade, Orfeu M. Buxton, Jeff H. Duyn, Xiao Liu, NeuroImage, 2022, https://doi.org/10.1016/j.neuroimage.2022.119720;
 

 Simultaneous EEG and functional MRI data during rest and sleep from humans, Yameng Gu, Lucas E. Sainburg, Feng Han, Xiao Liu, Data in Brief, 2023, https://doi.org/10.1016/j.dib.2023.109059.",National Institutes of Health Pathway to Independence Award K99/R00 (5R00NS092996-03) ===NEMAR-SEP=== Brain Initiative Award (1RF1MH123247-01) ===NEMAR-SEP=== National Institutes of Health R01 Award (1R01NS113889-01A1),"An orderly sequence of autonomic and neural events at transient arousal changes, Yameng Gu, Feng Han, Lucas E. Sainburg, Margeaux M. Schade, Orfeu M. Buxton, Jeff H. Duyn, Xiao Liu, NeuroImage, 2022, https://doi.org/10.1016/j.neuroimage.2022.119720 ===NEMAR-SEP=== Simultaneous EEG and functional MRI data during rest and sleep from humans, Yameng Gu, Lucas E. Sainburg, Feng Han, Xiao Liu, Data in Brief, 2023, https://doi.org/10.1016/j.dib.2023.109059",doi:10.18112/openneuro.ds003768.v1.0.11,,"rest, sleep",,"EEG, MRI","This dataset included 33 healthy participants collected at Pennsylvania State University with informed consent. Simultaneously collected EEG and BOLD signals for each participant were recorded and organized at each folder. 
 

 Each scanning section consisted of an anatomical session, two 10-min resting-state sessions, and several 15-min sleep sessions. The first resting-state session was conducted before a visual-motor adaptation task (Albouy et al, Journal of Sleep Research, 2013) and the second resting-state session was conducted after a visual-motor adaptation task. 
 

 The scored sleep stages for these 33 subjects were organized under 'sourcedata' folder. Each TSV file contained the sleep stages for each 30-sec epoch across different sessions for each subject. In the TSV file, “w” represents wakefulness and “1, 2, 3” represents NREM1, NREM2, and NREM3, respectively. Some epochs scoring with uncertainty are noted as “uncertain” and some epochs with too large artifacts to score reasonably are noted as “unscorable”. 
 

 MR imaging data were collected on a 3 Tesla Prisma Siemens Fit scanner using a Siemens 20-channel receive-array coil. Anatomical images were acquired using a MPRAGE sequence (TR: 2300 milliseconds, TE: 2.28 milliseconds, 1mm isotropic spatial resolution, FOV: 256 millimeters, flip angle: 8 degrees, matrix size: 256×256×192, acceleration factor: 2). Blood oxygenation level-dependent (BOLD) fMRI data were acquired using an EPI sequence (TR: 2100 milliseconds, TE: 25 milliseconds, slice thickness: 4mm, slices: 35, FOV: 240mm, in-plane resolution: 3mm×3mm).
 

 EEG data were collected using a 32-channel MR-compatible EEG system from Brain Products, Germany. Electrodes were placed based on the 10-20 international system. EOG and ECG recorded eye movement and cardiac signal, respectively. EEG data were collected at a sampling rate of 5000 Hz with a band-pass filter of 0-250 Hz. R128 in the EEG signals corresponds to the BOLD fMRI volume trigger. S1 markers in the EEG during sleep sessions correspond to participants hitting buttons indicating wakefulness state. S2 and S3 markers during sleep sessions represent no button hitting and can be ignored.
 

 For more information or any questions about this dataset, please see the two papers listed in the References and Links section or contact Dr. Yameng Gu (ymgu95@gmail.com)",1,1,0
ds003774,2021-08-23 10:09:25,Krishna Miyapuram,1.0.2,Music Listening- Genre EEG dataset (MUSIN-G),2021-08-24 7:53:32,0,12,10863700000,0.011 TB,1685,20,0,0,1.1.1,CC0,"Krishna Prasad Miyapuram, Pankaj Pandey, Nashra Ahmad, Bharatesh R Shiraguppi, Esha Sharma, Prashant Lawhatre, Dhananjay Sonawane, Derek Lomas",,"Pandey, P., Ahmad, N., Miyapuram, K. P., & Lomas, D. (2021, December). Predicting dominant beat frequency from brain responses while listening to music. In 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) (pp. 3058-3064). IEEE.
 

 Pankaj Pandey, Gulshan Sharma, Krishna P. Miyapuram, Ramanathan Subramanian, Derek Lomas: Music Identification Using Brain Responses to Initial Snippets. ICASSP 2022: 1246-1250
 

 Dhananjay Sonawane, Krishna Prasad Miyapuram, Bharatesh RS, Derek J. Lomas:
 GuessTheMusic: Song Identification from Electroencephalography response. COMAD/CODS 2021: 154-162","This research was funded by PlayPower Labs, USA",,doi:10.18112/openneuro.ds003774.v1.0.2,,MusicListening,,EEG,"The dataset contains Electroencephalography (EEG) responses from 20 Indian participants, on 12 songs of different genres (from Indian Classical to Goth Rock). Each session indicates a song by its number. 
 

 For the experiment, the participants were indicated to close their eyes indicated by a single beep, and the song was presented to them on speakers. After listening to each song, a double beep was presented, asking them to open their eyes and rate their familiarity and enjoyment to the song. The responses were taken on a scale of 1 to 5, where 1 meant most familiar or most enjoyable, and 5 meant least familiar or least enjoyable. 
 

 The events timeline in segmented data must be ignored as these are inherited from rawdata",1,1,0
ds003775,2021-08-25 11:43:32,Christoffer Hatlestad-Hall,1.2.1,SRM Resting-state EEG,2021-08-25 12:12:25,1,2,4815800000,0.005 TB,617,111,17,71,1.6.0,CC0,"Christoffer Hatlestad-Hall, Trine Waage Rygvold, Stein Andersson",,"Reference to:
 Hatlestad-Hall, C., Rygvold, T. W., & Andersson, S. (2022). BIDS-structured resting-state electroencephalography (EEG) data extracted from an experimental paradigm. Data in Brief, 45, 108647. https://doi.org/10.1016/j.dib.2022.108647",,,doi:10.18112/openneuro.ds003775.v1.2.1,,resteyesc,,EEG,"# SRM Resting-state EEG
 

 ## Introduction
 

 This EEG dataset contains resting-state EEG extracted from the experimental
 paradigm used in the *Stimulus-Selective Response Modulation* (SRM) project at
 the Dept. of Psychology, University of Oslo, Norway.
 

 The data is recorded with a BioSemi ActiveTwo system, using 64 electrodes
 following the positional scheme of the extended 10-20 system (10-10).
 Each datafile comprises four minutes of uninterrupted EEG acquired while the
 subjects were resting with their eyes closed. The dataset includes EEG from
 111 healthy control subjects (the ""t1"" session), of which a number underwent
 an additional EEG recording at a later date (the ""t2"" session). Thus, some
 subjects have one associated EEG file, whereas others have two.
 

 ### Disclaimer
 

 The dataset is provided ""as is"". Hereunder, the authors take no responsibility
 with regard to data quality. The user is solely responsible for ascertaining
 that the data used for publications or in other contexts fulfil the required
 quality criteria.
 

 

 

 ## The data
 

 ### Raw data files
 

 The raw EEG data signals are rereferenced to the average reference. Other than
 that, no operations have been performed on the data. The files contain no
 events; the whole continuous segment is resting-state data. The data signals
 are unfiltered (recorded in Europe, the line noise frequency is 50 Hz). The
 time points for the subject's EEG recording(s), are listed in the *\_scans.tsv
 file (particularly interesting for the subjects with two recordings).
 

 Please note that the quality of the raw data has **not** been carefully
 assessed. While most data files are of high quality, a few might be of poorer
 quality. The data files are provided ""as is"", and it is the user's
 esponsibility to ascertain the quality of the individual data file.
 

 

 ### /derivatives/cleaned\_data
 

 For convenience, a cleaned dataset is provided. The files in this derived
 dataset have been preprocessed with a basic, fully automated pipeline (see
 /code/s2\_preprocess.m for details) directory for details. The derived files are
 stored as EEGLAB .set files in a directory structure identical to that of the
 raw files. Please note that the *\*\_channels.tsv* files associated with the
 derived files have been updated with status information about each channel
 (""good"" or ""bad""). The ""bad"" channels are – for the sake of consistency –
 interpolated, and thus still present in the data. It might be advisable to
 remove these channels in some analyses, as they (per definition) do not provide
 anything to the EEG data. The cleaned data signals are referenced to the
 average reference (including the interpolated channels).
 

 Please mind the automatic nature of the employed pipeline. It might not perform
 optimally on all data files (*e.g.* over-/underestimating proportion of bad
 channels). For publications, we recommend implementing a more sensitive
 cleaning pipeline.
 

 

 ### Demographic and cognitive test data
 

 The *participants.tsv* file in the root folder contains the variables age,
 sex, and a range of cognitive test scores. See the sidecar participants.json
 for more information on the behavioural measures. Please note that these
 measures were collected in connection with the ""t1"" session recording.
 

 

 

 ## How to cite
 

 All use of this dataset in a publication context requires the following paper
 to be cited:
 

 Hatlestad-Hall, C., Rygvold, T. W., & Andersson, S. (2022). BIDS-structured
 resting-state electroencephalography (EEG) data extracted from an experimental
 paradigm. Data in Brief, 45, 108647. https://doi.org/10.1016/j.dib.2022.108647
 

 

 

 ## Contact
 

 Questions regarding the EEG data may be addressed to
 Christoffer Hatlestad-Hall (chr.hh@pm.me).
 

 Question regarding the project in general may be addressed to
 Stein Andersson (stein.andersson@psykologi.uio.no) or
 Trine W. Rygvold (t.w.rygvold@psykologi.uio.no).",1,1,0
ds003800,2021-09-12 18:22:56,Mojtaba Lahijanian,1.0.0,Auditory Gamma Entrainment,2021-09-28 20:17:41,0,1,198444000,0 TB,118,13,0,0,1.0.0,CC0,"Mojtaba Lahijanian, Mohammad Javad Sedghizadeh, Hamid Aghajan, Zahra Vahabi",The authors wish to thank Ziaeian Hospital in Tehran for providing staff time and equipment for data collection in this study. We are grateful to the patients and their families who participated in this study.,,"This work was partially funded by the Cognitive Sciences & Technologies Council of Iran and by the grant G970736 from Sharif University of Technology, which covered the cost of data collection. The funders had no role in the study conceptualization and design, data collection and analysis, decision to publish, or preparation of the manuscript.",,10.18112/openneuro.ds003800.v1.0.0,This study was approved by the Review Board of Tehran University of Medical Sciences (Approval ID: IR.TUMS.MEDICINE.REC.1398.524) and all participants provided informed consent before participating and were free to withdraw at any time.,"AuditoryGammaEntrainment, Rest",,EEG,"> Introduction
 This experiment was designed to entrain the brain oscillations through synthetic auditory stimulation conducted on a group of elderly suffering from dementia. Recently, gamma entrainment has been proposed and shown effective in improving several symptoms of Alzheimer's Diseases (AD). The aim of this study is to investigate the effect of entrainment on brain oscillations using EEG signal recording during the auditory brain stimulation. 
 This study was approved by the Review Board of Tehran University of Medical Sciences (Approval ID: IR.TUMS.MEDICINE.REC.1398.524) and all participants provided informed consent before participating and were free to withdraw at any time.
 

 > Rest data
 Before the main task, a one-minute data was recorded with open eyes for measuring raw resting-state potentials. The rest data for participants number 6 and 13 are missing.
 

 > Auditory stimulation
 Two speakers were placed in front of the participant 50cm apart from each other and directly pointed at the participant's ears at a distance of 50cm. The sound intensity was around -40dB within a fixed range for all participants. Before starting the task, the participant was asked if the volume was loud enough and the sound volume was set at a comfortable level for each participant. The auditory stimulus was a 5kHz carrier tone amplitude modulated with a 40Hz rectangular wave (40Hz On and Off cycles). Since a 40Hz audio signal cannot be easily heard, the 5KHz carrier frequency was used to render the 40Hz pulse train audible. In order to minimize the effect of the carrier sound, the duty cycle of the modulating 40Hz waveform was set to 4\% (1ms of the 25ms cycle was On). The auditory stimulant was generated in MATLAB and played as a .wav file. This file consisted of six trials of 40sec stimulus interleaved by five trials of 20sec rest (silence). The entire session resulted in 340sec (6*40+5*20) of recorded EEG signal. 
 

 > EEG recording and preprocessing 
 All EEG data were recorded using 19 monopolar channels in the standard 10/20 system referenced to the earlobes, sampled at 250Hz, and the impedance of the electrodes was kept under 20kOhm. 
 Data from all the participants were preprocessed identically following Makoto's preprocessing pipeline: Highpass filtering above 1Hz; removal of the line noise; rejecting potential bad channels; interpolating rejected channels; re-referencing data to the average; Artifact Subspace Reconstruction (ASR); re-referencing data to the average again; estimating the brain source activity using independent component analysis (ICA); dipole fitting; rejecting bad dipoles (sources) for further cleaning the data. These preprocessing steps were performed using EEGLab MATLAB toolbox.
 

 > Instructions
 During the experiment, participants were seated comfortably with open eyes in a quiet room. They were instructed to relax their body to avoid muscle artifacts and move their head as little as possible.",1,1,0
ds003801,2021-09-13 16:11:11,Lisa Straetmans,1.0.0,Neural Tracking to go,2021-11-15 12:45:01,0,1,1233080000,0.001 TB,165,20,20,40,v2.0,CC0,"Lisa Straetmans, Bjoern Holtze, Stefan Debener, Manuela Jaeger, Bojana Mirkovic",,,,article link follows,doi:10.18112/openneuro.ds003801.v1.0.0,,NeuralTrackingToGo,,EEG,"This mobile EEG auditory attention experiment consists of 20 participants.
 In a two-competing speaker paradigm subjects either sat on a chair or walked a route indoors 
 Attention was disrupted by environmental salient eventsfrom in front of the participant 
 

 - Lisa Straetmans (Sep, 2021)",1,1,0
ds003805,2021-09-15 8:31:45,Mojtaba Lahijanian,1.0.0,Multisensory Gamma Entrainment,2021-09-28 20:20:55,0,1,9223980,0 TB,10,1,0,0,1.0.0,CC0,"Mojtaba Lahijanian, Mohammad Javad Sedghizadeh, Hamid Aghajan",,,,,10.18112/openneuro.ds003805.v1.0.0,,MultisensoryGammaEntrainment,,EEG,"> Introduction
 This experiment was designed to study the effects of different sensory modalities (auditory, visual, and audio-visual) on brain entrainment. The EEG data was collected from a young healthy volunteer (23 years old male). Recently, gamma entrainment based on individual (auditory or visual) sensory stimulation as well as simultaneous auditory and visual stimulation have been proposed and shown effective in improving several symptoms of Alzheimer's Diseases (AD) in mice and humans. The aim of this study is to investigate the effect of different modalities in producing synchronized brain oscillations. The task is composed of three epochs of auditory, visual, and audio-visual stimulations respectively, each lasting for 40sec in one session.  
 

 > Auditory stimulation
 Two speakers were placed in front of the participant 50cm apart from each other and directly pointed at the participant's ears at a distance of 50cm. The sound intensity was set to around -40dB. Before starting the task, the participant was asked if the volume was loud enough and the sound volume was set at a comfortable level for him. The auditory stimulus was a 5kHz carrier tone amplitude modulated with a 40Hz rectangular wave (40Hz On and Off cycles). Since a 40Hz audio signal cannot be easily heard, the 5KHz carrier frequency was used to render the 40Hz pulse train audible. In order to minimize the effect of the carrier sound, the duty cycle of the modulating 40Hz waveform was set to 4\% (1ms of the 25ms cycle was On). The auditory stimulant was generated in MATLAB and played as a .wav file. This file consisted of 40sec of stimulus. 
 

 > Visual stimulation
 The visual stimulant was a 20Hz flickering white light produced by an array of LEDs and reflected from a white wall at 50cm distance in front of the participant (open eyes) with 50\% On cycles (duty cycle = 50\%) flickering for 40sec. Due to the presence of harmonic frequencies in the pulse train of the stimulus, the 20Hz stimulant is able to drive 40Hz oscillations in the brain. 
 

 > EEG recording and preprocessing 
 The EEG data were recorded using 19 monopolar channels in the standard 10/20 system referenced to the earlobes, sampled at 500Hz, and the impedance of the electrodes was kept under 20kOhm. 
 Data from all three epochs were preprocessed identically following Makoto's preprocessing pipeline: Highpass filtering above 1Hz; removal of the line noise; rejecting potential bad channels; interpolating rejected channels; re-referencing data to the average; Artifact Subspace Reconstruction (ASR); re-referencing data to the average again; estimating the brain source activity using independent component analysis (ICA); dipole fitting; rejecting bad dipoles (sources) for further cleaning the data. These preprocessing steps were performed using EEGLab MATLAB toolbox.
 

 > Instructions
 During the experiment, participant was seated comfortably with open eyes in a quiet room. He was instructed to relax his body to avoid muscle artifacts and move his head as little as possible. The participant was free to take a rest after each epoch but the EEG cap was not taken off.",1,1,0
ds003810,2021-09-20 13:30:31,María Paula Saavedra,2.0.2,Motor Imagery vs Rest - Low-Cost EEG System,2021-09-20 13:44:22,0,1,72366300,0 TB,158,10,20,30,1.1.1,CC0,"Victoria Peterson, Catalina Maria Galvan, Hugo Sacha Hernadez, Ruben Spies",,"@article{peterson2020feasibility, title={A feasibility study of a complete low-cost consumer-grade brain-computer interface system}, author={Peterson, Victoria and Galv{'a}n, Catalina and Hern{'a}ndez, Hugo and Spies, Ruben}, journal={Heliyon}, volume={6}, number={3}, pages={e03425}, year={2020}, publisher={Elsevier} }",,https://www.sciencedirect.com/science/article/pii/S240584402030270X,doi:10.18112/openneuro.ds003810.v2.0.2,,"MIvsRestRUN2, MIvsRestRUN3, MIvsRestRUN0, MIvsRestRUN1, MIvsRestRUN4",,EEG,"This dataset consists of electroencephalography (EEG) signals adquired with a low-cost consumer-grade device. The 10 participants had no previous BCI experience. The BCI protocol consisted of two conditions, namely the kinesthetic imagination of grasping movement (MI) of the dominant hand and rest/idle condition.Five protocol runs were asked to be performed by the user. The first run, called RUN0, involved real grasping movement in order to better explain the protocol and to help the subject to focus on the sensation of making the movement. The rest of the runs (RUN1-RUN4) were equal, consisting of MI vs.Rest conditions. The EMG signals of the dominant hand was adquired for protocol control. During acquisition, the EEG signals were filtered between 0.5 and 45 Hz with a 3rd order Butterworth bandpass-filter.",1,1,0
ds003816,2021-09-24 18:14:41,Eliza R. Sun,1.0.2,Loving Kindness Meditation for Mental Health: long-term and short-term effect,2021-09-25 10:34:29,0,1,0,0 GB,0,0,0,0,1.6,CC0,Ven. GoonFui Wong (Bhikkhu Khemasiri),,,Faculty Research Fund of Faculty of Education in HKU,,doi:10.18112/openneuro.ds003816.v1.0.2,"The University of Hong Kong Human Research Ethics Committee approved the project, Reference Number: EA210145.",,,,,1,1,0
ds003822,2021-09-29 16:25:42,James F Cavanagh,1.1.0,Probabilistic Learning with Affective Feedback: Exp #1,2021-09-29 17:02:18,0,1,6248470000,0.006 TB,259,25,18,36,1.1.1,CC0,"Darin R. Brown, Trevor Jackson, James F Cavanagh",,,,pending,10.18112/openneuro.ds003822.v1.1.0,,ProbabilisticSelection,,EEG,"RL task in N=25 college age participants. Data collected circa 2018 in the CRCL at UNM. The paper [Brown, D.R., Jackson, T.J. & Cavanagh, J.F. The Reward Positivity is sensitive to affective liking] is now coming out in Cognitive, Affective, & Behavioral Neuroscience. Your best bet for understanding this task would be to read that paper first. I've included additional scripts to help understand stimulus triggers etc. These additional scripts were for a secondary analysis: they were not the scripts used for the paper above. So they are slightly different and have some interesting (unfinished) tangents. - James F Cavanagh 09/29/2021",1,1,0
ds003825,2021-09-30 2:31:13,Tijl Grootswagers,1.2.0,"Human electroencephalography recordings from 50 subjects for 22,248 images from 1,854 object concepts",2021-10-01 3:34:07,1,1,44259000000,0.044 TB,257,50,17,30,1.0.2,CC0,"Grootswagers, Tijl, Zhou, Ivy, Robinson, Amanda, Hebart, Martin, Carlson, Thomas",The authors acknowledge the University of Sydney HPC service for providing High Performance Computing resources.,"Grootswagers, T., Zhou, I., Robinson, A.K. et al. Human EEG recordings for 1,854 concepts presented in rapid serial visual presentation streams. Sci Data 9, 3 (2022). https://doi.org/10.1038/s41597-021-01102-7",ARC DP160101300 (TAC) ===NEMAR-SEP=== ARC DP200101787 (TAC) ===NEMAR-SEP=== ARC DE200101159 (AKR),https://osf.io/hd6zk/ ===NEMAR-SEP=== https://doi.org/10.1038/s41597-021-01102-7,doi:10.18112/openneuro.ds003825.v1.2.0,,rsvp,,EEG,"Experiment Details
 Human electroencephalography recordings from 50 subjects for 1,854 concepts and 22,248 images in the THINGS stimulus database.
 Images were presented in rapid serial visual presentation streams at 10Hz rates. Participants performed an orthogonal fixation colour change detection task.
 

 Experiment length: 1 hour
 

 More information: 
 

 https://osf.io/hd6zk/ (osf repository with more information and example analysis code)
 

 Grootswagers, T., Zhou, I., Robinson, A.K. et al. Human EEG recordings for 1,854 concepts presented in rapid serial visual presentation streams. Sci Data 9, 3 (2022). https://doi.org/10.1038/s41597-021-01102-7",1,1,0
ds003838,2021-10-15 10:48:50,Yuri Pavlov,1.0.6,"EEG, pupillometry, ECG and photoplethysmography, and behavioral data in the digit span task",2021-10-19 13:51:25,0,1,107600000000,100.2 GB,947,65,18,44,1.1.1,CC0,"Yuri G. Pavlov, Dauren Kasanov, Alexandra I. Kosachenko, Alexander I. Kotyusov",,https://doi.org/10.1038/s41597-022-01414-2,,"Pavlov, Y. G., Kasanov, D., Kosachenko, A. I., Kotyusov, A. I., & Busch, N. A. (2022). Pupillometry and electroencephalography in the digit span task. Scientific Data, 9(1), 325. https://doi.org/10.1038/s41597-022-01414-2 ===NEMAR-SEP=== Pavlov, Y. G., Gashkova, A. S., Kasanov, D., Kosachenko, A. I., Kotyusov, A. I., & Kotchoubey, B. (2023). Task-evoked pulse wave amplitude tracks cognitive load. Scientific Reports, 13(1), 1–10. https://doi.org/10.1038/s41598-023-48917-5 ===NEMAR-SEP=== Kosachenko, A. I., Kasanov, D., Kotyusov, A. I., & Pavlov, Y. G. (2023). EEG and pupillometric signatures of working memory overload. Psychophysiology, n/a(n/a), e14275. https://doi.org/10.1111/psyp.14275",doi:10.18112/openneuro.ds003838.v1.0.6,,mixed,,EEG,"This dataset consists of raw 64-channel EEG, cardiovascular (electrocardiography and photoplethysmography), and pupillometry data from 86 human participants during 4 minutes of eyes-closed resting and during performance of a classic working memory task – digit span task with serial recall. The participants either memorized (memory) or just listened to (control condition) sequences of 5, 9, or 13 digits presented auditorily with 2 second stimulus onset asynchrony. The dataset can be used for (1) developing algorithms for cognitive load discrimination and detection of cognitive overload; (2) studying neural (event-related potentials and brain oscillations) and peripheral physiological (electrocardiography, photoplethysmography, and pupillometry) signals during encoding and maintenance of each sequentially presented memory item in a fine time scale; (3) correlating cognitive load and individual differences in working memory to neural and peripheral physiology, and studying the relationship between the physiological signals; (4) integration of the physiological findings with the vast knowledge coming from behavioral studies of verbal working memory in simple span paradigms.
 

 EEG, pupillometry, ECG and photoplethysmography, and behavioral data are stored separately in corresponding folders. Each data record can consist of four data folders:
 beh - behavioral data: correctness of the recall in the memory trials
 ecg - electrocardiography (ECG) and photoplethysmography (PPG) data
 eeg - EEG data
 pupil - pupillometry and eye-tracking data
 

 Some of the participants had some physiological data missing:
 sub-017, sub-094 have no pupillometry data
 sub-017, sub-037, sub-066 have no ECG and PPG data
 sub-013, sub-014, sub-015, sub-016, sub-017, sub-018, sub-019, sub-020, sub-021, sub-022, sub-023, sub-024, sub-025, sub-026, sub-027, sub-028, sub-029, sub-030, sub-031, sub-037, sub-066 have no EEG data",1,0,0
ds003844,2021-10-15 18:46:25,Epilab UMCU,1.0.3,RESPect,2021-10-16 12:00:01,0,18,2795790000,0.003 TB,435,6,0,0,Brain Imaging Data Structure v1.6.0,CC0,"Zweiphenning W., Demuru M., van Blooijs D., Hermes D., Leijten F., Zijlmans M.","dAngremont E., Wassenaar M.","Demuru M., van Blooijs D., Zweiphenning W. et al 2020, A practical workflow for organizing clinical intracranial EEG data in the intraoperative and long term monitoring settings in the Brain Imaging Data Structure",Epi-Sign Project ===NEMAR-SEP=== Alexandre Suerman Stipendium 2015 ===NEMAR-SEP=== Epilepsiefonds #17-07,articles and/or links,10.18112/openneuro.ds003844.v1.0.3,,task-acute,,iEEG,"Dataset description
 This dataset is part of a bigger dataset of intracranial EEG (iEEG) called RESPect (Registry for Epilepsy Surgery Patients), a dataset recorded at the University Medical Center of Utrecht, the Netherlands.
 It consists of 12 patients: six patients recorded intraoperatively using electrocorticography (acute ECoG), six patients with long-term recordings (3 patients recorded with ECoG and 3 patients recorded with stereo-encephalography SEEG). For a detailed description see (Demuru M, van Blooijs D, Zweiphenning W, Hermes D, Leijten F, Zijlmans M, on behalf of the RESPect group. “A Practical Workflow for Organizing Clinical Intraoperative and Long-Term iEEG data in BIDS”.).
 

 

 This data is organized according to the Brain Imaging Data Structure specification. A community- driven specification for organizing neurophysiology data along with its metadata. For more information on this data specification, see https://bids-specification.readthedocs.io/en/stable/
 

 

 Each patient has their own folder (e.g., `sub-RESP0280`) which contains the iEEG recordings data for that patient, as well as the metadata needed to understand the raw data and event timing.
 

 

 Two different implementation of the BIDS structure were done according to the different type of recordings (i.e. intraoperative or long-term)
 Intraoperative ECoG
 Surgery with intraoperative ECoG is composed of three main situations that can be logically grouped into BIDS sessions:
 

 

 * Pre-resection sessions, consisting of all recordings (with different configurations of the grid and strips/depth) carried out before the surgeon has started the planned resection.
 

 

 * Intermediate sessions, consisting of all subsequent recordings performed before any iterative extension of the resection area.
 

 

 * Post-resection sessions, consisting of all the recordings performed after the last resection.
 

 

 Each situation is labelled with an increasing number starting from 1, indicative of the period in time respective to the surgical resection and a consecutive letter (starting from A) indicative of the position of the grid and strip/depth for a given session.
 As an example see patient RESP0280 who had 4 sessions recorded: two pre-resection sessions, one intermediate sessions and one post-resection session. The first session is SITUATION1A consisting of the first recording, then the grid was moved to another position, resulting in SITUATION1B. After that, the surgeon resected part of the brain and then there was another recording(SITUATION2A). Finally the surgeon applied a resection for the last time and the recording after that was defined as SITUATION3A.
 

 

 Long-term iEEG
 In long-term recordings, data that are recorded within one monitoring period are logically grouped in the same BIDS session and stored across runs indicating the day and time point of recording in the monitoring period.
 If extra electrodes were added/removed during this period, the session was divided into different sessions (e.g. ses-1A and ses-1b).
 We use the optional run key-value pair to specify the day and the start time of the recording (e.g. run-021315, day 2 after implantation, which is day 1 of the monitoring period, at 13:15).
 The task key-value pair in long-term iEEG recordings describes the patient’s state during the recording of this file. Different tasks have been defined, such as “rest” when a patient is awake but not doing a specific task, “sleep” when a patient is sleeping the majority of the file, or “SPESclin” when the clinical SPES protocol has been performed in this file. Other task definitions can be found in the annotation syntax (https://github.com/UMCU-EpiLAB/umcuEpi\_longterm\_ieeg\_respect\_bids/master/manuals/IFU\_annotatingtrc\_ECoG).
 

 

 License
 This dataset is made available under the Public Domain Dedication and License CC v1.0, whose full text can be found at
 https://creativecommons.org/publicdomain/zero/1.0/.
 We hope that all users will follow the ODC Attribution/Share-Alike Community Norms (http://www.opendatacommons.org/norms/odc-by-sa/);
 in particular, while not legally required, we hope that all users of the data will acknowledge by citing
 Demuru M, van Blooijs D, Zweiphenning W, Hermes D, Leijten F, Zijlmans M, on behalf of the RESPect group. “A Practical Workflow for Organizing Clinical Intraoperative and Long-Term iEEG data in BIDS”. Submitted to Neuroinformatics.
 in any publications.
 

 Code available at: https://github.com/UMCU-EpiLAB.
 

 Acknowledgements
 We would like to thank the patients for providing their data for this dataset, the RESPect team of University Medical Center of Utrecht, for the acquisition of the dataset.
 Please cite Demuru M, van Blooijs D, Zweiphenning W, Hermes D, Leijten F, Zijlmans M, on behalf of the RESPect group. “A Practical Workflow for Organizing Clinical Intraoperative and Long-Term iEEG data in BIDS”. Submitted to Neuroinformatics.
 in any publications.",1,1,0
ds003846,2021-10-18 9:27:32,Lukas Gehrke,1.0.1,Prediction Error,2021-10-20 8:50:55,0,4,11329000000,0.011 TB,366,19,18,34,n/a,CC0,"Lukas Gehrke, Sezen Akman, Albert Chen, Pedro Lopes, Klaus Gramann","We thank Avinash Singh, Tim Chen and C.-T. Lin from the Univsersity of Sydney (New South Wales, Australia) for their help developing the task.","Please cite this openneuro data repository as well as this paper: 
 

 Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials. Lukas Gehrke, Sezen Akman, Pedro Lopes, Albert Chen, Avinash Kumar Singh, Hsiang-Ting Chen, Chin-Teng Lin and Klaus Gramann | In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI ’19). ACM, New York, NY, USA, Paper 427, 11 pages. DOI: https://doi.org/10.1145/3290605.3300657",n/a,"Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials. Lukas Gehrke, Sezen Akman, Pedro Lopes, Albert Chen, Avinash Kumar Singh, Hsiang-Ting Chen, Chin-Teng Lin and Klaus Gramann | In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI ’19). ACM, New York, NY, USA, Paper 427, 11 pages. DOI: https://doi.org/10.1145/3290605.3300657",10.18112/openneuro.ds003846.v1.0.1,GR\_10\_20180603,PredError,,EEG,"### Prediction Error EEG & Motion Study
 This is the dataset of the study ""prediction error"". In short, 19 participants were tested in a virtual reality (VR) reach-to-object task. In the task participants experienced visual, visual with vibrotactile or visual with vibrotactile (all subjects) and electrical muscle stimulation (EMS) feedback (10 subjects) . In 25\% of trials the feedback, 'button selection', was provided prematurely, resulting in prediction error / mismatch ERPs (oddball style paradigm). Participants rated their interactive experience on the Immersion and Presence Questionnaire (IPQ) and their workload on the NASA-TLX.
 

 Details about the study can be found in the following publication(s):
 - Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials. Lukas Gehrke, Sezen Akman, Pedro Lopes, Albert Chen, Avinash Kumar Singh, Hsiang-Ting Chen, Chin-Teng Lin and Klaus Gramann | In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI ’19). ACM, New York, NY, USA, Paper 427, 11 pages. DOI: https://doi.org/10.1145/3290605.3300657
 

 A full repository including data, experimental VR protocol (Unity), and publication resources can be found at [OSF, doi: 10.17605/OSF.IO/X7HNM](https://osf.io/x7hnm/)
 

 ### Available Data
 Data include `64 channel EEG + 1 reference` and `12 channel Motion` (6DOF right hand, 6DOF head). Motion metadata is formatted to current preliminary [BIDS Motion](https://docs.google.com/document/d/1iaaLKgWjK5pcISD1MVxHKexB3PZWfE2aAC5HF\_pCZWo/edit), see [how to get involved](https://bids.neuroimaging.io/get\_involved.html#extending-the-bids-specification).
 

 ### Data Format and Derivation
 Original data were recorded in `.xdf` format using [labstreaminglayer](https://github.com/sccn/labstreaminglayer). A `\sourcedata` directory is currently missing since our `.xdf` files did not comply with GDPR.",1,1,0
ds003848,2021-10-21 15:17:12,Epilab UMCU,1.0.3,Dataset Clinical Epilepsy iEEG to BIDS - RESPect\_longterm\_iEEG,2021-10-22 12:45:11,1,1,69773800000,0.07 TB,184,6,14,46,Brain Imaging Data Structure Specification v1.6.0,CC0,"van Blooijs D., Demuru M., Zweiphenning W, Hermes D., Leijten F., Zijlmans M.",Huiskamp G.J.M.,"Demuru M., van Blooijs D., Zweiphenning W. et al 2021, A practical workflow for organizing clinical intraoperative and long-term iEEG data in BIDS",Epi-Sign Project ===NEMAR-SEP=== Alexandre Suerman Stipendium 2015 ===NEMAR-SEP=== EpilepsieNL #17-07,see HowToAcknowledge,10.18112/openneuro.ds003848.v1.0.3,,"task-SPESclin, task-Sleep, task-sleep, task-slawtrans, task-CHOCS1, task-Rest",,"iEEG, MRI","Dataset description
 This dataset is part of a bigger dataset of intracranial EEG (iEEG) called RESPect (Registry for Epilepsy Surgery Patients), a dataset recorded at the University Medical Center of Utrecht, the Netherlands.
 It consists of 12 patients: six patients recorded intraoperatively using electrocorticography (acute ECoG), six patients with long-term recordings (3 patients recorded with ECoG and 3 patients recorded with stereo-encephalography SEEG). For a detailed description see Demuru M, van Blooijs D, Zweiphenning W, Hermes D, Leijten F, Zijlmans M, on behalf of the RESPect group. “A practical workflow for organizing clinical intraoperative and long-term iEEG data in BIDS“??, submitted to NeuroInformatics in 2020.
 

 This data is organized according to the Brain Imaging Data Structure specification. A community- driven specification for organizing neurophysiology data along with its metadata. For more information on this data specification, see https://bids-specification.readthedocs.io/en/stable/ 
 

 

 Each patient has their own folder (e.g., `sub-RESP0280`) which contains the iEEG recordings data for that patient, as well as the metadata needed to understand the raw data and event timing.
 

 Two different implementation of the BIDS structure were done according to the different type of recordings (i.e. intraoperative or long-term)
 Intraoperative ECoG
 Surgery with intraoperative ECoG is composed of three main situations that can be logically grouped into BIDS sessions: 
 

 * Pre-resection sessions, consisting of all recordings (with different configurations of the grid and strips/depth) carried out before the surgeon has started the planned resection. 
 

 * Intermediate sessions, consisting of all subsequent recordings performed before any iterative extension of the resection area.
 

 * Post-resection sessions, consisting of all the recordings performed after the last resection.
 

 Each situation is labelled with an increasing number starting from 1, indicative of the period in time respective to the surgical resection and a consecutive letter (starting from A) indicative of the position of the grid and strip/depth for a given session.
 As an example see patient RESP0280 who had 4 sessions recorded: two pre-resection sessions, one intermediate sessions and one post-resection session. The first session is SITUATION1A consisting of the first recording, then the grid was moved to another position, resulting in SITUATION1B. After that, the surgeon resected part of the brain and then there was another recording(SITUATION2A). Finally the surgeon applied a resection for the last time and the recording after that was defined as SITUATION3A.  
 

 In long-term recordings, data that are recorded within one monitoring period are logically grouped in the same BIDS session and stored across runs indicating the day and time point of recording in the monitoring period.
 If extra electrodes were added/removed during this period, the session was divided into different sessions (e.g. ses-1A and ses-1b). 
 We use the optional run key-value pair to specify the day and the start time of the recording (e.g. run-021315, day 2 after implantation, which is day 1 of the monitoring period, at 13:15).
 The task key-value pair in long-term iEEG recordings describes the patient´s state during the recording of this file. Different tasks have been defined, such as “rest“?? when a patient is awake but not doing a specific task, “sleep“?? when a patient is sleeping the majority of the file, or “SPESclin“?? when the clinical SPES protocol has been performed in this file. Other task definitions can be found in the annotation syntax (https://github.com/UMCU-EpiLAB/umcuEpi\_longterm\_ieeg\_respect\_bids/master/manuals/IFU\_annotatingtrc\_ECoG).
 

 License
 This dataset is made available under the Public Domain Dedication and License CC v1.0, whose full text can be found at 
 https://creativecommons.org/publicdomain/zero/1.0/. 
 We hope that all users will follow the ODC Attribution/Share-Alike Community Norms (http://www.opendatacommons.org/norms/odc-by-sa/); 
 in particular, while not legally required, we hope that all users of the data will acknowledge by citing 
 Demuru M, van Blooijs D, Zweiphenning W, Hermes D, Leijten F, Zijlmans M, on behalf of the RESPect group. “A practical workflow for organizing clinical intraoperative and long-term iEEG data in BIDS“??, submitted to NeuroInformatics in 2020, in any publications.
 

 Code available at: https://github.com/UMCU-EpiLAB.
 

 Acknowledgements
 We would like to thank the patients for providing their data for this dataset, the RESPect team of University Medical Center of Utrecht, for the acquisition of the dataset.
 Please cite Demuru M, van Blooijs D, Zweiphenning W, Hermes D, Leijten F, Zijlmans M, on behalf of the RESPect group. “A practical workflow for organizing clinical intraoperative and long-term iEEG data in BIDS“??, submitted to NeuroInformatics in 2020, in any publications.",1,1,0
ds003876,2021-11-09 21:38:53,Adam Li,1.0.2,Epilepsy-iEEG-Interictal-Multicenter-Dataset,2022-08-04 18:02:09,0,1,5368600000,0.005 TB,343,39,0,0,1.6.0,CC0,"Gunnarsdottir, Kristin, Li, Adam, Smith, Rachel, Kang, Joon, Korzeniewska, Anna, Crone, Nathan, Rouse, Adam, Cheng, Jennifer, Kinsman, Michael, Landazuri, Patrick, Uysal, Utku, Ulloa, Carol, Cameron, Nathaniel, Cajigas, Iahn, Jagid, Jonathan, Kanner, Andres, Elarjani, Turki, Bicchi, Manuel, Inati, Sara, Zaghloul, Kareem, Boerwinkle, Varina, Wyckoff, Sarah, Barot, Niravkumar, Gonzalez-Martinez, Jorge, Sarma, Sridevi",MNE-BIDS was used for converting datasets to BIDS format.,Please cite the following papers: https://www.biorxiv.org/content/10.1101/2021.10.15.464594v1 and https://www.nature.com/articles/s41593-021-00901-w,NIH T32 EB003383 ===NEMAR-SEP=== NSF GRFP (DGE-1746891) ===NEMAR-SEP=== ARCS Scholarship ===NEMAR-SEP=== Whitaker Fellowship ===NEMAR-SEP=== Chateaubriand Fellowship ===NEMAR-SEP=== NIH R21 NS103113 ===NEMAR-SEP=== US NSF Career Award 1055560 ===NEMAR-SEP=== Burroughs Well CASI Award 1007274,https://www.nature.com/articles/s41593-021-00901-w ===NEMAR-SEP=== https://www.biorxiv.org/content/10.1101/2021.10.15.464594v1,doi:10.18112/openneuro.ds003876.v1.0.2,"IRBs at JHH, NIH, KUMC, UMF, CClinic","interictal, interictalasleep, interictalawake",,iEEG,"Epilepsy Interictal Dataset
 =====================
 

 This dataset was updated and prepared for release as part of a manuscript by Bernabei & Li et al. (in preparation). A subset of the data has been featured in [1] and [2].
 

 Summary
 -------------
 

 This dataset comprises of de-identified subjects with interictal iEEG recordings possibly with sleep or awake state annotated. The subjects come from the following centers:
 

 - National Institute of Health (NIH)
 - Johns Hopkins Hospital (JHH)
 - University of Miami Florida Jackson Memorial Hospital (UMF)
 

 In the actual study, there is also data from Kansas University Medical Center (KUMC), University of Pittsburgh Medical Center and Cleveland Clinic, whose data is not shared due to restrictions imposed by the centers there.
 

 Some subjects, namely with the ``rns`` prefix in their subject ID were treated with RNS rather then surgical resection/ablation.
 

 Derivatives
 ----------------
 

 The processed data corresponding to the ``source-sink`` analysis and ``hfo`` comparisons are shown in the ``derivatives/`` folder. The HFO analysis consists of two folders, one is an RMS detector and the other is a Hilbert detector. See the paper for details.
 

 Ties to Other Datasets
 --------------------------------
 

 NIH ``pt1, pt2, pt3``, JHH ``jh103, jh105`` subjects are also datasets in ``https://openneuro.org/datasets/ds003029``, where the ictal snapshots are stored. These correspond to the following:
 

 - pt1: pt01
 - pt2: pt2
 - pt3: pt3
 - jh103: jh103
 - jh105: jh105
 

 Moreover, the cclinic subjects are used in that study, but not open-access due to data sharing limitations at Cleveland Clinic. Those ictal datasets were analyzed in https://www.nature.com/articles/s41593-021-00901-w. 
 

 References
 ----------------
 

 [1] Li, A., Huynh, C., Fitzgerald, Z. et al. Neural fragility as an EEG marker of the seizure onset zone. Nat Neurosci 24, 1465â??1474 (2021). https://doi.org/10.1038/s41593-021-00901-w
 

 [2] Kristin M. Gunnarsdottir, Adam Li, Rachel J. Smith, Joon-Yi Kang, Nathan E. Crone, Anna Korzeniewska, Adam Rouse, Nathaniel Cameron, Iahn Cajigas, Sara Inati, Kareem A. Zaghloul, Varina L. Boerwinkle, Sarah Wyckoff, Nirav Barot, Jorge Gonzalez-Martinez, Sridevi V. Sarma. Source-sink connectivity: a novel resting-state EEG marker of the epileptogenic zone. bioRxiv 2021.10.15.464594; doi: https://doi.org/10.1101/2021.10.15.464594
 

 [3] Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., HÃ¶chenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 [4] Holdgraf, C., Appelhoff, S., Bickel, S., Bouchard, K., D'Ambrosio, S., David, O., â?¦ Hermes, D. (2019). iEEG-BIDS, extending the Brain Imaging Data Structure specification to human intracranial electrophysiology. Scientific Data, 6, 102. https://doi.org/10.1038/s41597-019-0105-7",1,1,0
ds003885,2021-11-11 6:53:05,Sophia Shatek,1.0.8,Capacity for movement is a major organisational principle in object representations: EEG data from Experiment 2,2021-11-12 5:57:06,0,1,49551700000,0.05 TB,526,24,18,26,1.0.2,CC0,"Shatek, Sophia M., Robinson, Amanda K., Grootswagers, Tijl, Carlson, Thomas A.",,,ARC DP160101300 (TAC) ===NEMAR-SEP=== ARC DP200101787 (TAC),,doi:10.18112/openneuro.ds003885.v1.0.8,,movement,,EEG,"Overview
 ——————————————————
 This data is from the paper ""Capacity for movement is a major organisational principle in object representations"". This is the data of Experiment 1 (EEG: aliveness). The paper is now published in NeuroImage: https://doi.org/10.1016/j.neuroimage.2022.119517
 

 Abstract:
 The ability to perceive moving objects is crucial for threat identification and survival. Recent neuroimaging evidence has shown that goal-directed movement is an important element of object processing in the brain. However, prior work has primarily used moving stimuli that are also animate, making it difficult to disentangle the effect of movement from aliveness or animacy in representational categorisation. In the current study, we investigated the relationship between how the brain processes movement and aliveness by including stimuli that are alive but still (e.g., plants), and stimuli that are not alive but move (e.g., waves). We examined electroencephalographic (EEG) data recorded while participants viewed static images of moving or non-moving objects that were either natural or artificial. Participants classified the images according to aliveness, or according to capacity for movement. Movement explained significant variance in the neural data over and above that of aliveness, showing that capacity for movement is an important dimension in the representation of visual objects in humans.  
 

 

 In this experiment, participants completed two tasks - classification and passive viewing. In the classification task, participants classified single images that appeared on the screen as ""alive"" or ""not alive"". This task was time-pressured, and trials timed out after 1 second. In the passive viewing task, participants viewed rapid (RSVP) streams of images, and pressed a button to indicate when the fixation cross changed colour.
 

 Contents of the dataset: 
  - Raw EEG data is available in individual subject folders (BrainVision raw formats .eeg, .vmrk, .vhdr). Pre-processed EEG data is available in the derivatives folders in EEGlab (.set, .fdt) and cosmoMVPA dataset (.mat) format. This experiment has 24 subjects.
  - Scripts for data analysis and running the experiment are available in the code folder. Note that all code runs on both EEG experiments together, so you must download both this and the movement experiment data in order to replicate analyses.
  - Stimuli are also available (400 CC0 images)
  - Results of decoding analyses are available in the derivatives folder.
 

 Further notes: 
 

 Note that the code is designed to run analyses for data and its partner data (experiments 2 and 3 of the paper). Copies in both folders are identical. Scripts need to be run in a particular order (detailed at the top of each script)
 #### Further explanations of the code:
 

 1. Run pre-processing of EEG (analyse\_EEG\_preprocessing.m), and behavioural data (analyse\_behavioural\_EEG.m)
 2. Ensure that the MTurk data has been run (analyse\_behavioural\_MTurk.m), from the Experiment 1 folder.
 3. Run RSA (analyse\_rsa.m; reliant on behavioural data and pre-processed EEG data), and run decoding (analyse\_decoding.m; reliant on pre-processed EEG data)
 4. Run GLMs (analyse\_glms.m; reliant on RSA, behavioural)
 

 To only look at the results, the results for each of these analyses is saved in the derivatives already, so there is no need to run any of them again.
 

 Each file named plot\_X.m will create a graph as in the paper. Each is reliant on saved data from the above analyses, which are saved in the derivatives folder.
 

 

 Citing this dataset
 ———————————————————
 If using this data, please cite the associated paper: 
 Shatek, S. M., Robinson, A. K., Grootswagers, T., & Carlson, T. A. (2022). Capacity for movement is an organisational principle in object representations. NeuroImage, 261, 119517. https://doi.org/10.1016/j.neuroimage.2022.119517
 

 

 Contact 
 ———————————————————
 Contact Sophia Shatek (sophia.shatek@sydney.edu.au) for additional information. ORCID: 0000-0002-7787-1379",1,1,0
ds003887,2021-11-11 8:00:33,Sophia Shatek,1.2.3,Capacity for movement is a major organisational principle in object representations: EEG data from Experiment 3,2021-11-12 6:37:59,0,1,49100700000,0.049 TB,526,24,18,26,1.0.2,CC0,"Shatek, Sophia M., Robinson, Amanda K., Grootswagers, Tijl, Carlson, Thomas A.",,,ARC DP160101300 (TAC) ===NEMAR-SEP=== ARC DP200101787 (TAC),,doi:10.18112/openneuro.ds003887.v1.2.3,,movement,,EEG,"Overview
 ——————————————————
 This data is from the paper ""Capacity for movement is a major organisational principle in object representations"". This is the data of Experiment 2 (EEG: movement). The paper is now published in NeuroImage: https://doi.org/10.1016/j.neuroimage.2022.119517
 

 Abstract:
 The ability to perceive moving objects is crucial for threat identification and survival. Recent neuroimaging evidence has shown that goal-directed movement is an important element of object processing in the brain. However, prior work has primarily used moving stimuli that are also animate, making it difficult to disentangle the effect of movement from aliveness or animacy in representational categorisation. In the current study, we investigated the relationship between how the brain processes movement and aliveness by including stimuli that are alive but still (e.g., plants), and stimuli that are not alive but move (e.g., waves). We examined electroencephalographic (EEG) data recorded while participants viewed static images of moving or non-moving objects that were either natural or artificial. Participants classified the images according to aliveness, or according to capacity for movement. Movement explained significant variance in the neural data over and above that of aliveness, showing that capacity for movement is an important dimension in the representation of visual objects in humans.  
 

 

 In this experiment, participants completed two tasks - classification and passive viewing. In the classification task, participants classified single images that appeared on the screen as ""can move"" or ""still"". This task was time-pressured, and trials timed out after 1 second. In the passive viewing task, participants viewed rapid (RSVP) streams of images, and pressed a button to indicate when the fixation cross changed colour.
 

 Contents of the dataset: 
  - Raw EEG data is available in individual subject folders (BrainVision raw formats .eeg, .vmrk, .vhdr). Pre-processed EEG data is available in the derivatives folders in EEGlab (.set, .fdt) and cosmoMVPA dataset (.mat) format. This experiment has 24 subjects.
  - Scripts for data analysis and running the experiment are available in the code folder. Note that all code runs on both EEG experiments together, so you must download both this and the movement experiment data in order to replicate analyses.
  - Stimuli are also available (400 CC0 images)
  - Results of decoding analyses are available in the derivatives folder.
 

 Further notes: 
 

 Note that the code is designed to run analyses for data and its partner data (experiments 2 and 3 of the paper). Copies in both folders are identical. Scripts need to be run in a particular order (detailed at the top of each script)
 

 Further explanations of the code:
 

 1. Run pre-processing of EEG (analyse\_EEG\_preprocessing.m), and behavioural data (analyse\_behavioural\_EEG.m)
 2. Ensure that the MTurk data has been run (analyse\_behavioural\_MTurk.m), from the Experiment 1 folder.
 3. Run RSA (analyse\_rsa.m; reliant on behavioural data and pre-processed EEG data), and run decoding (analyse\_decoding.m; reliant on pre-processed EEG data)
 4. Run GLMs (analyse\_glms.m; reliant on RSA, behavioural)
 

 To only look at the results, the results for each of these analyses is saved in the derivatives already, so there is no need to run any of them again.
 

 Each file named plot\_X.m will create a graph as in the paper. Each is reliant on saved data from the above analyses, which are saved in the derivatives folder.
 

 

 Citing this dataset
 ———————————————————
 If using this data, please cite the associated paper:
 

 

 Contact 
 ———————————————————
 

 Contact Sophia Shatek (sophia.shatek@sydney.edu.au) for additional information. ORCID: 0000-0002-7787-1379",1,1,0
ds003922,2021-11-19 13:29:49,Jacques Pesnot Lerousseau,1.0.1,Multisensory Correlation Detector,2022-05-02 14:15:02,1,1,81285100000,0.081 TB,674,13,0,0,1.6.0,CC0,"Pesnot Lerousseau, J., Parise, C., Ernst, MO., van Wassenhove, V.",We thank the members of UNIACT for their logistical help on the recruitment of volunteers at NeuroSpin. We thank Leila Azizi for her help with the LED apparatus.,,ERC-YStG-263584 ===NEMAR-SEP=== ANR-16-CE37-0004-04,"Pesnot Lerousseau, J., Parise, C., Ernst, MO., van Wassenhove, V. (2022). Multisensory correlation computations in the human brain identified by a time-resolved encoding model. *Nature Communications*. http://doi.org/10.1038/s41467-022-29687-6",doi:10.18112/openneuro.ds003922.v1.0.1,"Local Ethics Committee on Human Research at NeuroSpin (Gif-sur-Yvette, France)","mcd, rest, noise",,"MEG, MRI","### DESCRIPTION
 Magnetoencephalography (MEG) dataset recorded during the presentation of audiovisual sequences with a causality judgment task and temporal order judgment task. This MEG dataset was prepared in the Brain Imaging Data Structure (MEG-BIDS, Niso et al. 2018) format using MNE-BIDS (Appelhoff et al. 2019).
 

 ### PUBLISHED IN
 Pesnot Lerousseau, J., Parise, C., Ernst, MO., van Wassenhove, V. (2022). Multisensory correlation computations in the human brain identified by a time-resolved encoding model. *Nature Communications*. http://doi.org/10.1038/s41467-022-29687-6
 

 ### PARTICIPANTS
 The dataset contains 13 participants (Ab140232, Jl150443, Mm150194, Al150424, Mp110340, Rt160359, Cb140229, Cc160310, Lb160367, Mb160304, Mk150295, Sl160372, Mp150285). 
 

 ### EXPERIMENT
 The experiment consisted of 10 consecutive recording blocks of 8 minutes each, whose order was counterbalanced across participants. Three blocks tested participants on a Causality judgement, and three blocks tested participants with a Temporal order judgement. Importantly, the same audiovisual sequences were used in both tasks in order to maintain a constant flow of feedforward multisensory inputs while manipulating the endogenous task requirements. Each block was composed of 25 repetitions of the 6 possible audiovisual sequences. A total of 75 presentations of each stimulus sequence were thus tested in each task. Four additional recording blocks consisted of participants passively hearing (auditory localizer, 2 blocks) or viewing (visual localizer, 2 blocks) one constitutive modality of the audiovisual sequence. Each localizer block was composed of 25 repetitions of the 6 possible stimuli (auditory or visual part of each stimuli), yielding a total of 50 presentations of each auditory and visual stimuli (2 tasks x 3 blocks x 25 repetitions x 6 sequences + 2 modalities x 25 repetitions x 2 blocks x 6 sequences = 1500 trials in total). 
 

 ### STIMULI
 Six audiovisual sequences were presented (DD, DC, CC, AA, AV, VV). 
 

 ### BLOCKS
 Ten blocks were presented (3 Causality, 3 Temporal, 2 Auditory, 2 Visual). 
 

 ### EVENTS
 - 'Causality/DD':11
 - 'Causality/DC':12
 - 'Causality/CC':13
 - 'Causality/AA':14
 - 'Causality/AV':15
 - 'Causality/VV':16
 - 'Temporal/DD':21
 - 'Temporal/DC':22
 - 'Temporal/CC':23
 - 'Temporal/AA':24
 - 'Temporal/AV':25
 - 'Temporal/VV':26
 - 'Auditory/DD':41
 - 'Auditory/DC':42
 - 'Auditory/CC':43
 - 'Auditory/AA':44
 - 'Auditory/AV':45
 - 'Auditory/VV':46
 - 'Visual/DD':51
 - 'Visual/DC':52
 - 'Visual/CC':53
 - 'Visual/AA':54
 - 'Visual/AV':55
 - 'Visual/VV':56
 

 ### MEG
 Brain magnetic fields were recorded in a MSR using a 306 MEG system (Neuromag Elekta LTD, Helsinki). MEG recordings were sampled at 1 kHz and band-pass filtered between 0.03 Hz and 330 Hz.
 

 Four head position coils (HPI) measured the head position of participants before each block; three fiducial markers (nasion and pre-auricular points) were used for digitization and anatomicalMRI (aMRI) immediately following MEG acquisition.
 

 Electrooculograms (EOG, horizontal and vertical eye movements) and electrocardiogram (ECG) were simultaneously recorded. Prior to the session, 2 min of empty room recordings was acquired for the computation of the noise covariance matrix.
 

 Bad MEG channels were marked manually.
 

 ### MRI
 The T1 weighted aMRI was recorded using a 3-T Siemens Trio MRI scanner. Parameters of the sequence were: voxel size: 1.0 × 1.0 × 1.1 mm; acquisition time: 466 s; repetition time TR = 2300 ms; and echo time TE = 2.98 ms
 

 ### BEHAVIOR
 

 File sourcedata/behavioral\_data.txt
 

 ### REFERENCES
 

 Pesnot Lerousseau, J., Parise, C., Ernst, MO., van Wassenhove, V. (2022). Multisensory correlation computations in the human brain identified by a time-resolved encoding model. Nature Communications. http://doi.org/10.1038/s41467-022-29687-6
 

 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., Moreau, J., Oostenveld, R., Schoffelen, J., Tadel, F., Wexler, J., Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5, 180110. http://doi.org/10.1038/sdata.2018.110",1,1,0
ds003944,2021-12-01 17:11:13,Arnaud Delorme,1.0.1,EEG: First Episode Psychosis vs. Control Resting Task 1,2022-01-24 19:05:25,1,1,6606400000,0.007 TB,351,82,12,35,v1.6.0,CC0,"Dean Salisbury, Dylan Seebold, Brian Coffman",,,,"Phalen, H., Coffman, B.A., Ghuman, A., Sejdic, E. and Salisbury, D.F., 2020. Non-negative matrix factorization reveals resting-state cortical alpha network abnormalities in the first-episode schizophrenia spectrum. Biological Psychiatry: Cognitive Neuroscience and Neuroimaging, 5(10), pp.961-970., https://pubmed.ncbi.nlm.nih.gov/31451387/",doi:10.18112/openneuro.ds003944.v1.0.1,,Rest,,EEG,"Resting EEG and MEG data was gathered for two independently collected samples of healthy and First Episode Psychosis (FEP) individuals.
 To obtain resting data, EEG channels were recorded for 5 minutes using an Elekta Neuromag Vectorview system.
 EEG was recorded using a low-impedance 10-10 system 60-channel cap. 
 The first collected sample of EEG data is provided here. This sample includes a portion of subjects from the second acquisition (EEG: First Episode Psychosis vs. Control Resting Task 2), since they were collected using the same montage.
 The subjects from Task 2 that have been included here are: sub-2140A, sub-2170A, sub-2174A, sub-2176A, sub-2177A, sub-2184A, sub-2193A, sub-2214A, sub-2217A, sub-2221A.
 

 The phenotype directory contains clinical assessment results and data divided by type for all subjects. The assessment results were categorized as follows:
 BPRS - Brief Psychiatric Rating Scale,
 SANS - Scale for Assessment of Negative Symptoms,
 SAPS - Scale for Assessment of Positive Symptoms,
 GAFGAS - Global Assessment of Functioning,
 SFS - Social Functioning Scale,
 MATRICS - MATRICS Consensus Cognitive Battery,
 WASI - Wechsler Abbreviated Scale of Intelligence, 
 Hollingshead - Hollingshead Four-Factor Index of Socioeconomic Status,
 Medications - Chlorpromazine equivalency of prescribed medication at time of EEG scan.
 Values/scores that were not collected and questions without given responses are denoted by n/a.",1,1,0
ds003947,2021-12-01 17:35:50,Arnaud Delorme,1.0.1,EEG: First Episode Psychosis vs. Control Resting Task 2,2022-01-24 19:04:37,1,1,13466600000,0.013 TB,267,61,14,38,v1.6.0,CC0,"Dean Salisbury, Dylan Seebold, Brian Coffman",,,,"Phalen, H., Coffman, B.A., Ghuman, A., Sejdic, E. and Salisbury, D.F., 2020. Non-negative matrix factorization reveals resting-state cortical alpha network abnormalities in the first-episode schizophrenia spectrum. Biological Psychiatry: Cognitive Neuroscience and Neuroimaging, 5(10), pp.961-970., https://pubmed.ncbi.nlm.nih.gov/31451387/",doi:10.18112/openneuro.ds003947.v1.0.1,,rest,,EEG,"Resting EEG and MEG data was gathered for two independently collected samples of healthy and First Episode Psychosis (FEP) individuals.
 To obtain resting data, EEG channels were recorded for 5 minutes using an Elekta Neuromag Vectorview system.
 EEG was recorded using a low-impedance 10-10 system 60-channel cap. 
 The second collected sample of EEG data is provided here. This sample excludes a portion of subjects that have been included with the first acquisition (EEG: First Episode Psychosis vs. Control Resting Task 1), since they were collected using the same montage.
 The subjects that have been excluded here and are included in Task 1 are: sub-2140A, sub-2170A, sub-2174A, sub-2176A, sub-2177A, sub-2184A, sub-2193A, sub-2214A, sub-2217A, sub-2221A.
 

 The phenotype directory contains clinical assessment results and data divided by type for all subjects. The assessment results were categorized as follows:
 BPRS - Brief Psychiatric Rating Scale,
 SANS - Scale for Assessment of Negative Symptoms,
 SAPS - Scale for Assessment of Positive Symptoms,
 GAFGAS - Global Assessment of Functioning,
 SFS - Social Functioning Scale,
 MATRICS - MATRICS Consensus Cognitive Battery,
 WASI - Wechsler Abbreviated Scale of Intelligence, 
 Hollingshead - Hollingshead Four-Factor Index of Socioeconomic Status,
 Medications - Chlorpromazine equivalency of prescribed medication at time of EEG scan.
 Values/scores that were not collected and questions without given responses are denoted by n/a.",1,1,0
ds003969,2021-12-23 5:36:27,Arnaud Delorme,1.0.0,Meditation vs thinking task,2022-01-03 18:38:28,0,1,58478000000,0.058 TB,1181,98,22,74,v1.2.1,CC0,"Arnaud Delorme, Claire Braboszcz",,,,https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0170647,doi:10.18112/openneuro.ds003969.v1.0.0,,mixed,,EEG,"Data collection took place at the Meditation Research Institute (MRI) in Rishikesh, India, under the supervision of Arnaud Delorme, Ph.D. The project was approved by the local MRI Indian ethical committee and the ethical committee of the University of California San Diego (IRB project # 090731).
 

 Participants sat either on a blanket on the floor or on a chair for both experimental periods depending on their personal preference. Participants were asked to keep their eyes closed, and all lighting in the room was turned off during data collection. An intercom allowed communication between the experimental and the recording room.
 

 Participants performed four blocks, 2 meditation blocks interspaced by two thining blocks (in which they are instructed to think actively). Half of the participants start with a meditation block, and half of them start with a thinking block. The first meditation block is a breath counting meditation for all participants. The second block is a tradition-specific meditation - except for the control group, for which it is a breath counting meditation.",1,1,0
ds003987,2022-01-10 19:15:07,James F Cavanagh,1.0.0,EEG: Amphetamine trials 5CCPT and Probabilistic Learning,2022-02-01 20:21:02,1,3,27451700000,0.027 TB,557,23,18,35,1.1.1,CC0,"James F Cavanagh, Greg Light, Neal Swerdlow, Jonathan Brigman, Jared Young",,,,pending,doi:10.18112/openneuro.ds003987.v1.0.0,,5CCPTxPSTxAmphetamine,,EEG,"Two different tasks.  From: ""Electrophysiological biomarkers of behavioral dimensions from cross-species paradigms""  Second phase (UH3 phase): Amphetamine trials. Data for both Bhakta et al. 5CCPT paper (humans only) and Cavanagh et al. PLT paper (humans and mice). N=23 humans. 3 drug conditions: placebo, 10mg, 20mg. N=28 mice in code folder. 4 drug condis: placebo, 0.1, 0.3, 1.0 mg/kg. EEG Triggers were odd binary recombinations that were re-translated into 0-255 in Matlab.  See .m scripts and Trigger Translator.xls *********** OK! LISTEN!  The .bdf files were to big to import using this function. So I imported them in EEGLab, downsampled to 500 Hz, then saved them as .set files. THEN I ran the import script on these .set files. So you do not need to re-downsample in STEP1 if you run anything from the code folder.  *********** Data collected circa 2016-2019 in San Diego. Data analyzed circa 2017-2021 in New Mexico.  - James F Cavanagh 06/16/2021",1,1,0
ds004000,2022-01-19 12:17:04,Anna Padée,1.0.0,Fribourg Ultimatum Game in Schizophrenia Study,2022-01-20 13:46:46,1,1,24161100000,0.024 TB,392,43,0,0,v1.6.0,CC0,"Anna Padée, Pascal Missonnier, Anne Prévot, Grégoire Favre, Isabelle Gothuey, Marco Merlo, Jonas Richiardi",,,,,doi:10.18112/openneuro.ds004000.v1.0.0,,responder,,EEG,"This is a schizophrenia in ultimatum game task study for Fribourg University. Participants were asked to play the UG in both roles, both as responder and proposer. 128 electrode EEG was recorded during the task. 19 patients with psychosis epoisodes and 24 healths controls were recorded during the task.
 

 This dataset was recorded at the Fribourg University in Switzerland. The project was approved by the Ethics Committee of the University of Fribourg (reference number: 054/13-CER-FR).
 

 Participants sat in a shielded room, in a comfortable chair and played the game, while EEG was recorded. 
 

 For each role, participants performed three blocks, consisting of 30 repetitions each.",1,1,0
ds004010,2022-01-24 19:47:18,Arnaud Delorme,1.0.0,MAVIS,2022-01-26 19:34:34,0,1,24844900000,0.025 TB,102,24,0,0,1.2.0,CC0,"Leonhard Waschke, Thomas Donoghue, Lorenz Fiedler, Sydney Smith, Douglas Garrett, Bradley Voytek, Jonas Obleser",,,,Waschke L ===NEMAR-SEP=== Donoghue T ===NEMAR-SEP=== Fiedler L ===NEMAR-SEP=== Smith S ===NEMAR-SEP=== Garrett DD ===NEMAR-SEP=== Voytek B ===NEMAR-SEP=== Obleser J. Modality-specific tracking of attention and sensory statistics in the human electrophysiological spectral exponent. Elife. 2021 Oct 21;10:e70068. doi: 10.7554/eLife.70068. PMID: 34672259; PMCID: PMC8585481. https://pubmed.ncbi.nlm.nih.gov/34672259/,doi:10.18112/openneuro.ds004010.v1.0.0,,MAVIS,,EEG,EEG data from 24 healthy participants performing a multisensory detection task was collected to investigate the dynamics of EEG activity during varying selective attention and the processing of sensory stimuli with distinct features. Participants detected targets in simultaneous audio-visual noise.,1,1,0
ds003082,2020-08-17 10:13:10,Jonathan Cote,1.0.1,Auditory Cortex Mapping Dataset,2020-11-30 21:41:33,1,1,14215300000,13.2 GB,62,1,23,0,1.2.0,CC0,"Jonathan Cote, Etienne de Villers-Sidani",,"please acknowledge its authors (Jonathan Cote, Jean-Pierre R. Falet, Veronica Tarka , Zaida-Escila Martinez-Moreno and Etienne de Villers-Sidani) and cite the mapping technique publication (under review).","This work was funded in part by grants from the Natural Sciences and Engineering Research Council of Canada (NSERC), the Centre for Research on Brain, Language and Music (CRBLM), and the Reseau quebecois de recherche sur le vieillissement (RQRV).",,doi:10.18112/openneuro.ds003082.v1.0.1,,"mapping, noise",,"MEG, MRI","# Brainstorm - Auditory Cortex Mapping Dataset
 

 ## License
 

 This dataset (MEG and MRI data) was collected by Jonathan Cote of the Neuroplasticity and Sensory Biomarking Lab, Montreal Neurological Institute, McGill University, Canada. Its purpose is to serve as a data example to be used with our MEG-based auditory cortex mapping technique. It is presently released in the Public Domain, and is not subject to copyright in any jurisdiction.
 

 We would appreciate though that you reference this dataset in your publications: please acknowledge its authors (Jonathan Cote and Etienne de Villers-Sidani) and cite the mapping technique publication (under review)
  
 This dataset will first be a single subject, but might be expanded up to the 10 participants in the future.
 

 ## Presentation of the experiment
 

 #### Experiment
 

 * One subject, one acquisition run of around 12 minutes
 * Subject stimulated binaurally with intra-aural earphones (air tubes+transducers)
 * The run contains:
  * 1795 iso-intensity pure tones (IIPT)
  * The frequency of these ranges between 100 Hz and 21527 Hz, spaced by 1/4 octave.
 * Random inter-stimulus interval: randomized but averaging at a presentation rate of 3Hz
 * The subject passively listened while looking at a fixation cross
 * Auditory stimuli generated with the Matlab Psychophysics toolbox
 

 

 #### MEG acquisition
 

 * Acquisition at **120000Hz**, with a **CTF 275** system, subject in seating position
 * Recorded at the Montreal Neurological Institute in January 2015
 * Anti-aliasing low-pass filter at 3000Hz, files saved with the 3rd order gradient
 * Recorded channels (340):
  * 1 Trigger channel indicating the presentation times of the audio stimuli: UADC001 (#306)
  * 26 MEG reference sensors (#4-#29)
  * 272 MEG axial gradiometers (#30-#302)
  * 1 ECG bipolar (#303)
  * 2 EOG bipolar (vertical #304, horizontal #305)
  * 3 Unused channels (#1-#3)
 * 3 datasets:
  * **sub-0001_ses-0001_task-mapping_run-01_meg.ds**: Run #1, 653s, 1795 IIPT, sampled at 12000 Hz
 

  * **sub-emptyroom_ses-0001_emptyroom_run-01_meg.ds**: Empty room recording, 120s long, sampled at 12000 Hz
 

  * **sub-emptyroom_ses-0001_emptyroom_run-02_meg.ds**: Empty room recording, 120s long, sampled at 2400 Hz
 

 * Use of the .ds, not the AUX (standard at the MNI) because they are easier to manipulate in FieldTrip
 

 #### Stimulation delays
 

 

 * **Delay #1**: Transmission of the sound. 
 Between when the sound card plays the sound and when the subject receives the sound in the ears. This is the time it takes for the transducer to convert the analog audio signal into a sound, plus the time it takes to the sound to travel through the air tubes from the transducer to the subject's ears. This delay cannot be estimated from the recorded signals: before the acquisition, we placed a sound meter at the extremity of the tubes to record when the sound is delivered. Delay **between 4.8ms and 5.0ms** (std = 0.08ms). At a sampling rate of 2400Hz, this delay can be considered **constant**, we will not compensate for it.
 * **Delay #2**: Recording of the signals. 
 The CTF MEG systems have a constant delay of **4 samples** between the MEG/EEG channels and the analog channels (such as the audio signal UADC001), because of an anti-aliasing filtered that is applied to the first and not the second. This translate here to a **constant delay** of **1.7ms**.
 * **Uncorrected delays**: We will keep the delays. We decide not to compensate for these delays because they do not introduce any jitter in the responses and they are not going to change anything in the interpretation of the data.
 

 #### Head shape and fiducial points
 

 * 3D digitization using a Polhemus Fastrak device driven by Brainstorm (S01_20131218_*.pos)
 * More information: [Digitize EEG electrodes and head shape][1]
 * The output file is copied to each .ds folder and contains the following entries:
  * The position of the center of CTF coils
  * The position of the anatomical references we use in Brainstorm: Nasion and connections tragus/helix, as illustrated [here][2].
 

 * Around 150 head points distributed on the hard parts of the head (no soft tissues)
 

 #### Subject anatomy
 * Subject with 1.5T MRI
 * Processed with FreeSurfer 5.3
 

 

 [1]: http://neuroimage.usc.edu/brainstorm/Tutorials/TutDigitize
 [2]: http://neuroimage.usc.edu/brainstorm/CoordinateSystems#Pre-auricular_points_.28LPA.2C_RPA.29",1,0,0
ds004015,2022-02-03 14:40:10,Björn Holtze,1.0.2,Attended speaker paradigm (cEEGrid data),2022-03-30 9:29:01,1,1,6475870000,0.006 TB,305,36,18,33,v5.2,CC0,"Bjoern Holtze, Marc Rosenkranz, Manuela Jaeger, Stefan Debener, Bojana Mirkovic",,,,https://doi.org/10.3389/fnins.2022.869426,doi:10.18112/openneuro.ds004015.v1.0.2,,AttendedSpeakerParadigmcEEGridAttention,,EEG,"Within this study cEEGrid data from two previous studies were pooled. 
 15 participants from Jaeger et al. (2020) and 21 from Holtze et al. (2021) were included. 
 Participants performed a two-competing speaker paradigm in both original studies. 
 Participants were instructed to either attend to the left or right audio book. 
 The paradigm consisted of six (Jaeger et al. 2020) or five (Holtze et al. 2021) 
 10-minute blocks of audio book presentation. In Jaeger et al. (2020) both audio 
 books were always presented equally loud. In Holtze et al. 2021, a 10-minute block could 
 either be presented in the omnidirectional condition (both audio books were presented 
 equally loud) or in the beamforming condition (the to-be-attended audio book 
 was louder than the to-be-ignored audio book). The first 10-minute block was always 
 presented in the omnidirectional condition whereas the conditions were alternated 
 for the later four blocks, with one half of the participants starting with the 
 omnidirectonal condition and the other half starting with the beamforming condition. 
 The article (https://doi.org/10.3389/fnins.2022.869426) contains all methodological details
 

 - Björn Holtze (February, 2022)",1,1,0
ds004017,2022-02-08 16:49:30,Marta Topor,1.0.3,B/D Motor Intervention for Learning,2023-03-17 15:10:54,0,3,22471500000,0.022 TB,382,21,0,0,1.2.0,CC0,"Linn Damsgaard, Marta Topor, Anne-Mette Veber Nielsen, Anne Kær Gejl, Anne Sofie Bøgh Malling, Mark Schram Christensen, Rasmus Ahmt Hansen, Søren Kildahl, Jacob Wienecke",,,n/a,https://github.com/MovementAndNeuroscience/B-D\_EEG\_Child,doi:10.18112/openneuro.ds004017.v1.0.3,,,,EEG,"There are three files per participant collected for each of the three stages of the procedure.
 

 Stage 1 (before measurement): A two-alternative forced choice discrimination task including letters ""b"" and ""d""
 

 Stage 2 (intervention measurement): A simple visual search task including a target letter (either b or d) and three distractor letters chosen at random (p or q).
 

 Stage 3 (after measurement): A two-alternative forced choice discrimination task including letters ""b"" and ""d""
 \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
 Participants were assigned to two groups.
 

 Participants in the intervention group were: sub-04, sub-05, sub-06, sub-07, sub-09, sub-10, sub-12, sub-14, sub-15, sub-16, sub-21
 

 Participants in the control group were: sub-01, sub-02, sub-03, sub-08, sub-11, sub-13, sub-17, sub-18, sub-19, sub-20
 

 \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
 Events in all recordings correspond to stimulus presentation. The value of 100 represents letter b stimuli and 200 represents letter d stimuli.
 Events marked with 10 (b) and 20 (d) represent practice trials.
 

 \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
 The detailed description of the tasks and the procedure can be found in this preprint:
 

 For questions about the tasks and the data please email Jacob Wienecke at wienecke@nexs.ku.dk.
 

 

 Marta Topor 17/03/2022",1,1,0
ds004018,2022-02-09 5:24:34,Tijl Grootswagers,2.0.0,EEG recordings for 200 object images presented in RSVP sequences at 5Hz or 20Hz,2022-02-09 7:00:37,1,1,11334200000,0.011 TB,350,16,18,38,1.0.0 + bep006,CC0,"Grootswagers, T., Robinson, A.K., Carlson, T.A.",,"Grootswagers T., Robinson A.K., Carlson T.A. (2019). The representational dynamics of visual objects in rapid serial visual processing streams. NeuroImage, 188, 668-679 https://doi.org/10.1016/j.neuroimage.2018.12.046",,https://doi.org/10.1016/j.neuroimage.2018.12.046 ===NEMAR-SEP=== https://osf.io/a7knv/,doi:10.18112/openneuro.ds004018.v2.0.0,,rsvp,,EEG,"Grootswagers T.*, Robinson A.K.*, Carlson T.A. (2019). The representational dynamics of visual objects in rapid serial visual processing streams. NeuroImage, 188, 668-679 https://doi.org/10.1016/j.neuroimage.2018.12.046
 

 See also https://osf.io/a7knv/",1,1,0
ds004019,2022-02-09 20:45:42,Graciela Catalina Alatorre Cruz,1.0.0,Effect of obesity on arithmetic processing in preteens with high and low math skills. An event-related potentials study,2022-02-09 22:18:37,0,1,18550700000,0.019 TB,441,62,0,0,1,CC0,"Graciela C. Alatorre-Cruz, Heather Downs, Darcy Hagood, Seth T. Sorensen, D. Keith Williams, Linda Larson-Prior",,,This research was supported by USDA/Agricultural Research Service Project 6026-51000-012-06S,https://doi.org/10.3389/fnhum.2022.760234,doi:10.18112/openneuro.ds004019.v1.0.0,,Math task,,EEG,"Introduction
 

 This EEG dataset contains the electrophysiological signal from sixty-two obese and non-obese preteens 
 during a delayed-verification math task. The stimuli were designed and administered using E-Prime 
 software (Version 2) at Arkansas Children Nutrition Center (ACNC), Little Rock, Arkansas.
 The University of Arkansas for Medical Sciences (UAMS) approved the study protocol. This research
 was supported by USDA/Agricultural Research Service Project 6026-51000-012-06S.
 

  Raw data files
 

 The data was acquired with a Geodesic Net Amps 300 system running Netstation 4.5.2 software using 
 the 128-channel Geodesic Hydrocell Sensor Net™ (Magstim EGI., Eugene OR, USA).
 No operations have been performed on the data.
 

 Participant data
 

 The *Participants.tsv* file contains age, gender, body mass index (BMI), and performance. 
 

 

 How to cite
 

 All use of this dataset in a publication context requires the following paper
 to be cited:
 Alatorre-Cruz, G.C., Downs, H., Hagood, D., Sorensen, S.T., Williams, D.K., Larson-Prior, L. (2022).
 Effect of obesity on arithmetic processing in preteens with high and low math skills. An event-related
 potentials study. Frontiers in Human Neurosciences, In press.
 

 

 Contact
 Questions regarding the EEG data may be addressed to
 Catalina Alatorre-Cruz (gcalatorrecruz@uams.edu).
 

 Question regarding the project, in general, may be addressed to
 Linda Larson-Prior (ljlarsonprior@uams.edu).",1,1,0
ds004022,2022-02-11 5:01:21,????[ ??? / ???? ],1.0.0,Multimodal EEG and fNIRS Biosignal Acquisition during Motor Imagery Tasks in Patients with Orthopedic Impairment,2022-02-15 4:52:06,1,1,646523000,0.001 TB,89,7,48,83,1.0.2,CC0,"Seho Lee, Hee Ra Jung, In-Nea Wang, Min-Kyung Jung, Hakseung Kim, Dong-Joo Kim",,,#ERROR!,,doi:10.18112/openneuro.ds004022.v1.0.0,,Motor Imagery,,EEG,"This dataset consists of raw 18-channel EEG and functional near infrareds(fNIRS) from 7 human paticipants with orthopedic Impairment during motor imagery(MI). The participants performed a series of MI-related trials across three sessions. These sessions comprised 40 trials, of which four different MI tasks were presented in random order (e.g., Reach -> Twist -> Lift -> Reach -> Grasp -> Grasp -> Twist -> Reach -> Lift -> Reach). Each trial began with 3 s of fixation cross. The monitor then displayed a 4 s visual cue, followed by 3 s of letters indicating the ready state with a gray screen to eliminate the afterimage. The participants were then instructed to perform the imaginary movement for 5 s in the given order.",1,1,0
ds004024,2022-02-11 8:04:22,Tommi Raij,1.0.1,"TMS-EEG-MRI-fMRI-DWI data on paired associative stimulation and connectivity (Shirley Ryan AbilityLab, Chicago, IL)",2022-02-15 2:13:14,1,4,1096520000000,1.1 TB,3141,13,0,0,1.6.0,CC0,"Julio Cesar Hernandez Pavon, Nils Schneider Garces, John Patrick Begnoche, Lee Miller, Tommi Raij",We thank Mainak Jas for expert advice on BIDS.,"Hernandez-Pavon JC, Schneider-Garces N, Begnoche JP, Miller LE, Raij T (2022) Targeted modulation of human brain interregional effective connectivity with spike-timing dependent plasticity. Neuromodulation: Technology at the Neural Interface. Accepted.",Dr. Ralph and Marian Falk Medical Research Trust,NCT03723434,doi:10.18112/openneuro.ds004024.v1.0.1,STU00204239,"ccPAS, rest, spTMS, Rest",,"EEG, MRI","References
 ----------
 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G., Phillips, C., Delorme, A., Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Scientific Data, 6, 103. https://doi.org/10.1038/s41597-019-0104-8",1,1,0
ds004033,2022-02-18 11:39:43,Joanna Scanlon,1.0.0,Electrode walking study,2022-04-14 17:41:56,1,2,21270400000,0.021 TB,293,18,20,28,v2.0,CC0,"Joanna Scanlon, Nadine Jacobsen, Marike Maack, Stefan Debener",,,,article link follows:,doi:10.18112/openneuro.ds004033.v1.0.0,,mixed,,EEG,"The electrode and synchronized walking study: Participants performed 3 tasks outside:
 eyes open/eyes close, standing, walking alone & walking with experimenter oddball task, and 3 walking with experimenter tasks
 Only part of the second task (standing and walking oddball) was used for the first paper.
 Each of 18 participants performed the task using both active (session 1) and passive (session 2) electrodes, in counterbalanced order 
 Task 1: Eyes open / Eyes closed; 1. Participant stands facing a wall with eyes open (or closed) for 1 min. 
 Then 1 min of eyes closed (or open). This is counterbalanced and repeated. 2X each type. Task 2: Oddball task: Standing / Walking alone / 
 Walking together. Participants performed an oddball task in which they listened to the tones through headphones and silently counted the deviant tones. 
 The tones were 800 and 1000 Hz, with the standard/target status counterbalanced across participants. During the walking conditions, 
 participants walked clockwise around an outdoor (roofed) basketball arena, following pylons. Each block was about 5-6 minutes. 
 Blocks were counterbalanced and repeated 2X each. Task 3: Walking together: Natural / Control / Synchronize 3. Participants walked with 
 the experimenter for 6 minutes in 3 conditions. The experimenter listened to a metronome while walking, and synchronized their steps with it 
 (also true during walking together oddball task). During Natural walking, participant is just asked to walk with the experimenter, with no other
 instruction. In Control, participant is `blinded` using `side-blinders` which block their view of the experimenter. In Synchronize, participants 
 try to synchronize their steps with the experimenter. All walking & oddball conditions started with a countdown (this has a specific trigger for oddball conds,
 not for task 3 conds. It plays during the first ~ 12 seconds of the 6 min trial
 

 - Joanna Scanlon (Feb 2022)",1,1,0
ds004040,2022-02-23 1:14:32,Arnaud Delorme,1.0.0,Trance channeling EEG study,2022-02-24 3:20:20,0,2,12440300000,0.012 TB,160,13,0,0,1.1.1,CC0,"Cedric Cannard, Arnaud Delorme, Helane Wahbeh",,,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6384530.2/,doi:10.18112/openneuro.ds004040.v1.0.0,,trance,,EEG,"This group contains 13 participants that went through a thorough 
 screening and did 2 sessions (different days) each. Experiment design 
 corresponded in alternating (5 minutes) blocs of trance channeling and 
 resting state (3 periods per session for each condition).",1,1,0
ds004043,2022-02-25 0:59:06,Tijl Grootswagers,1.1.0,The time-course of feature-based attention effects dissociated from temporal expectation and target-related processes,2022-02-25 3:17:40,1,1,16582800000,0.017 TB,84,20,0,0,1.0.2,CC0,"Moerel, Denise, Grootswagers, Tijl, Robinson, Amanda, Shatek, Sophia, Woolgar, Alexandra, Carlson, Thomas, Rich, Anina",The authors acknowledge the University of Sydney HPC service for providing High Performance Computing resources.,"Moerel, D., Grootswagers T., Robinson, A.K. et al. Undivided attention: The temporal effects of attention dissociated from decision, memory, and expectation. bioRxiv. (2021). https://doi.org/10.1101/2021.05.24.445376","ARC DP170101840 (ANR,AW) ===NEMAR-SEP=== ARC DP160101300 (TAC) ===NEMAR-SEP=== ARC DP200101787 (TAC) ===NEMAR-SEP=== ARC DE200101159 (AKR) ===NEMAR-SEP=== MRC intramural funding SUAG/052/G101400 (AW)",https://doi.org/10.17605/OSF.IO/5B8K6 ===NEMAR-SEP=== https://doi.org/10.1101/2021.05.24.445376,doi:10.18112/openneuro.ds004043.v1.1.0,,Temporal expectation,,EEG,"Experiment Details
 

 Human electroencephalography recordings from 20 participants. Participants viewed rapid sequences of overlaid oriented grating pairs while detecting a “target” grating of a particular orientation. We manipulated attention, one grating was attended and the other ignored (cued by colour), and temporal expectation, with stimulus onset timing either predictable or not.
 

 Experiment length: 1 hour
 

 More information: 
 

 https://doi.org/10.17605/OSF.IO/5B8K6 (OSF repository with more information and example analysis code)
 

 Moerel, D., Grootswagers, T., Robinson, A. K., Shatek, S. M., Woolgar, A., Carlson, T. A., & Rich, A. N. (2021). Undivided attention: The temporal effects of attention dissociated from decision, memory, and expectation. bioRxiv. doi: https://doi.org/10.1101/2021.05.24.445376",1,1,0
ds004067,2022-03-13 20:01:22,Keith Yoder,1.0.1,Moral conviction and metacognitive ability shape multiple stages of information processing,2022-03-13 21:32:33,1,1,108218000000,0.108 TB,257,80,0,0,1.0.2,CC0,"Yoder, Keith J, Decety, Jean",The authors acknowledge the University of Chicago Grossman Ceneter for Qualitative Biology and Human Behavior.,"Yoder, K.J. & Decety, J. Moral conviction and metacognitive ability shape multiple states of information processing. Cortex.",Seed grant from Grossman Center for Quantitative Biology and Human Behavior (JD),https://doi.org/10.1016/j.cortex.2022.03.008 ===NEMAR-SEP=== https://osf.io/32das/,doi:10.18112/openneuro.ds004067.v1.0.1,,Sociopolitical Judgment Task,,EEG,"Experiment Details
 

 Human electroencephalography recordings from 80 participants. Participants first provided their attitudes about a set of sociopolitical issues, then view photographs of protests that were ostensibly about those same issues. Prior to each photo, they saw a pie chart indicating social support for the issue (low, medium, or high). After each photo, they indicated their support for the protestors.
 

 Other data and analysis scripts can be found on OSF (DOI 10.17605/OSF.IO/32DAS) or at the github repository for the project (https://github.com/Social-Cognitive-Neuroscience-Lab/EEGMoralization)",1,1,0
ds004075,2022-03-18 1:32:05,Adam Boncz,1.0.0,what\_are\_we\_talking\_about,2022-03-18 1:48:59,1,1,7936060000,0.008 TB,466,29,0,0,v1.7.0,CC0,"Adam Boncz, Brigitta Toth, István Winkler",,,"Hungarian Academy of Sciences, grant LP2012-36/2012 (IW) ===NEMAR-SEP=== National Research, Development and Innovation Office, Hungary, grant K132642 (IW) ===NEMAR-SEP=== National Research, Development and Innovation Office, Hungary, grant PD123790 (BT)",,doi:10.18112/openneuro.ds004075.v1.0.0,,"context03, context09, context10, context11",,EEG,,1,1,0
ds004078,2022-03-20 8:43:39,Xiaohan Zhang,1.2.1,A synchronized multimodal neuroimaging dataset to study brain language processing,2022-04-11 13:09:08,1,1,677638000000,0.678 TB,5406,12,0,0,1.6.0,CC0,"Shaonan Wang, Xiaohan Zhang, Jiajun Zhang, Chengqing Zong",,,,,doi:10.18112/openneuro.ds004078.v1.2.1,,"Chinese-RDR, rest",,"MEG, MRI","#### Overview
 

 This synchronized multimodal neuroimaging dataset for studying brain language processing (SMN4Lang) contains:
 

 1. fMRI and MEG data collected on the same 12 participant while they were listening to 6 hours of naturalistic stories;
 

 2. high-resolution structural (T1, T2), diffusion MRI and resting-state fMRI data for each participant;
 3. rich linguistic annotations for the stimuli, including word frequencies, part-of-speech tags, syntactic tree structures, time-aligned characters and words, various types of word and character embeddings.
 

 More details about the dataset are described as follows.
 

 #### Participants
 

 All 12 participants were recruited from universities in Beijing, of which 4 were female, and 8 were male, with an age range 23-30 year. They completed both fMRI and MEG visits (first completed fMRI then MEG experiments which had a gap of 1 month at least), All participants were right-handed adults with Mandarin Chinese as native language who reported having normal hearing and no history of neurological disorders. They were paid and gave written informed consent. The study was conducted under the approval of the Institutional Review Board of Peking University. 
 

 #### Experimental Procedures
 

 Before each scanning, participants first completed a simple information survey form and an informed consent. During both fMRI and MEG scanning, participants were instructed to listen and pay attention to the story stimulus, remain still, answer questions on the screen after each audio was finished. Stimulus presentation was implemented using Psychtoolbox-3. Specifically, at the beginning of each run, there was instruction of ""Waiting for the scanning"" on the screen followed with 8 seconds blank. Then, the instruction became ""This audio is about to start, please listen carefully"" which lasted for 2.65 seconds before playing the audio; during audio play, a centrally located fixation cross was presented; finally, two questions about the story were presented each with four answers to choose from during which time was controlled by participants. Auditory story stimuli were delivered via S14 insert earphones for fMRI studies (with headphones or foam padding were placed over the earphones to reduce scanner noise) and Elekta matching insert earphones for MEG studies. 
 

 The fMRI recording was split into 7 visits with each lasting 1.5 hours in which the T1, T2, resting MRI were collected on the first visit, fMRI with listening tasks was collected from 1 to 6 visits, and the diffusion MRI were collected on the last visit. During MRI scanning including T1, T2, diffusion and resting, participants were instructed to lie relaxed and still in the machine. The MEG recording was split into 6 visits with each lasting 1.5 hours.
 

 #### Stimuli
 

 Stimuli are 60 story audios with 4 to 7 minutes long, comprising various topics such as education and culture. All audios were downloaded from Renmin Daily Review website read by the same male broadcaster. The corresponding texts were also downloaded from the Renmin Daily Review website in which errors were manually corrected to make sure audio and texts are aligned.
 

 #### Annotations
 

 Rich annotations of audios and texts are provided in the derivatives/annotations folder, including:
 

 1. Speech to text alignment: The onset and offset time of each character and words in the audio are provided in the ""stimuli/time\_align"" folder. Note that the onset and offset time were added by 10.65 seconds to align with the time of fMRI images because the fMRI scan was started 10.65 seconds before playing the audio.
 2. Frequency: Character and word frequencies in the ""stimuli/frequency"" folder were calculated from the Xinhua news corpus and then log-transformed.
 3. Textual embeddings: Text embeddings computed by different pre-trained language models (including Word2Vec, BERT, and GPT2) are provided in the ""stimuli/embeddings"" folder. Both the character-level and word-level embeddings computed by Word2Vec and BERT model and the word-level embeddings computed by GPT2 model are provided.
 4. Syntactic annotations: The POS tag of each word, the constituent tree structure, and the dependency tree structure are provided in the ""stimuli/syntactic\_annotations"" folder. The POS tags were annotated by experts following criterion of Peking Chinese Treebank. The constituent tree structure was manually annotated by linguistic students following PKU Chinese Treebank criterion with the TreeEditor tools and all results were double checked by different experts. The dependency tree structure was transformed from the constituent tree using Stanford CoreNLP tools.
 

 #### Preprocessing
 

 The MRI data, including the structural, functional, resting and diffusion images, were preprocessed using the “minimal preprocessing pipelines (HCP)” .
 

 The MEG data was first preprocessed using the temporal Signal Space Separation (tSSS) method and the bad channels were excluded. And then the independent component analysis (ICA) method was applied to remove the ocular artefacts using the MNE software.
 

 #### Usage Notes
 

 For the MEG data of sub-08\_run-16 and sub-09\_run-7, the stimuli-starting triggers were not recorded due to technical problems. The first trigger in these two runs were the stimuli-ending triggers and the starting time can be computed by subtracting the stimuli duration from the time point of the first trigger.",1,1,0
ds004100,2022-04-17 18:17:45,John Bernabei,1.1.3,HUP iEEG Epilepsy Dataset,2022-09-21 20:30:45,0,1,14196300000,0.014 TB,1341,57,16,59,1.4.0,CC0,"John M. Bernabei, Adam Li, Andrew Y. Revell, Rachel J. Smith, Kristin M. Gunnarsdottir, Ian Z. Ong, Kathryn A. Davis, Nishant Sinha, Sridevi Sarma, Brian Litt",,,,,doi:10.18112/openneuro.ds004100.v1.1.3,,"ictal, interictal",,iEEG,"<h1>HUP iEEG dataset</h1>
 

 This dataset was prepared for release as part of a manuscript by Bernabei & Li et al. (in preparation). A subset of the data has been featured in Kini & Bernabei et al., Brain (2019) [1], and Bernabei & Sinha et al., Brain (2022) [2]. 
 

 <h3>Dataset description</h3>
 These files contain de-identified patient data collected as part of surgical treatment for drug resistant epilepsy at the Hospital of the University of Pennsylvania. Each of the 58 subjects underwent intracranial EEG with subdural grid, strip, and depth electrodes (ECoG) or purely stereotactically-placed depth electrodes (SEEG). Each patient also underwent subsequent treatment with surgical resection or laser ablation. Electrophysiologic data for both interictal and ictal periods is available, as are electrode localizations in ICBM152 MNI space. Furthermore, clinically-determined seizure onset channels are provided, as are channels which overlap with the resection/ablation zone, which was rigorously determined by segmenting the resection cavity. 
 

 <h3>BIDS Conversion</h3>
 MNE-BIDS was used to convert the dataset into BIDS format. 
 

 <h3>References</h3>
 [1] Kini L.*, Bernabei J.M.*, Mikhail F., Hadar P., Shah P., Khambhati A., Oechsel K., Archer R., Boccanfuso J.A., Conrad E., Stein J., Das S., Kheder A., Lucas T.H., Davis K.A., Bassett D.S., Litt B., Virtual resection predicts surgical outcome for drug resistant epilepsy. Brain, 2019.
 

 [2] Bernabei J.M.*, Sinha N.*, Arnold T.C., Conrad E., Ong I., Pattnaik A.R., Stein J.M., Shinohara R.T., Lucas T.H., Bassett D.S., Davis K.A., Litt B., Normative intracranial EEG maps epileptogenic tissues in focal epilepsy. Brain, 2022
 

 [3] Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 [4] Holdgraf, C., Appelhoff, S., Bickel, S., Bouchard, K., D'Ambrosio, S., David, O., … Hermes, D. (2019). iEEG-BIDS, extending the Brain Imaging Data Structure specification to human intracranial electrophysiology. Scientific Data, 6, 102. https://doi.org/10.1038/s41597-019-0105-7",1,1,0
ds004105,2022-04-21 18:46:23,Kay Robbins,1.0.0,BCIT Auditory Cueing,2022-05-05 13:49:31,0,1,21876500000,0.022 TB,245,17,0,0,1.7.0,CC0,"Javier Garcia (data), Justin Brooks (data), Scott Kerick (data), Tony Johnson (data and curation), Tim Mullen (data), Jean Vettel (data), Jonathan Touryan (curation), Kay Robbins (curation)",,"Garcia, J.O., Brooks, J., Kerick, S., Johnson, T., Mullen, T.R., Vettel, J.M., 2017. Estimating direction in brain-behavior interactions: Proactive and reactive brain states in driving. NeuroImage 150, 239-249, https://doi.org/10.1016/j.neuroimage.2017.02.057.",This research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-10-0-0002.,"Garcia, J.O., Brooks, J., Kerick, S., Johnson, T., Mullen, T.R., Vettel, J.M., 2017. Estimating direction in brain-behavior interactions: Proactive and reactive brain states in driving. NeuroImage 150, 239-249. https://doi.org/10.1016/j.neuroimage.2017.02.057. ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis I: Spectral and amplitude characteristics across studies, NeuroImage, p. 116361, https://doi.org/10.1016/j.neuroimage.2019.116361 ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis II: Cognitive aspects of event related features, NeuroImage, p. 116054, https://doi.org/10.1016/j.neuroimage.2019.116054",doi:10.18112/openneuro.ds004105.v1.0.0,,DriveRandomSound,8.0.0,EEG,"### Introduction
 

 **Overview:** Subjects in the Auditory Cueing study performed a long-duration simulated driving task with
 perturbations and audio stimuli in a visually sparse environment.
 

 The purpose of this effort was to supplement and extend the related driving research to collect
 prolonged time-on-task measurements of subjects performing a driving task in a simulated environment
 in order to assess fatigue-based performance through novel biomarkers.
 

 Similar to the Baseline Driving study, the Auditory Cueing study was intended to identify periods
 of driver fatigue via predictive algorithms formulated from the analysis of driver EEG data,
 in comparison to the objective performance measures, and in contrast with the (non-fatigued)
 Calibration driving session for the subject. Auditory Cueing extended the Baseline Driving
 paradigm by adding predictive and non-predictive (random) pre-perturbation onset audio cues and
 increasing the frequency and magnitude of perturbation events vs. baseline driving.
 Further information is available on request from [cancta.net](https://cancta.net).
 

 

 ### Methods 
 

 **Subjects:** Volunteers from the local community recruited through advertisements.  
  
 **Apparatus:** Driving simulator with steering wheel and brake / foot pedals (Real Time Technologies; Dearborn, MI);
 Video Refresh Rate (VRR) = 900 Hz; Vehicle data log file Sampling Rate (SR) = 100 Hz);
 EEG (BioSemi 64 (+8) channel systems with 4 eye and 2 mastoid channels recorded; SR=2048 Hz);
 Eye Tracking (Sensomotoric Instruments (SMI); REDEYE250).
 

 **Initial setup:** Upon arrival to the lab, subjects were given an introduction to the
 primary study for which they were recruited and provided informed consent and provided demographics information.
 This was followed by a practice session, to acclimate the subject to the driving simulator.
 The driving practice task lasted 10-15 min, until asymptotic performance in steering and speed control
 was demonstrated and lack of motion sickness was reported.
 Subjects were then outfitted and prepped for eye tracking and EEG acquisition.
 

 **Task organization within the study:** Subjects always began recording sessions by performing
 a Calibration Driving task, which was a 15-minute drive where the subject controlled only the steering
 (and speed was controlled by the simulator). Following this, subjects would perform Auditory Cueing
 condition A and Auditory Cueing condition B, with counter-balancing used across subjects as to
 which of them came first. This study only contains the Auditory Cueing portion of the study.
 

 **Auditory cueing task details:** Auditory Cueing A was 45 minutes of continuous driving,
 with subjects responsible for steering and maintaining speed, while a tone was played periodically at random.
 Auditory Cueing B was similar, but the tones were correlated with the onset of a perturbation event.
 Both driving tasks were conducted on the same simulated long, straight road.
 In each case, the subject was instructed to stay within the boundaries of the right-most lane,
 and to drive at the posted speed limits.
 

 The vehicle was periodically subject to lateral perturbing forces, which could be applied to
 either side of the vehicle, pushing the vehicle out of the center of the lane;
 and the subject was instructed to execute corrective steering actions to return the vehicle to the center of the lane.
 

 **Independent variables:** Auditory Cue (randomly presented before perturbation vs. predictive) 
 

 **Dependent variables:** Reaction times to perturbations, continuous performance based on
 vehicle log (steering wheel angle, lane position, heading error, etc.),
 reaction times to target vehicles (police), Task-Induced Fatigue Scale (TIFS),
 Karolinska Sleepiness Scale (KSS), Visual Analog Scale of Fatigue (VAS-F).
 

 Note: Questionnaire data is available upon request from [cancta.net](https://cancta.net).
 

 **Additional data acquired:** Participant Enrollment Questionnaire, Subject Questionnaire
 for Current Session, Simulator Sickness Questionnaire.
 

 **Experimental Location:** Teledyne Corporation, Durham, NC.
 

 **Note:** This dataset has a corresponding dataset in the BCIT Calibration Driving ds004118 which has the
 15 minute driving task performed prior to this one.",1,1,1
ds004106,2022-04-21 22:44:56,Kay Robbins,1.0.0,BCIT Advanced Guard Duty,2022-04-29 20:29:26,0,1,72590800000,0.073 TB,260,27,0,0,1.7.0,CC0,"Jonathan Touryan (data and curation), Brent Lance (data), Scott Kerick (data), Anthony Ries (data), Kaleb McDowell(data), Tony Johnson (curation), Kay Robbins (curation)",,"Touryan, J., Lance, B.J., Kerick, S.E., Ries, A.J., McDowell, K., 2016. Common EEG features for behavioral estimation in disparate, real-world tasks. Biol. Psychol. 114, 93–107 https://doi.org/10.1016/j.biopsycho.2015.12.009",This research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-10-0-0002.,"Touryan, J., Lance, B.J., Kerick, S.E., Ries, A.J., McDowell, K., 2016. Common EEG features for behavioral estimation in disparate, real-world tasks. Biol. Psychol. 114, 93–107 https://doi.org/10.1016/j.biopsycho.2015.12.009 ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis I: Spectral and amplitude characteristics across studies, NeuroImage, p. 116361, https://doi.org/10.1016/j.neuroimage.2019.116361 ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis II: Cognitive aspects of event related features, NeuroImage, p. 116054, https://doi.org/10.1016/j.neuroimage.2019.116054",doi:10.18112/openneuro.ds004106.v1.0.0,,GuardDuty,8.0.0,EEG,"### Introduction
 

 **Overview:** The Advanced Guard Duty study was designed to measure sustained vigilance in realistic settings by having subjects verify information on replica ID badges. The task was performed in conjunction with two other tasks a calibration driving task and a baseline driving task. The data collected for the two driving tasks is not included in this dataset. Another study (Basic Guard Duty) not included in this collection had a similar set-up but a different experimental design and a different subject pool. In the Basic Guard Duty study the rate of ID presentation varied among tasks. In the Advanced Guard Duty study both the rate of ID presentation and the criteria for verification varied among blocks. Further information is available on request from [cancta.net](https://cancta.net).
 

 

 ### Methods 
 

 **Subjects:** volunteers from the local community recruited through advertisements. 
  
 **Apparatus:** Driving simulator with steering wheel and brake / foot pedals (Real Time Technologies; Dearborn, MI);
 Video Refresh Rate (VRR) = 900 Hz; Vehicle data log file Sampling Rate (SR) = 100 Hz);
 EEG (BioSemi 256 (+8) channel systems with 4 eye and 2 mastoid channels recorded; SR=1024 Hz);
 Eye Tracking (Sensomotoric Instruments (SMI); REDEYE250).
 

 **Initial setup:** Upon arrival to the lab, subjects were given an introduction to the primary study
 for which they were recruited and provided informed consent and provided demographics information.
 This was followed by a practice session, to acclimate the subject to the driving simulator.
 The driving practice task lasted 10-15 min, until asymptotic performance in steering and speed
 control was demonstrated and lack of motion sickness was reported. Subjects were then outfitted
 and prepped for eye tracking and EEG acquisition.
 

 **Task organization:** Subjects always began recording sessions by performing a Calibration Driving task,
 which was a 15-minute drive where the subject controlled only the steering (and speed was controlled by the simulator).
 Following this, subjects would perform the Baseline Driving task and the Guard Duty task,
 with counter-balancing used across subjects as to which of them came first.
 This dataset only contains the Guard Duty task.
 

 The Baseline Driving run was 60 minutes of driving, performed in 6 blocks of 10 minutes each,
 with subjects responsible for speed and steering control. The Calibration and Baseline driving
 tasks were conducted on the same simulated long, straight road in a visually sparse environment.
 The subject was instructed to stay within the boundaries of the right-most lane, and to drive
 at the posted speed limits.
 

 The vehicle was periodically subject to lateral perturbing forces, which could be applied to either
 side of the vehicle, pushing the vehicle out of the center of the lane; and the subject was instructed
  to execute corrective steering actions to return the vehicle to the center of the lane.
 

 **Guard duty task details:** The guard duty task entailed a serial presentation of replica identification (ID) cards
 (750 x 450 pixels) paired with a reference image (300 x 400 pixels).
 

 The replica ID cards had eight components or fields in addition to a common background.
 These components were: photo, name, date of birth (DOB), date of issue, date of expiration, area access,
 ID number, bar code and watermark. The reference images consisted of color photographs of faces.
 Both the ID photo and reference image were chosen from the Multi-PIE database
 (Gross, Matthews, Cohn, Kanade, & Baker, 2010). This database consists of color photographs
 (forward facing head shots) of individuals taken at different points in time.
 Therefore, while the ID photo and reference image were of the same individual,
 the images were not identical (e.g., different hair style, different clothes, different lighting).
 

 The task was divided into ten blocks of five minutes each.
 

 At the beginning of each block, participants were instructed that they were guarding a restricted area
 that required a particular letter designation on the ID card for access (e.g., area C access required).
 Participants were asked to determine if the individual in the image, paired with the corresponding ID card,
 should have access to their restricted area. Some of the ID cards were valid and some were not
 (e.g., expiration date passed, incorrect access area, or photos did not match).
 Participants were instructed to press either an *allow* or *deny* button for each image-ID pairing.
 The two-alternative forced-choice response was self-paced with a maximum time limit of 20 s.
 If the participants chose to deny access, they were subsequently asked to provide a reason.
 Reasons for denied access were selected from a numerical list of five options:
 1:incorrect access, 2:expired ID, 3:suspicious DOB, 4:face mismatch, 5:no watermark.
 If the participant did not respond within the allotted time, the computer forced a deny decision.
 

 The restricted area (area A-E) assigned at the beginning of each block was randomly chosen without
 replacement such that all participants completed two blocks guarding each of the five areas.
 To maintain consistency across participants, expiration dates were automatically generated at
 the beginning of the experiment to have a symmetrical distribution around the current date.
 This distribution was such that the majority of IDs had expiration dates temporally close
 to the current date (i.e., in the near future or recent past).
 

 In each block, the image-ID pairings were presented at one of six different stochastic queuing rates,
 ranging from 1 to 25 per minute (1, 2.5, 10, 15, 20, and 25 per minute).
 The queuing rate varied within each block according to a predefined profile.
 The rate profile had randomly permuted epochs of each queuing rate.
 

 Each epoch lasted 30 s with approximately twice as many low rate epochs (1 and 2.5 image-IDs per minute) as high.
 The rate profiles were shifted for each participant (Latin square design) so that each rate profile
 was assigned to every block for at least two participants. The current rate was indicated through
 a processing queue, on the extreme right-hand side of the display, notifying each participant how
 many IDs are waiting to be checked. For slow rates, most participants were able to process all IDs
 in their queue and had periods where they were waiting for the next ID (i.e., blank screen).
 

 For fast rates, most participants were not able to processes IDs as quickly as they were added to the queue,
 increasing the size of the processing queue. IDs in the queue persisted until they were processed by the
 participant or the block ended.
 

 At the beginning of the experiment, participants were instructed to correctly process each image-ID while
 keeping the queue as short as possible. The stochastic queuing rate was used to increase task realism,
 incorporating periods of high and low task demand, the dynamic rate itself was not explicitly considered
 an independent factor in the present study.
 

 All blocks contained the same ratio of valid and invalid image-ID pairings (82\% valid, 18\% invalid).
 The majority of invalid IDs were due to incorrect access (6\%) and expiration (6\%) whereas the rest were
 invalid for the other reasons: suspicious DOB (2\%), face mismatch (2\%), no watermark (2\%).
 This second group of invalid IDs served as catch trials to verify that participants were examining all fields of the ID.
 

 **Independent variables:** ID presentation rate and verification criteria (varied by block).
 

 **Dependent variables:** ID disposition accuracy and processing times, Task-Induced Fatigue Scale (TIFS),
 Karolinska Sleepiness Scale (KSS), Visual Analog Scale of Fatigue (VAS-F).
 

 Note: questionnaire data is available upon request from [cancta.net](https://cancta.net).
 

 **Additional data acquired:** Participant Enrollment Questionnaire, Subject Questionnaire for Current Session,
 Simulator Sickness Questionnaire.
 

 **Experimental Location:** Science Applications International Corporation, Louisville, CO.",1,1,1
ds004107,2022-04-22 16:06:47,Alexandre Gramfort,1.0.0,MIND DATA,2022-04-22 19:51:09,1,2,82901400000,0.083 TB,397,8,0,0,1.6.0,CC0,"M.P. Weisend, F.M. Hanlon, R. Montano, S.P. Ahlfors, A.C. Leuthold, D. Pantazis, J.C. Mosher, A.P. Georgopoulos, M.S. Hamalainen, C.J. Aine",,"Please cite: 
 

 M.P. Weisend, F.M. Hanlon, R. Montaño, S.P. Ahlfors, A.C. Leuthold, D. Pantazis, J.C. Mosher, A.P. Georgopoulos, M.S. Hämäläinen, C.J. Aine,, V. (2007). Paving the way for cross-site pooling of magnetoencephalography (MEG) data. International Congress Series, Volume 1300, Pages 615-618,.",This work was supported by U.S. DOE Award Number DE-FG02-99ER62764 to The MIND Institute ===NEMAR-SEP=== NIBIB grant R01-EB002010 ===NEMAR-SEP=== by The Center for Functional Neuromaging Technologies (NIH grant P41-RR14075) ===NEMAR-SEP=== and by Los Alamos National Security ===NEMAR-SEP=== LLC ===NEMAR-SEP=== for the NNSA of the U.S. DOE under contract DE-AC52-06NA25396.,https://doi.org/10.1016/j.ics.2006.12.095,doi:10.18112/openneuro.ds004107.v1.0.0,,"noise, auditory, index, median, rest, visual",,"MEG, MRI","This data was part of the study of:
 

 M.P. Weisend, F.M. Hanlon, R. Montaño, S.P. Ahlfors, A.C. Leuthold, D. Pantazis, J.C. Mosher, A.P. Georgopoulos, M.S. Hämäläinen, C.J. Aine,, V. (2007). Paving the way for cross-site pooling of magnetoencephalography (MEG) data. International Congress Series, Volume 1300, Pages 615-618,.
 

 It was converted to BIDS with MNE-BIDS:
 

 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Following the MEG-BIDS format:
 

 Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., Moreau, J., Oostenveld, R., Schoffelen, J., Tadel, F., Wexler, J., Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5, 180110. https://doi.org/10.1038/sdata.2018.110",1,1,0
ds004117,2022-05-01 20:28:00,Kay Robbins,1.0.1,Sternberg Working Memory,2022-06-16 23:23:02,1,1,6229870000,0.006 TB,601,23,19,40,1.7.0,CC0,"Julie Onton (data), Scott Makeig (data and curation), Arnaud Delorme (data and curation), Dung Truong (curation), Kay Robbins (curation)",,Cite this paper: https://pubmed.ncbi.nlm.nih.gov/15927487/ and consider including the following message: 'This data was obtained from the OpenNeuro database. Its accession number is ds004117.',National Institutes of Health NS047293 ===NEMAR-SEP=== Swartz Foundation,"Onton, J., Delorme, A., and Makeig, S. (2005). Frontal midline EEG dynamics during working memory. Neuroimage 27, 241-356. https://doi.org/10.1016/j.neuroimage.2005.04.014.",doi:10.18112/openneuro.ds004117.v1.0.1,,WorkingMemory,8.0.0,EEG,"## Modified Sternberg Working Memory Experiment
 

 **Project name:** EEG and working memory
 

 **Years the project ran:** 2004-05
 

 **Brief overview of experiment task:** The purpose of this Modified Sternberg task study was to
 explore source-resolved EEG brain dynamics associated with selectively committing a series of letters to memory,
 then after a brief maintenance period responding by button press either yes or no to the question of whether
 a presented query letter had been in the just-presented set of to-be-memorized letters.
 

 The task is a modified version of the classic Sternberg working memory task, with two added features:
 (1) interspersing the sequence of presented (black) letters to be memorized with (green) letters to be ignored,
 and (2) delivering auditory feedback on each trial as to the correctness of the participant response
 (beep = correct, buzz = incorrect).
 

 **Data collection:** Scalp EEG data were collected from 71 scalp electrode channels,
 each referred to a right mastoid electrode, at a sampling rate of 250 Hz/channel within an
 analog passband of 0.1 to 100 Hz.
 

 **Contact person:  Julie Onton <julieonton@gmail.com>, ORCID#:0000-0002-5602-3557.
 

 **Access information:** Contributed to OpenNeuro.org and NEMAR.org in BIDS format
 following annotation using HED 8.0.0 in April, 2022.
 

 **Independent variables:** Letter category (to\_memorize, to\_ignore);
 numbers of presented letters to\_memorize/to\_ignore (3/5, 5/3, 7/1);
 probe letter category (in/not in the presented set). Note, only letters to be memorized appear as in set probe letters.
 

 **Dependent variables:** EEG; button press response latency; participant response (correct/incorrect).
 

 **Participant pool:** The dataset includes data collected from 23 healthy young adult subjects
 (7 male, 6 female, 11 unidentified) between the ages of 19 and 40 years of age.
 

 **Apparatus:** A Neurobehavioral Systems, Inc. EEG system running under Window98 acquired the data.
 The experiment control program was Presentation (Neurobehavioral Systems, Inc.).
 

 **Initial setup:** EEG data were collected from 71 channels (69 scalp and two periocular electrodes,
 all referred to right mastoid) with an analog pass band of 0.01 to 100 Hz (SA Instrumentation, San Diego).
 Input impedances were brought under 5 kOhms by careful scalp preparation.
 

 Data for subjects 1-12 was acquired at a sampling rate of 250Hz. The data for subject 14 was acquired at
 1000 Hz and the data for subjects 15-24 was a acquired using a 500 Hz sampling rate.
 

 **Task organization:** Data was organized into runs of 25 trials each followed by a rest.
 Each block was a separate run in the BIDS dataset.
 

 **Task details:** Each trial consisted of the following sequence of events:
 

 **[Trial initiation]**. After a self-selected, variable delay,
 the subject initiated the next trial by pressing either response button,
 triggering the reappearance of the fixation cross.
 

 **[Letter sequence presentation]**. In these experiments, following a 5s presentation of a central
 fixation cross cue, a series of 8 visual letters (~2 deg of visual angle) were presented at
 screen center for 1.2s followed by a 0.2s ISI:
 

 - Either 3, 5, or 7 of these were colored black.
 - The participant was to memorize as letters in this set.
 - The other 5, 3, or 1 letters in the sequence were colored green and participants were to ignore these.
 - The letters were drawn without substitution from the English alphabet (omitting only A, E, I, O, and U).
 - The presentation order of black and green letters was pseudo-random.
 

 **[Memory maintenance]**. In place of a ninth letter, a dash appeared on the screen to signal the
 beginning of a Memory Maintenance period lasting between 2 to 4 s.
 During this period subjects were to silently rehearse the identities of the memorized letters.
 

 **[Memory probe]**. A (red) probe letter then appeared, prompting the subject to respond by
 pressing one of two buttons (with the thumb or index finger of their dominant hand)
 to indicate whether or not the probe letter had been in the trial?s to-be-memorized letter set.
 

 **[Response feedback]**. An auditory feedback signal (a confirmatory beep or cautionary buzz),
 then presented beginning at 400 ms after the button press, informed the subject whether their
 response was correct or incorrect. Note: responses in the task were largely correct.
 

 **[Session time structure]**. Each task session comprised of 3 or 4 task blocks of 25 trials each
 separated into individual run files.
 

 **Experiment location**: Swartz Center for Neural Computation (SCCN), University of California San Diego, La Jolla CA (USA).
 

 **Note 1**: Results presented in Onton, J., Delorme, A. and Makeig, S., 2005.
 Frontal midline EEG dynamics during working memory. Neuroimage, 27(2), pp.341-356.
 

 **Note 2**: This paradigm is one of 20 event-related EEG task paradigms selected for replication by the EEGManyLabs project.
 For details, see https://psyarxiv.com/528nr/. Contact: Yuri Pavlov <pavlovug@gmail.com>.
 

 **Note 3**: Participant 5 did not have feedback events in the trials.
 

 **Note 4**: The code subdirectory has several auxilliary files that were produced during
 the curation process. The curation was done using a series of Jupyter notebooks
 that are available as run in the code/curation\_notebooks subdirectory.
 

 During the running of these curation notebooks information about the status was logged
 using the HEDLogger. The output of the logging process is in code/curation\_logs.
 

 Updated versions of the curation notebooks can be found at:
 https://github.com/hed-standard/hed-examples/tree/main/hedcode/jupyter\_notebooks/dataset\_specific\_processing/sternberg",1,1,1
ds004118,2022-05-02 16:03:12,Kay Robbins,1.0.1,BCIT Calibration Driving,2022-05-04 22:58:22,0,7,133504000000,0.134 TB,2294,156,0,0,1.7.0,CC0,"Jonathan Touryan (data and curation), Greg Apker (data), Brent Lance (data), Scott Kerick (data), Anthony Ries (data), Kaleb McDowell (data), Tony Johnson (curation), Kay Robbins (curation)",,"Touryan, J., Apker, G., Lance, B.J., Kerick, S.E., Ries, A.J., McDowell, K., 2014. Estimating endogenous changes in task performance from EEG. Front. Neurosci. 8, https://doi.org/10.3389/fnins.2014.00155.",This research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-10-0-0002.,"Touryan, J., Apker, G., Lance, B.J., Kerick, S.E., Ries, A.J., McDowell, K., 2014. Estimating endogenous changes in task performance from EEG. Front. Neurosci. 8, https://doi.org/10.3389/fnins.2014.00155. ===NEMAR-SEP=== Brooks, J., Kerick, S., 2015. Event-related alpha perturbations related to the scaling of steering wheel corrections. Physiol. Behav. 149, 287?293, https://doi.org/10.1016/j.physbeh.2015.05.026. ===NEMAR-SEP=== Brooks, J.R., Kerick, S.E., McDowell, K., 2015. Novel measure of driver and vehicle interaction demonstrates transient changes related to alerting. J. Mot. Behav. J. Mot. Behav. 47, 47, 106, 106?116, https://doi.org/10.1080/00222895.2014.959887, 10.1080/00222895.2014.959887. ===NEMAR-SEP=== Garcia, J.O., Brooks, J., Kerick, S., Johnson, T., Mullen, T.R., Vettel, J.M., 2017. Estimating direction in brain-behavior interactions: Proactive and reactive brain states in driving. NeuroImage 150, 239?249, https://doi.org/10.1016/j.neuroimage.2017.02.057. ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis I: Spectral and amplitude characteristics across studies, NeuroImage, p. 116361, https://doi.org/10.1016/j.neuroimage.2019.116361. ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis II: Cognitive aspects of event related features, NeuroImage, p. 116054, https://doi.org/10.1016/j.neuroimage.2019.116054.",doi:10.18112/openneuro.ds004118.v1.0.1,,Drive,8.0.0,EEG,"## BCIT Calibration Driving
 

 ### Introduction
 

 **Overview:** The Calibration Driving study was intended to provide calibration data for applying
 fatigue-based driver performance prediction algorithms. Calibration data sets were designed to be
 the first component of every recording session within the BCIT program, which featured multiple
 studies investigating fatigue.
 

 Collectively, the Calibration Driving recordings comprise a 'virtual' study, in which driving performance
 at the calibration level can be analyzed. When analyzed with other same-subject data, involving much longer tasks,
 the calibration data sets can be used as the basis for non-fatigue state performance.
 

 Further information is available on request from [cancta.net](https://cancta.net).
 

 The task was performed using identical systems at three different sites:
 

 - Army Research Laboratory, Aberdeen MD (T1)  
 - Teledyne Corporation, Durham, NC (T2)  
 - Science Applications International Corporation (SAIC), Louisville, CO (T3)  
 

 All sites used identical driving simulator setups.
 

 The data collected at site T1 used a 64-channel Biosemi EEG headset as did the data collected at site T2,
 while site T3 used a 256-channel Biosemi EEG headset.
 

 Data from site T1 has legacy subject IDs in the range 1000 to 1999.
 Data from site T2 has legacy subject IDs in the range 2000 to 2999.
 Data from site T3 has legacy subject IDs in the range 3000 to 3999.
 Legacy subject IDs are unique across the entire BCIT program.
 

 ### Methods 
 

 **Subjects:** Subjects at Aberdeen Proving Grounds were recruited, on a voluntary basis from among the
 scientists and engineers working at APG.
 

 Subjects recruited by Teledyne and SAIC were found via advertising and community outreach efforts,
 and primarily consisted of local college students.
  
 **Apparatus:** Driving simulator with steering wheel and brake / foot pedals (Real Time Technologies; Dearborn, MI);
  Video Refresh Rate (VRR) = 900 Hz; Vehicle data log file Sampling Rate (SR) = 100 Hz);
  EEG (BioSemi 256 (+8) channel systems with 4 eye and 2 mastoid channels recorded; SR=1024 Hz);
  Eye Tracking (Sensomotoric Instruments (SMI); REDEYE250).
 

 **Initial setup:** Upon arrival to the lab, subjects were given an introduction to the primary study
 for which they were recruited and provided informed consent and provided demographics information.
 This was followed by a practice session, to acclimate the subject to the driving simulator.
 The driving practice task lasted 10-15 min, until asymptotic performance in steering and speed control
 was demonstrated and lack of motion sickness was reported.
 

 Subjects were then outfitted and prepped for eye tracking and EEG acquisition.
 

 **Task organization:** The Calibration study featured a 15-minute trial, requiring the driver to control
 the steering of a simulated vehicle on a long, straight road in a visually sparse environment.
 

 With the vehicle speed controlled by the driving simulator, the only task for the subject was to maintain
 the vehicle position in the center of the lane. The vehicle was periodically subject to lateral perturbing forces,
 which could be applied to either side of the vehicle, pushing the vehicle out of the center of the lane;
 and the subject was instructed to execute corrective steering actions to return the vehicle to the center of the lane.
 

 **Independent variables:** None.
 

 **Dependent variables:** Reaction times to perturbations, continuous performance based on vehicle log
 (steering wheel angle, lane position, heading error, etc.), Task-Induced Fatigue Scale (TIFS),
 Karolinska Sleepiness Scale (KSS), Visual Analog Scale of Fatigue (VAS-F).
 

 **Note:** questionnaire data is available upon request from [cancta.net](https://cancta.net).
 

 **Additional data acquired:** Participant Enrollment Questionnaire, Subject Questionnaire for Current Session,
 Simulator Sickness Questionnaire.
 

 **Experimental Locations:** Army Research Laboratory, Aberdeen MD (site T1);
 Teledyne Corporation, Durham, NC (site T2);
 Science Applications International Corporation (SAIC), Louisville, CO (site T3).
 

 **Note:** This 15-minute task was performed prior to every run in the BCIT experimental series. Thus,
 the runs have corresponding runs in one or more of BCIT Advanced Guard Duty (ds004106),
 BCIT Basic Guard Duty (ds004119), BCIT Baseline Driving (ds004120), BCIT Mind Wandering (ds004121),
 BCIT Speed Control (ds004122) and Traffic Complexity (ds004123) that were conducted on the
 same subject during the same session. The Calibration Driving run was always conducted first.",1,1,1
ds004119,2022-05-02 20:24:56,Kay Robbins,1.0.0,BCIT Basic Guard Duty,2022-05-04 22:57:53,0,1,59203200000,0.059 TB,201,21,0,0,1.7.0,CC0,"Jonathan Touryan (data and curation), Brent Lance (data), Scott Kerick (data), Anthony Ries (data), Kaleb McDowell (data), Tony Johnson (curation), Kay Robbins (curation)",,"Touryan, J., Lance, B.J., Kerick, S.E., Ries, A.J., McDowell, K., 2016. Common EEG features for behavioral estimation in disparate, real-world tasks. Biol. Psychol. 114, 93–107 https://doi.org/10.1016/j.biopsycho.2015.12.009",This research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-10-0-0002.,"Touryan, J., Lance, B.J., Kerick, S.E., Ries, A.J., McDowell, K., 2016. Common EEG features for behavioral estimation in disparate, real-world tasks. Biol. Psychol. 114, 93–107 https://doi.org/10.1016/j.biopsycho.2015.12.009 ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis I: Spectral and amplitude characteristics across studies, NeuroImage, p. 116361, https://doi.org/10.1016/j.neuroimage.2019.116361 ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis II: Cognitive aspects of event related features, NeuroImage, p. 116054, https://doi.org/10.1016/j.neuroimage.2019.116054",doi:10.18112/openneuro.ds004119.v1.0.0,,GuardDuty,8.0.0,EEG,"## BCIT Basic Guard Duty
 

 ### Introduction
 

 **Overview:** The Basic Guard Duty study was designed to measure sustained vigilance in realistic settings
 by having subjects verify information on replica ID badges.
 The task was performed in conjunction with two other tasks a calibration driving task and a baseline driving task.
 The data collected for the two driving tasks is not included in this dataset.
 Another study (Advanced Guard Duty), which included a similar set-up but a different experimental design
 and a different subject pool, is not included in this dataset. In the Basic Guard Duty study the rate of
 ID presentation varied among tasks. In the Advanced Guard Duty study both the rate of ID presentation
 and the criteria for verification varied among blocks. Further information is available on request
 from [cancta.net](https://cancta.net).
 

 

 ### Methods 
 

 **Subjects:** Volunteers from the local community recruited through advertisements.
  
 **Apparatus:** Driving simulator with steering wheel and brake / foot pedals (Real Time Technologies; Dearborn, MI);
 Video Refresh Rate (VRR) = 900 Hz; Vehicle data log file Sampling Rate (SR) = 100 Hz);
 EEG (BioSemi 256 (+8) channel systems with 4 eye and 2 mastoid channels recorded; SR=1024 Hz);
 Eye Tracking (Sensomotoric Instruments (SMI); REDEYE250).
 

 **Initial setup:** Upon arrival to the lab, subjects were given an introduction to the primary study
 for which they were recruited and provided informed consent and provided demographics information.
 This was followed by a practice session, to acclimate the subject to the driving simulator.
 The driving practice task lasted 10-15 min, until asymptotic performance in steering and speed control
 was demonstrated and lack of motion sickness was reported. Subjects were then outfitted and prepped
 for eye tracking and EEG acquisition.
 

 **Task organization:** Subjects always began recording sessions by performing a Calibration Driving task,
 which was a 15-minute drive where the subject controlled only the steering (and speed was controlled by the simulator).
 Following this, subjects would perform the Baseline Driving task and the Guard Duty task,
 with counter-balancing used across subjects as to which of them came first.
 The Baseline Driving and Calibration Driving tasks are not included in this dataset.
 

 **Guard duty task details:** The guard duty task entailed a serial presentation of replica
 identification (ID) cards (750 ? 450 pixels) paired with a reference image (300 x 400 pixels).
 The replica ID cards had eight components or fields in addition to a common background.
 These components were: photo, name, date of birth (DOB), date of issue, date of expiration,
 area access, ID number, bar code and watermark. The reference images consisted of color photographs of faces.
 Both the ID photo and reference image were chosen from the Multi-PIE database
 (Gross, Matthews, Cohn, Kanade, & Baker, 2010). This database consists of color photographs (forward facing head shots) of individuals taken at different points in time. Therefore, while the ID photo and reference image were of the same individual, the images were not identical (e.g., different hair style, different clothes, different lighting). The task was divided into ten blocks of five minutes each.
 

 At the beginning of each block, participants were instructed that they were guarding a restricted area
 that required a particular letter designation on the ID card for access (e.g., area C access required).
 Participants were asked to determine if the individual in the image, paired with the corresponding ID card,
 should have access to their restricted area. Some of the ID cards were valid and some were not
 (e.g., expiration date passed, incorrect access area, or photos did not match).
 Participants were instructed to press either an *allow* or *deny* button for each image-ID pairing.
 

 The two-alternative forced-choice response was self-paced with a maximum time limit of 20s.
 If the participants chose to deny access, they were subsequently asked to provide a reason.
 Reasons for denied access were selected from a numerical list of five options:
 1:incorrect access, 2:expired ID, 3:suspicious DOB, 4:face mismatch, 5:no watermark.
 If the participant did not respond within the allotted time, the computer forced a deny decision.
 The restricted area (area A-E) assigned at the beginning of each block was randomly chosen without
 replacement such that all participants completed two blocks guarding each of the five areas.
 To maintain consistency across participants, expiration dates were automatically generated
 at the beginning of the experiment to have a symmetrical distribution around the current date.
 This distribution was such that the majority of IDs had expiration dates temporally close to
 the current date (i.e., in the near future or recent past).
 

 In each block, the image-ID pairings were presented at one of six different stochastic queuing rates,
 ranging from 1 to 25 per minute (1, 2.5, 10, 15, 20, and 25 per minute).
 The queuing rate varied within each block according to a predefined profile.
 The rate profile had randomly permuted epochs of each queuing rate.
 Each epoch lasted 30s with approximately twice as many low rate epochs (1 and 2.5 image-IDs per minute) as high.
 

 The rate profiles were shifted for each participant (Latin square design) so that each rate profile
 was assigned to every block for at least two participants. The current rate was indicated through
 a processing queue, on the extreme right-hand side of the display, notifying each participant
 how many IDs are waiting to be checked. For slow rates, most participants were able to process
 all IDs in their queue and had periods where they were waiting for the next ID (i.e., blank screen).
 

 For fast rates, most participants were not able to processes IDs as quickly as they were added to the queue,
 increasing the size of the processing queue. IDs in the queue persisted until they were processed by
 the participant or the block ended. At the beginning of the experiment, participants were instructed
 to correctly process each image-ID while keeping the queue as short as possible.
 Whereas the stochastic queuing rate was used to increase task realism, incorporating periods of high
 and low task demand, the dynamic rate itself was not explicitly considered an independent factor in the present study.
 

 All blocks contained the same ratio of valid and invalid image-ID pairings (82\% valid, 18\% invalid).
 The majority of invalid IDs were due to incorrect access (6\%) and expiration (6\%) whereas the rest
 were invalid for the other reasons: suspicious DOB (2\%), face mismatch (2\%), no watermark (2\%).
 This second group of invalid IDs served as catch trials to verify that participants were examining
 all fields of the ID.
 

 **Independent variables:** ID presentation rate (varied by block)
 

 **Dependent variables:** ID disposition accuracy and processing times, Task-Induced Fatigue Scale (TIFS),
 Karolinska Sleepiness Scale (KSS), Visual Analog Scale of Fatigue (VAS-F).
 Note: The questionnaire data is available upon request from [cancta.net](https://cancta.net).
 

 **Additional data acquired:** Participant Enrollment Questionnaire, Subject Questionnaire
 for Current Session, Simulator Sickness Questionnaire.
 

 **Experimental Location:** Science Applications International Corporation, Louisville, CO
 

 **Note 1:** This dataset has corresponding runs in the BCIT Calibration Driving ds004118 during
 which a the 15 minute driving task was performed prior to this one.
 

 **Note 2:** This dataset has a corresponding runs in the BCIT Baseline Driving ds004120 which
 were conducted on the same subject during the same session, counterbalanced with these.",1,1,1
ds004120,2022-05-03 0:06:41,Kay Robbins,1.0.0,BCIT Baseline Driving,2022-05-04 22:57:30,0,3,324791000000,0.325 TB,1266,109,0,0,1.7.0,CC0,"Jonathan Touryan (data and curation), Greg Apker (data), Brent Lance (data), Scott Kerick (data), Anthony Ries (data), Kaleb McDowell (data), Tony Johnson (curation), Kay Robbins (curation)",,"Touryan, J., Apker, G., Lance, B.J., Kerick, S.E., Ries, A.J., McDowell, K., 2014. Estimating endogenous changes in task performance from EEG. Front. Neurosci. 8. https://doi.org/10.3389/fnins.2014.00155.",This research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-10-0-0002.,"Touryan, J., Apker, G., Lance, B.J., Kerick, S.E., Ries, A.J., McDowell, K., 2014. Estimating endogenous changes in task performance from EEG. Front. Neurosci. 8. https://doi.org/10.3389/fnins.2014.00155. ===NEMAR-SEP=== Brooks, J., Kerick, S., 2015. Event-related alpha perturbations related to the scaling of steering wheel corrections. Physiol. Behav. 149, 287-293. https://doi.org/10.1016/j.physbeh.2015.05.026. ===NEMAR-SEP=== Brooks, J.R., Kerick, S.E., McDowell, K., 2015. Novel measure of driver and vehicle interaction demonstrates transient changes related to alerting. J. Mot. Behav. J. Mot. Behav. 47, 47, 106, 106?116. https://doi.org/10.1080/00222895.2014.959887, 10.1080/00222895.2014.959887. ===NEMAR-SEP=== Garcia, J.O., Brooks, J., Kerick, S., Johnson, T., Mullen, T.R., Vettel, J.M., 2017. Estimating direction in brain-behavior interactions: Proactive and reactive brain states in driving. NeuroImage 150, 239?249. https://doi.org/10.1016/j.neuroimage.2017.02.057. ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis I: Spectral and amplitude characteristics across studies, NeuroImage, p. 116361, https://doi.org/10.1016/j.neuroimage.2019.116361. ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis II: Cognitive aspects of event related features, NeuroImage, p. 116054, https://doi.org/10.1016/j.neuroimage.2019.116054.",doi:10.18112/openneuro.ds004120.v1.0.0,,DriveWithSpeedChange,8.0.0,EEG,"## BCIT Baseline Driving
 

 ### Introduction
 

 **Overview:** The Baseline Driving study was designed to collect extended time-on-task measurements of
 subjects performing a driving task in a simulated environment in order to assess fatigue-based performance
 through novel biomarkers. The Baseline Driving study was intended to identify periods of driver fatigue
 via predictive algorithms formulated from the analysis of driver EEG data, in comparison to the objective
 performance measures, and in contrast with the (non-fatigued) Calibration driving session for the subject.
 

 Baseline driving data sets were designed to be the second component of every recording session within the
 BCIT program, which featured multiple studies investigating fatigue.
 

 Collectively, the Baseline Driving recordings comprise a virtual study, in which long time-on-task
 driving performance can be analyzed for fatigue-related EEG biomarkers based on measured driving
 performance degradation. Further information is available on request from [cancta.net](https://cancta.net).
 

 The task was performed using identical systems at three different sites:
 

 - Army Research Laboratory, Aberdeen MD (T1)  
 - Teledyne Corporation, Durham, NC (T2)  
 - Science Applications International Corporation (SAIC), Louisville, CO (T3) 
 

 All sites used identical driving simulator setups.
 The data collected at site T1 used a 64-channel Biosemi EEG headset as did the data collected at site T2,
 while site T3 used a 256-channel Biosemi EEG headset.
 

 Data from site T1 has legacy subject IDs in the range 1000 to 1999.
 Data from site T2 has legacy subject IDs in the range 2000 to 2999.
 Data from site T3 has legacy subject IDs in the range 3000 to 3999.
 Legacy subject IDs are unique across the entire BCIT program.
 

 ### Methods 
 

 **Subjects:** Subjects at Aberdeen Proving Grounds were recruited, on a voluntary basis from among
 the scientists and engineers working at APG.
 

 Subjects recruited by Teledyne and SAIC were found via advertising and community outreach efforts,
 and primarily consisted of local college students.
  
 **Apparatus:** Driving simulator with steering wheel and brake / foot pedals (Real Time Technologies; Dearborn, MI);
 Video Refresh Rate (VRR) = 900 Hz; Vehicle data log file Sampling Rate (SR) = 100 Hz);
 EEG (BioSemi 256 (+8) channel systems with 4 eye and 2 mastoid channels recorded; SR=1024 Hz);
 Eye Tracking (Sensomotoric Instruments (SMI); REDEYE250). Eye tracking data is not included in this dataset.
 

 **Initial setup:** Upon arrival to the lab, subjects were given an introduction to the primary
 study for which they were recruited and provided informed consent and provided demographics information.
 This was followed by a practice session, to acclimate the subject to the driving simulator.
 The driving practice task lasted 10-15 min, until asymptotic performance in steering and speed
 control was demonstrated and lack of motion sickness was reported. Subjects were then outfitted
 and prepped for eye tracking and EEG acquisition.
 

 **Task organization:** Subjects always began recording sessions by performing a Calibration Driving task,
 which was a 15-minute drive where the subject controlled only the steering (and speed was controlled by the simulator).
 

 Following this, subjects would perform the Baseline Driving task and the Guard Duty task,
 with counter-balancing used across subjects as to which of them came first.
 

 The Baseline Driving run was 60 minutes of driving, performed in 6 blocks of 10 minutes each,
 with subjects responsible for speed and steering control.
 

 The subject was instructed to stay within the boundaries of the right-most lane,
 and to drive at the posted speed limits. The vehicle was periodically subject to lateral perturbing forces,
 which could be applied to either side of the vehicle, pushing the vehicle out of the center of the lane;
 and the subject was instructed to execute corrective steering actions to return the vehicle to the center of the lane.
 

 **Independent variables:** For T1 (ARL) and T3 (SAIC) there were no independent variables.
 For T2 data sets (Teledyne), independent variables were Visual Complexity (high vs. low),
 Perturbation Frequency (high vs. low).
 

 **Dependent variables:** Reaction times to perturbations, continuous performance based
 on vehicle log (steering wheel angle, lane position, heading error, etc.),
 Task-Induced Fatigue Scale (TIFS), Karolinska Sleepiness Scale (KSS), Visual Analog Scale of Fatigue (VAS-F).
 Note: questionnaire data is available upon request from [cancta.net](https://cancta.net).
 

 **Additional data acquired:** Participant Enrollment Questionnaire, Subject Questionnaire for Current Session,
 Simulator Sickness Questionnaire.
 

 **Experimental Locations:** Army Research Laboratory, Aberdeen MD (site T1);
 Teledyne Corporation, Durham, NC (site T2);
 Science Applications International Corporation (SAIC), Louisville, CO (site T3).
 

 **Note 1:** This dataset has a corresponding dataset in the BCIT Calibration Driving ds004118 which has the
 15 minute driving task performed prior to this one.
 

 **Note 2:** Some of the subjects in this dataset performed either the BCIT Basic Guard Duty Task (ds004118) or
 the BCIT Advanced Guard Duty Task (ds004106) counterbalanced during the same session.",1,1,1
ds004121,2022-05-03 11:46:09,Kay Robbins,1.0.0,BCIT Mind Wandering,2022-05-04 22:55:48,0,1,25671400000,0.026 TB,391,21,0,0,1.7.0,CC0,"Jonathan Touryan (data and curation), Greg Apker (data), Brent Lance (data), Scott Kerick (data), Anthony Ries (data), Justin Brooks (data), Kaleb McDowell (data), Tony Johnson (curation), Kay Robbins (curation)",,"Garcia, J.O., Brooks, J., Kerick, S., Johnson, T., Mullen, T.R., Vettel, J.M., 2017. Estimating direction in brain-behavior interactions: Proactive and reactive brain states in driving. NeuroImage 150, 239?249. https://doi.org/10.1016/j.neuroimage.2017.02.057.",This research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-10-0-0002.,"Touryan, J., Apker, G., Lance, B.J., Kerick, S.E., Ries, A.J., McDowell, K., 2014. Estimating endogenous changes in task performance from EEG. Front. Neurosci. 8, https://doi.org/10.3389/fnins.2014.00155. ===NEMAR-SEP=== Brooks, J., Kerick, S., 2015. Event-related alpha perturbations related to the scaling of steering wheel corrections. Physiol. Behav. 149, 287?293,https://doi.org/10.1016/j.physbeh.2015.05.026. ===NEMAR-SEP=== Brooks, J.R., Kerick, S.E., McDowell, K., 2015. Novel measure of driver and vehicle interaction demonstrates transient changes related to alerting. J. Mot. Behav. J. Mot. Behav. 47, 47, 106, 106?116, https://doi.org/10.1080/00222895.2014.959887, 10.1080/00222895.2014.959887. ===NEMAR-SEP=== Garcia, J.O., Brooks, J., Kerick, S., Johnson, T., Mullen, T.R., Vettel, J.M., 2017. Estimating direction in brain-behavior interactions: Proactive and reactive brain states in driving. NeuroImage 150, 239?249, https://doi.org/10.1016/j.neuroimage.2017.02.057. ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis I: Spectral and amplitude characteristics across studies, NeuroImage, p. 116361, https://doi.org/10.1016/j.neuroimage.2019.116361 ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis II: Cognitive aspects of event related features, NeuroImage, p. 116054, https://doi.org/10.1016/j.neuroimage.2019.116054",doi:10.18112/openneuro.ds004121.v1.0.0,,DriveWithTaskAudio,8.0.0,EEG,"## BCIT Mind Wandering
 

 ### Introduction
 

 **Overview:** Subjects in the Mind Wandering study performed a long-duration simulated driving task
 with perturbations and audio stimuli in a visually complex environment.
 The purpose of this effort was to supplement and extend the related driving research to collect
 prolonged time-on-task measurements of subjects performing a driving task in a simulated environment
 in order to assess fatigue-based performance through novel biomarkers. Similar to the Baseline Driving study,
 the Mind Wandring study was intended to identify periods of driver fatigue via predictive algorithms formulated
 from the analysis of driver EEG data, in comparison to the objective performance measures,
 and in contrast with the (non-fatigued) Calibration driving session for the subject.
 

 Mind Wandering extended the paradigm by adding different types of background audio
 (task relevant, non-task relevant, internal focus) and a vigilance task (identify police vehicles),
 in addition to increasing perturbation magnitude and frequency vs. baseline driving.
 

 Further information is available on request from [cancta.net](https://cancta.net).
 

 

 ### Methods 
 

 **Subjects:** Volunteers from the local community recruited through advertisements.  
  
 **Apparatus:** Driving simulator with steering wheel and brake / foot pedals (Real Time Technologies; Dearborn, MI);
 Video Refresh Rate (VRR) = 900 Hz; Vehicle data log file Sampling Rate (SR) = 100 Hz);
 EEG (BioSemi 64 (+8) channel systems with 4 eye and 2 mastoid channels recorded; SR=2048 Hz);
 Eye Tracking (Sensomotoric Instruments (SMI); REDEYE250).
 

 **Initial setup:** Upon arrival to the lab, subjects were given an introduction to the primary
 study for which they were recruited and provided informed consent and provided demographics information.
 This was followed by a practice session, to acclimate the subject to the driving simulator.
 The driving practice task lasted 10-15 min, until asymptotic performance in steering and
 speed control was demonstrated and lack of motion sickness was reported.
 

 Subjects were then outfitted and prepped for eye tracking and EEG acquisition.
 

 **Task organization within the study:** Subjects always began recording sessions by performing a
 Calibration Driving task, which was a 15-minute drive where the subject controlled only
 the steering (and speed was controlled by the simulator).
 

 **Mind wandering task details:** Subjects would perform Mind Wandering conditions
 A, B, and C, with counter-balancing used across subjects as to which of them came first.
 

 Mind Wandering A was 30 minutes of continuous driving, with subjects responsible for
 steering and maintaining speed, while task relevant audio (traffic safety) played in the background.
 Subjects were instructed to look for police vehicles and respond by pressing a button on the steering wheel.
 

 Mind Wandering B and C were similar, with non-task relevant audio (e.g. sports broadcast) in B
 and internal focus audio (mindfulness breathing exercise) in C.
 Both driving tasks were conducted on the same simulated long, straight road,
 that contained a mix of regular traffic and police vehicles.
 

 In each case, the subject was instructed to stay within the boundaries of the right-most lane,
 and to drive at the posted speed limits.
 

 The vehicle was periodically subject to lateral perturbing forces,
 which could be applied to either side of the vehicle, pushing the vehicle out of the center
 of the lane; and the subject was instructed to execute corrective steering actions to return
 the vehicle to the center of the lane.
 

 **Independent variables:** Background Audio (task relevant vs. non-task relevant vs. internal focus).
 

 **Dependent variables:** Reaction times to perturbations, continuous performance based on vehicle log
 (steering wheel angle, lane position, heading error, etc.), reaction times to target vehicles (police),
 Task-Induced Fatigue Scale (TIFS), Karolinska Sleepiness Scale (KSS), Visual Analog Scale of Fatigue (VAS-F).
 Note: questionnaire data is available upon request from [cancta.net](https://cancta.net).
 

 **Additional data acquired:** Participant Enrollment Questionnaire, Subject Questionnaire for Current Session,
 Simulator Sickness Questionnaire.
 

 **Experimental Location:** Teledyne Corporation, Durham, NC.
 

 **Note:** This dataset has a corresponding dataset in the BCIT Calibration Driving ds004118 which has the
 15 minute driving task prior to this one.",1,1,1
ds004122,2022-05-03 13:04:29,Kay Robbins,1.0.0,BCIT Speed Control,2022-05-04 22:56:55,0,1,38826700000,0.039 TB,450,32,0,0,1.7.0,CC0,"Jonathan Touryan (data and curation), Greg Apker (data), Brent Lance (data), Scott Kerick (data), Anthony Ries (data), Justin Brooks (data), Kaleb McDowell (data), Tony Johnson (curation), Kay Robbins (curation)",,"Garcia, J.O., Brooks, J., Kerick, S., Johnson, T., Mullen, T.R., Vettel, J.M., 2017. Estimating direction in brain-behavior interactions: Proactive and reactive brain states in driving. NeuroImage 150, 239?249. https://doi.org/10.1016/j.neuroimage.2017.02.057.",This research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-10-0-0002.,"Touryan, J., Apker, G., Lance, B.J., Kerick, S.E., Ries, A.J., McDowell, K., 2014. Estimating endogenous changes in task performance from EEG. Front. Neurosci. 8. https://doi.org/10.3389/fnins.2014.00155. ===NEMAR-SEP=== Brooks, J., Kerick, S., 2015. Event-related alpha perturbations related to the scaling of steering wheel corrections. Physiol. Behav. 149, 287?293. https://doi.org/10.1016/j.physbeh.2015.05.026. ===NEMAR-SEP=== Brooks, J.R., Kerick, S.E., McDowell, K., 2015. Novel measure of driver and vehicle interaction demonstrates transient changes related to alerting. J. Mot. Behav. J. Mot. Behav. 47, 47, 106, 106?116. https://doi.org/10.1080/00222895.2014.959887, 10.1080/00222895.2014.959887. ===NEMAR-SEP=== Garcia, J.O., Brooks, J., Kerick, S., Johnson, T., Mullen, T.R., Vettel, J.M., 2017. Estimating direction in brain-behavior interactions: Proactive and reactive brain states in driving. NeuroImage 150, 239?249. https://doi.org/10.1016/j.neuroimage.2017.02.057. ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis I: Spectral and amplitude characteristics across studies, NeuroImage, p. 116361, https://doi.org/10.1016/j.neuroimage.2019.116361. ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis II: Cognitive aspects of event related features, NeuroImage, p. 116054, https://doi.org/10.1016/j.neuroimage.2019.116054.",doi:10.18112/openneuro.ds004122.v1.0.0,,Drive,8.0.0,EEG,"## BCIT Speed Control
 

 ### Introduction
 

 **Overview:** The Speed Control study was designed to collect extended time-on-task measurements of subjects
 performing a driving task in a simulated environment in order to assess fatigue-based performance
 through novel biomarkers. Similar to the Baseline Driving study, the Speed Control study was intended
 to identify periods of driver fatigue via predictive algorithms formulated from the analysis of driver
 EEG data, in comparison to the objective performance measures, and in contrast with the (non-fatigued)
 Calibration driving session for the subject. Speed Control extended the paradigm by modulating driver
 control of the throttle and increasing the frequency and magnitude of perturbation events vs. Baseline Driving.
 Further information is available on request from [cancta.net](https://cancta.net).
 

 ### Methods 
 

 **Subjects:** Volunteers from the local community recruited through advertisements.  
  
 **Apparatus:** Driving simulator with steering wheel and brake / foot pedals (Real Time Technologies; Dearborn, MI);
 Video Refresh Rate (VRR) = 900 Hz; Vehicle data log file Sampling Rate (SR) = 100 Hz);
 EEG (BioSemi 64 (+8) channel systems with 4 eye and 2 mastoid channels recorded; SR=2048 Hz);
 Eye Tracking (Sensomotoric Instruments (SMI); REDEYE250). Eye tracking data is not included with this dataset.
 

 **Initial setup:** Upon arrival to the lab, subjects were given an introduction to the primary study
 for which they were recruited and provided informed consent and provided demographics information.
 This was followed by a practice session, to acclimate the subject to the driving simulator.
 The driving practice task lasted 10-15 min, until asymptotic performance in steering and speed
 control was demonstrated and lack of motion sickness was reported. Subjects were then outfitted
 and prepped for eye tracking and EEG acquisition.
 

 **Task organization:** Subjects performed the Speed Control condition A and Speed Control condition B,
 with counter-balancing used across subjects as to which of them came first.
 

 Speed Control A was 45 minutes of continuous driving, with subjects responsible for steering control,
 with the simulator maintaining a constant speed automatically.
 

 Speed Control B was similar, but the subject was responsible for both steering and maintaining speed.
 Both driving tasks were conducted on the same simulated long, straight road.
 In each case, the subject was instructed to stay within the boundaries of the right-most lane,
 and to drive at the posted speed limits. The vehicle was periodically subject to lateral perturbing forces,
 which could be applied to either side of the vehicle, pushing the vehicle out of the center of the lane;
 and the subject was instructed to execute corrective steering actions to return the vehicle to the center of the lane.
 

 **Independent variables:** Speed Control (cruise vs. manual).
 

 **Dependent variables:** Reaction times to perturbations, continuous performance based on vehicle log
 (steering wheel angle, lane position, heading error, etc.), Task-Induced Fatigue Scale (TIFS),
 Karolinska Sleepiness Scale (KSS), Visual Analog Scale of Fatigue (VAS-F).
 

 **Note:** questionnaire data is available upon request from [cancta.net](https://cancta.net).
 

 **Additional data acquired:** Participant Enrollment Questionnaire, Subject Questionnaire for Current Session,
 Simulator Sickness Questionnaire.
 

 **Experimental Locations:** Teledyne Corporation, Durham, NC.
 

 **Note:** This dataset has a corresponding dataset in the BCIT Calibration Driving ds004118 which has the
 15 minute driving task performed prior to this one.",1,1,1
ds004123,2022-05-03 14:10:02,Kay Robbins,1.0.0,BCIT Traffic Complexity,2022-05-04 22:56:21,0,1,18814100000,0.019 TB,273,29,0,0,1.7.0,CC0,"Jonathan Touryan (data and curation), Greg Apker (data), Brent Lance (data), Scott Kerick (data), Anthony Ries (data), Justin Brooks (data), Kaleb McDowell (data), Tony Johnson (curation), Kay Robbins (curation)",,"Garcia, J.O., Brooks, J., Kerick, S., Johnson, T., Mullen, T.R., Vettel, J.M., 2017. Estimating direction in brain-behavior interactions: Proactive and reactive brain states in driving. NeuroImage 150, 239?249. https://doi.org/10.1016/j.neuroimage.2017.02.057.",This research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-10-0-0002.,"Touryan, J., Apker, G., Lance, B.J., Kerick, S.E., Ries, A.J., McDowell, K., 2014. Estimating endogenous changes in task performance from EEG. Front. Neurosci. 8. https://doi.org/10.3389/fnins.2014.00155. ===NEMAR-SEP=== Brooks, J., Kerick, S., 2015. Event-related alpha perturbations related to the scaling of steering wheel corrections. Physiol. Behav. 149, 287?293. https://doi.org/10.1016/j.physbeh.2015.05.026. ===NEMAR-SEP=== Brooks, J.R., Kerick, S.E., McDowell, K., 2015. Novel measure of driver and vehicle interaction demonstrates transient changes related to alerting. J. Mot. Behav. J. Mot. Behav. 47, 47, 106, 106?116. https://doi.org/10.1080/00222895.2014.959887, 10.1080/00222895.2014.959887. ===NEMAR-SEP=== Garcia, J.O., Brooks, J., Kerick, S., Johnson, T., Mullen, T.R., Vettel, J.M., 2017. Estimating direction in brain-behavior interactions: Proactive and reactive brain states in driving. NeuroImage 150, 239?249. https://doi.org/10.1016/j.neuroimage.2017.02.057. ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis I: Spectral and amplitude characteristics across studies, NeuroImage, p. 116361, https://doi.org/10.1016/j.neuroimage.2019.116361. ===NEMAR-SEP=== Bigdely-Shamlo, N., Touryan, J., Ojeda, A., Kothe, C., Mullen, T., Robbins, K., 2019. Automated EEG mega-analysis II: Cognitive aspects of event related features, NeuroImage, p. 116054, https://doi.org/10.1016/j.neuroimage.2019.116054.",doi:10.18112/openneuro.ds004123.v1.0.0,,DriveWithComplexity,8.0.0,EEG,"## BCIT Traffic Complexity
 

 ### Introduction
 

 **Overview:** The Traffic Complexity study was designed to collect extended time-on-task measurements of
 subjects performing a driving task in a simulated environment in order to assess fatigue-based performance
 through novel biomarkers. Similar to the Baseline Driving study, the Speed Control study was intended to
 identify periods of driver fatigue via predictive algorithms formulated from the analysis of driver EEG data,
 in comparison to the objective performance measures, and in contrast with the (non-fatigued)
 Calibration driving session for the subject. Traffic Complexity extended the paradigm by modulating
 the visual complexity and the frequency of perturbation events vs. Baseline Driving.
 

 Further information is available on request from [cancta.net](https://cancta.net).
 

 ### Methods 
 

 **Subjects:** Volunteers from the local community recruited through advertisements.  
  
 **Apparatus:** Driving simulator with steering wheel and brake / foot pedals (Real Time Technologies; Dearborn, MI);
 Video Refresh Rate (VRR) = 900 Hz; Vehicle data log file Sampling Rate (SR) = 100 Hz);
 EEG (BioSemi 64 (+8) channel systems with 4 eye and 2 mastoid channels recorded; SR=2048 Hz);
 Eye Tracking (Sensomotoric Instruments (SMI); REDEYE250).
 

 **Initial setup:** Upon arrival to the lab, subjects were given an introduction to the primary study
 for which they were recruited and provided informed consent and provided demographics information.
 This was followed by a practice session, to acclimate the subject to the driving simulator.
 The driving practice task lasted 10-15 min, until asymptotic performance in steering and speed control
 was demonstrated and lack of motion sickness was reported.
 Subjects were then outfitted and prepped for eye tracking and EEG acquisition.
 

 **Task organization:** Subjects would perform the Baseline Driving task and the Traffic Complexity task,
 with counter-balancing used across subjects as to which of them came first.
 The Baseline Driving run was 45 minutes of continuous driving, with subjects responsible
 for speed and steering control. Both driving tasks were conducted on the same simulated long,
 straight road. The Baseline run was done in a visually sparse environment, and the Traffic Complexity
 runs included pedestrians and other traffic. In each case, the subject was instructed to stay
 within the boundaries of the right-most lane, and to drive at the posted speed limits.
 

 The vehicle was periodically subject to lateral perturbing forces, which could be applied to either
 side of the vehicle, pushing the vehicle out of the center of the lane; and the subject was instructed
 to execute corrective steering actions to return the vehicle to the center of the lane.
 

 **Independent variables:** Visual Complexity (high vs. low), Perturbation Frequency (high vs. low).
 

 **Dependent variables:** Reaction times to perturbations, continuous performance based on vehicle
 log (steering wheel angle, lane position, heading error, etc.), Task-Induced Fatigue Scale (TIFS),
 Karolinska Sleepiness Scale (KSS), Visual Analog Scale of Fatigue (VAS-F).
 

 **Note:** Questionnaire data is available upon request from [cancta.net](https://cancta.net).
 

 **Additional data acquired:** Participant Enrollment Questionnaire, Subject Questionnaire for Current Session,
 Simulator Sickness Questionnaire.
 

 **Experimental Locations:** Teledyne Corporation, Durham, NC.
 

 **Note 1:** This dataset has a corresponding dataset in the BCIT Calibration Driving ds004118 which has the
 15 minute driving task performed prior to this one.
 

 **Note 2:** This dataset has a corresponding dataset in the BCIT Baseline Driving ds004120 which was a 
 longer driving task in a sparse environment.",1,1,1
ds004127,2022-05-10 20:43:29,Amada Abrego Mancilla,3.0.0,Somatosensory Cortex Rat DISC Data,2022-08-22 18:07:57,0,1,201353000000,0.201 TB,222,8,0,0,1.1.1,CC0,"Amada Abrego, Wasif Khan, John P Seymour",,,,,doi:10.18112/openneuro.ds004127.v3.0.0,,,,iEEG,"Project Title: DISC Validation in Rat Somatosensory Cortex
 Project ID: 000
 

 Expected experimentation period:
 Start date: N/A
 End date: N/A
 

 Project Description:
 DISC were implanted in rat somatosensory cortex. While anesthetized, whiskers were being stimulated using an air puffer. Task name corresponds to the id of the whisker being stimulated. 
 For more information of the task go to our biorxiv's paper: https://www.biorxiv.org/content/10.1101/2021.09.20.460996v3 
 

 Participant categories:
 N/A
 

 Trigger channels:
 N/A
 

 Events:
 N/A",1,1,0
ds004147,2022-06-07 14:19:20,Cameron Hassall,1.0.2,Average Task Value,2022-06-07 15:05:27,1,1,4291180000,4 GB,113,12,22,77,1.2.1,CC0,"Cameron D. Hassall, Laurence T. Hunt, Clay B. Holroyd",,,,https://doi.org/10.1101/2021.09.16.460600,doi:10.18112/openneuro.ds004147.v1.0.2,,casinos,,EEG,"### Average Task Value
 

 Twelve participants completed three learning tasks. In each task the goal was to learn cue-response mappings for six cues. The cues were various coloured shapes. The possible responses were left ('d' key) or right ('k' key). There were two types of cues. Low-value cues had a feedback validity of 0.5 (i.e., a coin toss). High-value cues had a feedback validity of 0.8 (80% chance of a win if the correct action was chosen). The low-value task contained only low-value cues. The high-value task contained only high-value cues. The mid-value task contained three low-value cues and three high-value cues. Participants completed 144 trials of each task.
 

 Preprint: https://doi.org/10.1101/2021.09.16.460600
 

 Analysis code: https://github.com/chassall/averagetaskvalue",1,0,0
ds004148,2022-06-08 8:39:15,Yulin Wang,1.0.1,A test-retest resting and cognitive state EEG dataset,2022-06-13 8:35:00,1,3,32936800000,0.033 TB,5585,60,18,28,1.2,CC0,"Yulin Wang, Wei Duan, Debo Dong, Lihong Ding, Xu Lei",This dataset has received funding supproted from the National Natural Science Foundation of China (grant number 31971028) and the National Key Research and Development Program of China (2021YFC2501500).,,,,doi:10.18112/openneuro.ds004148.v1.0.1,,"eyes closed, mathematic, eyes open, memory, music",,EEG,"# General information
 

 This dataset contains resting(eyes closed, eyes open) and cognitive(subtraction, music, memory) state EEG recordings with 60 participants 
 during three experimental sessions together with sleep, emotion, mental health, and mind-wandering related measures 
 

 # Dataset 
 

 ##  Presentation
 

  The data collection was initiated in September 2019 and was terminated in April 2021. The detailed description of the dataset is currently under working 
  by Yulin Wang, and will submit to Scientific Data for publication.
  
  
 

 #### EEG acquisition
 

  * EEG system (Brain Products GmbH, Steing- rabenstr, Germany, 64 electrodes) 
  * Sampling frequency: 500Hz
  * Impedances were kept below 5k
 

 

 ## Contact
  * If you have any questions or comments, please contact: 
  * Xu Lei: xlei@swu.edu.cn
  * Yulin Wang: yulin.wang90.swu@gmail.com",1,1,0
ds004151,2022-06-13 16:57:01,Graciela Catalina Alatorre Cruz,1.0.0,"Effect of obesity on inhibitory control in preadolescents during stop-signal 
 task. An event-related potentials study",2022-06-14 18:40:59,0,1,24845600000,0.025 TB,408,57,0,0,1,CC0,"Graciela C. Alatorre-Cruz, Heather Downs, Darcy Hagood, Seth T. Sorensen, D. Keith Williams, Linda Larson-Prior",,,This research was supported by USDA/Agricultural Research Service Project 6026-51000-012-06S,doi: 10.1016/j.ijpsycho.2021.04.003.,doi:10.18112/openneuro.ds004151.v1.0.0,,Stop-signal task,,EEG,"Introduction
 

 This EEG dataset contains the electrophysiological signal from fifty-seven obese and non-obese preteens 
 during a stop-signal task. The stimuli were designed and administered using E-Prime 
 software (Version 2) at Arkansas Children Nutrition Center (ACNC), Little Rock, Arkansas.
 The University of Arkansas for Medical Sciences (UAMS) approved the study protocol. This research
 was supported by USDA/Agricultural Research Service Project 6026-51000-012-06S.
 

 Raw data files
 

 The data were acquired with a Geodesic Net Amps 300 system running Netstation 4.5.2 software using 
 the 128-channel Geodesic Hydrocell Sensor Net™ (Magstim EGI., Eugene OR, USA).
 No operations have been performed on the data.
 

 Participant data
 

 The *Participants.tsv* file contains age, gender, body mass index (BMI), and weight status (WS)
 

 How to cite
 

 All use of this dataset in a publication context requires the following paper to be cited: Alatorre-Cruz GC, Downs H, Hagood D, Sorensen ST, Williams DK, Larson-Prior L. Effect of obesity on inhibitory control in preadolescents during stop-signal task. An event-related potentials study. Int J Psychophysiol. 2021 Jul;165:56-67. doi: 10.1016/j.ijpsycho.2021.04.003
 

 Contact
 Questions regarding the EEG data may be addressed to
 Catalina Alatorre-Cruz (gcalatorrecruz@uams.edu).
 

 Question regarding the project, in general, may be addressed to
 Linda Larson-Prior (ljlarsonprior@uams.edu).",1,1,0
ds004152,2022-06-14 13:44:05,Cameron Hassall,1.1.2,Drum Trainer,2022-06-14 15:45:13,1,1,5118530000,0.005 TB,194,21,21,41,1.2.1,CC0,"Cameron D. Hassall, Yan Yan, Laurence T. Hunt",,,,,doi:10.18112/openneuro.ds004152.v1.1.2,,drumtrainer,,EEG,"# Drum Trainer
 

 Twenty-one participants learned to play two drumming patterns (pattern 1: AABA, pattern 2: AAABAA) at three different tempos (fast: 150 bpm, medium: 100 bpm, slow: 60 bpm). Responses were recorded using the f and j keys on a standard keyboard. Visual feedback in the form of a coloured circle coincided with each button press and indicated whether the response was early, on time, or late. Feedback was determined by comparing the response time, relative to the previous response, to a window around the target duration. The window was adapted trial-by-trial to ensure that roughly half the outcomes were ""on time"".
 

 Participant 12 should be excluded from event-locked analyses due to bad triggers (trigger cable was partially disconnected). 
 

 Timing  
 Repeat for 72 trials: fixation dot (until response) -> feedback circle (50 ms)
 

 Condition Codes  
 1: ""Fast, pattern 1, left-hand start""  
 2: ""Fast, pattern 1, right-hand start""  
 3: ""Fast, pattern 2, left-hand start""  
 4: ""Fast, pattern 2, right-hand start""  
 5: ""Medium, pattern 1, left-hand start""  
 6: ""Medium, pattern 1, right-hand start""  
 7: ""Medium, pattern 2, left-hand start""  
 8: ""Medium, pattern 2, right-hand start""  
 9: ""Slow, pattern 1, left-hand start""  
 10: ""Slow, pattern 1, right-hand start""  
 11: ""Slow, pattern 2, left-hand start""  
 12: ""Slow, pattern 2, right-hand start""  
 

 Trigger Modifiers  
 Add 0: Metronome beat (pre-block)  
 Add 20: Ready screen (pre-block)  
 Add 40: First response (can't be early, on time, or late)  
 Add 60: Early response  
 Add 80: On time response  
 Add 100: Late response  
 Add 120: Red X (wrong key pressed)",1,1,0
ds004166,2022-06-17 1:05:58,Jun Li,1.0.0,"Effects of Forward and Backward Span Trainings on Working Memory: Evidence from a Randomized, Controlled Trial",2022-06-17 10:33:00,1,3,83074400000,0.083 TB,431,71,18,27,1.7.0,CC0,"Yang Li (data and curation), Wenjin Fu (data), Qiumei Zhang (data), Xiongying Chen (data), Xiaohong Li (data), Boqi Du (data), Xiaoxiang Deng (data), Feng Ji (curation), Qi Dong (curation), Feng Ji (curation), Susanne M. Jaeggi (curation), Chuansheng Chen (curation), Jun Li (data and curation)",,,This work was supported by grants from the National Natural Science Foundation of China (31771242). The authors declared that they have no conflict of interest.,,doi:10.18112/openneuro.ds004166.v1.0.0,,,8.0.0,EEG,"## Effects of Forward and Backward Span Trainings on Working Memory: Evidence from a Randomized, Controlled Trial
 

 ### Introduction
 

 **Overview:** Both forward and backward working memory span tasks have been used in cognitive training, but no study has
  been conducted to test whether the two types of trainings are equally effective. Based on data from a larger randomized
  controlled trial, this study tested the effects of backward span training, forward span training, and no intervention. 
 Event-related potential (ERP) signals were recorded at the pre-, mid-, and post-tests while the subjects were performing 
 a distractor version of the change detection task, which included three conditions (2 targets and 0 distractor [2T0D];
  4 targets and 0 distractor [4T0D]; and 2 targets and 2 distractors [2T2D]). Behavioral data were collected from two additional
  tasks: a multi-object version of the change detection task, and a suppress task. Compared to no intervention, both forward 
 and backward span trainings led to significantly greater improvement in working memory maintenance, based on indices from 
 both behavioral (Kmax) and ERP data (CDA\_2T0D and CDA\_4T0D). Backward span training also improved interference control based 
 on the ERP data (CDA\_filtering efficiency) to a greater extent than did forward span training and no intervention, but the three groups 
 did not differ in terms of behavioral indices of interference control. These results have potential implications for optimizing the current 
 cognitive training on working memory.
 

 ### Methods 
 

 **Subjects:** Volunteers from university recruited through advertisements.  
  
 **Apparatus:** At all three time points (pre-, mid-, and post-tests), we used a 64-channel Synamps RT system (Neuroscan, El Paso, USA) 
 to record the electroencephalogram (EEG) signals. Subjects were required to sit in a comfortable chair inside a darkened, electrically shielded 
 recording chamber during the EEG recording. The electrode impedance was low (below 5k?). The reference electrode was on the left mastoid. 
 Electrodes were set both below and above the right eye to record the vertical electrooculographies (EOGs). Electrodes were set at the outer canthi
  of each eye to record the horizontal EOGs. 
 **EEG dataset:** Backward group (sub-01~sub020); Forward group (sub-101~sub120); Control group (sub-201~sub220); Sudoku group (sub-301~sub320). 
  Pre-test(ses-01); Mid-test(ses-01); Post-test(ses-01);",1,1,0
ds004186,2022-07-01 6:16:39,Arnaud Delorme,2.0.0,HBN EO/EC task,2022-05-15 17:04:27,1,1,231450000000,0.231 TB,177065,2951,5,21,v1.2.1,CC0,"Alexander LM, Escalera J, Ai L, Andreotti C, Febre K, Mangone A, Vega-Potler N, Langer N, Alexander A, Kovacs M, Litke S, O'Hagan B, Andersen J, Bronstein B, Bui A, Bushey M, Butler H, Castagna V, Camacho N, Chan E, Citera D, Clucas J, Cohen S, Dufek S, Eaves M, Fradera B, Gardner J, Grant-Villegas N, Green G, Gregory C, Hart E, Harris S, Horton M, Kahn D, Kabotyanski K, Karmel B, Kelly SP, Kleinman K, Koo B, Kramer E, Lennon E, Lord C, Mantello G, Margolis A, Merikangas KR, Milham J, Minniti G, Neuhaus R, Levine A, Osman Y, Parra LC, Pugh KR, Racanello A, Restrepo A, Saltzman T, Septimus B, Tobe R, Waltz R, Williams A, Yeo A, Castellanos FX, Klein A, Paus T, Leventhal BL, Craddock RC, Koplewicz HS, Milham MP",,,,"Alexander LM, Escalera J, Ai L, Andreotti C, Febre K, Mangone A, Vega-Potler N, Langer N, Alexander A, Kovacs M, Litke S, O'Hagan B, Andersen J, Bronstein B, Bui A, Bushey M, Butler H, Castagna V, Camacho N, Chan E, Citera D, Clucas J, Cohen S, Dufek S, Eaves M, Fradera B, Gardner J, Grant-Villegas N, Green G, Gregory C, Hart E, Harris S, Horton M, Kahn D, Kabotyanski K, Karmel B, Kelly SP, Kleinman K, Koo B, Kramer E, Lennon E, Lord C, Mantello G, Margolis A, Merikangas KR, Milham J, Minniti G, Neuhaus R, Levine A, Osman Y, Parra LC, Pugh KR, Racanello A, Restrepo A, Saltzman T, Septimus B, Tobe R, Waltz R, Williams A, Yeo A, Castellanos FX, Klein A, Paus T, Leventhal BL, Craddock RC, Koplewicz HS, Milham MP. An open resource for transdiagnostic research in pediatric mental health and learning disorders. Sci Data. 2017 Dec 19;4:170181. doi: 10.1038/sdata.2017.181. PMID: 29257126; PMCID: PMC5735921",doi:10.18112/openneuro.ds004186.v2.0.0,,mixed,,EEG,"HBN EO/EC datasets 
 The Healthy Brain Network recorded 2952 children's eyes-open and eyes-closed EEG. Eyes-open lasted for 20 seconds, and eyes closed for 40 seconds. Because such files are difficult to process, they have been segmented here with five runs for eyes open and five runs for eyes closed. See notes in the code folder for included datasets.",1,1,0
ds004194,2022-07-04 12:51:42,Iris Groen,2.0.0,Visual ECoG dataset,2022-07-06 15:43:04,1,7,15311300000,14.3 GB,1566,14,18,47,1.9.2,CC0,"Iris Groen, Kenichi Yuasa, Amber Brands, Giovanni Piantoni, Stephanie Montenegro, Adeen Flinker, Sasha Devore, Orrin Devinsky, Werner Doyle, Patricia Dugan, Daniel Friedman, Nick Ramsey, Natalia Petridou, Jonathan Winawer",Other contributors to this dataset: Dora Hermes,"Please cite Groen et al., (2022) when using the spatial pattern or temporal pattern task data. Please cite Yuasa et al., (2023) when using the prf task data. Please cite Brands et al., (2024) when using the six category localizer task data. (see ReferencesAndLinks)","This work was funded by BRAIN Initiative, the National Institute of Mental Health of the National Institutes of Health under Award Number R01MH111417","Groen IIA, Piantoni G, Montenegro S, Flinker A, Devore S, Devinsky O, Doyle W, Dugan P, Friedman D, Ramsey N, Petridou N, Winawer JA. Temporal dynamics of neural responses in human visual cortex. The Journal of Neuroscience 42(40):7562-7580 https://doi.org/10.1523/JNEUROSCI.1812-21.2022 ===NEMAR-SEP=== Yuasa K, Groen IIA, Piantoni G, Montenegro S, Flinker A, Devore S, Devinsky O, Doyle W, Dugan P, Friedman D, Ramsey N, Petridou N, Winawer JA. Precise spatial tuning of visually driven alpha oscillations in human visual cortex. eLife12:RP90387 https://doi.org/10.7554/eLife.90387.1 ===NEMAR-SEP=== Brands AM, Devore S, Devinsky O, Doyle W, Flinker A, Friedman D, Dugan P, Winawer JA, Groen IIA (2024) Temporal dynamics of short-term neural adaptation in human visual cortex. https://doi.org/10.1101/2023.09.13.557378",doi:10.18112/openneuro.ds004194.v2.0.0,"The study was approved by the NYU Grossman School of Medicine Institutional Review Board and the ethical committee of the University Medical Center Utrecht, in accordance with the Declaration of Helsinki (2013)","prf, spatialpattern, temporalpattern, soc, n/a",,"iEEG, MRI","## Details related to access to the data
 

 - Contact person
 

 Please contact Iris Groen (i.i.a.groen@uva.nl, https://orcid.org/0000-0002-5536-6128) for more information.
 

 Please see the following papers for more details on the data collection and preprocessing: 
 

 Groen IIA, Piantoni G, Montenegro S, Flinker A, Devore S, Devinsky O, Doyle W, Dugan P, Friedman D, Ramsey N, Petridou N, Winawer JA (2022) Temporal dynamics of neural responses in human visual cortex. The Journal of Neuroscience 42(40):7562-7580 (https://doi.org/10.1523/JNEUROSCI.1812-21.2022) 
 

 Yuasa K, Groen IIA, Piantoni G, Montenegro S, Flinker A, Devore S, Devinsky O, Doyle W, Dugan P, Friedman D, Ramsey N, Petridou N, Winawer JA. Precise Spatial Tuning of Visually Driven Alpha Oscillations in Human Visual Cortex. eLife12:RP90387 https://doi.org/10.7554/eLife.90387.1
 

 Brands AM, Devore S, Devinsky O, Doyle W, Flinker A, Friedman D, Dugan P, Winawer JA, Groen IIA (2024). Temporal dynamics of short-term neural adaptation in human visual cortex. https://doi.org/10.1101/2023.09.13.557378
 

 - Practical information to access the data
 

 Processed data and model fits reported in Groen et al., (2022) are available in derivatives/Groenetal2022TemporalDynamicsECoG as matlab .mat files. Matlab code to load, process and plot these data (including 3D renderings of the participant's surface reconstructions and electrode positions) is available in https://github.com/WinawerLab/ECoG_utils and https://github.com/irisgroen/temporalECoG. These repositories have dependencies on other Matlab toolboxes (e.g., FieldTrip). See instructions on Github for relevant links and guidelines. 
 

 Processed data and model fits reported in Yuasa et al., (2023) are available in the Github repositories described in the paper.
 

 Processed data and model fits reported in Brands et al., (2024) are available in derivatives/Brandsetal2024TemporalAdaptationECoGCategories as python .py files. Python code to process and analyze these data is available in the Github repositories described in the paper.
 

 ## Overview
 

 - Project name
 

 Visual ECoG dataset
 

 - Years that the project ran
 

 Data were collected between 2017-2020. Exact recording dates have been scrubbed for anonymization purposes.
 

 - Brief overview of the tasks in the experiment
 

 Participants sub-p01 to sub-p11 viewed grayscale visual pattern stimuli that were varied in temporal or spatial properties. Participans sub-p11 to sub-p14 additionally saw color images of different image classes (faces, bodies, buildings, objects, scenes, and scrambled) that were varied in temporal properties. See 'Independent Variables' below for more details.
 

 In all tasks, participants were instructed to fixate a cross or point in the center of the screen and monitor it for a color change, i.e. to perform a stimulus-orthogonal task (see the task-specific _events.json files, e.g., task-prf_events.json, for further details).
 

 - Description of the contents of the dataset
 

 The data consists of cortical iEEG recordings in 14 epilepsy patients in response to visual stimulation. Patients were implanted with standard clinical surface (grid) and depth electrodes. Two patients were additionally implanted with a high-density research grid. In addition to the ieeg recordings, pre-implantation MRI T1 scans are provided for the purpose of localizing electrodes. Participants performed a varying number of tasks and runs. 
 

 - Independent variables
 

 The data are divided in 6 different sets of stimulus types or events:
 

 1. prf: grayscale, oriented bar stimuli consisting of curved, band-pass filtered lines that were swept across the screen (up to (~16 degree of visual angle) in a fixed order for the purpose of estimating spatial population receptive fields (pRFs).
 2. spatialpattern: grayscale, centrally presented pattern stimuli (~16 degree of visual angle diameter) consisting of curved, band-pass filtered lines that were systematically varied in level of contrast and density, as well as various oriented grating stimuli.
 3. temporalpattern: grayscale, centrally presented pattern stimuli (~16 degree of visual angle diameter) consisting of curved, band-pass filtered lines that were systematically varied in temporal duration and interval.
 4. soc: combination of the spatialpattern and temporalpattern stimuli. 
 5. sixcatloctemporal: color images of six stimulus classes: faces, bodies (hands/feet only), buildings, objects, scenes and scrambled, systematically varied in temporal duration and interval, whereby interval stimuli consisted of direct repeats of the identical image. 
 6. sixcatlocisidiff/sixcatlocdiffisi: color images of six stimulus classes: faces, bodies (hands/feet only), buildings, objects, scenes and scrambled, systematically varied in temporal duration and interval, whereby the first interval stimulus was followed by images from either the same or a different category (but not the identical image).  
 

 Participant-, task- and run-specific stimuli are provided in the /stimuli folder as matlab .mat files. 
 

 - Dependent variables
 

 The main BIDS folder contains the raw voltage data, split up in individual task runs. 
 The /derivatives/ECoGCAR folder contains common-average-referenced version of the data. 
 The /derivatives/ECoGBroadband folder contains time-varying broadband responses estimated by band-pass filtering the common-average-referenced voltage data and taking the average power envelope. 
 The /derivatives/ECoGPreprocessed folder contains epoched trials used in Brands et al., (2024).
 The /derivatives/freesurfer folder contains surface reconstructions of each participant's T1, along with retinotopic atlas files. 
 The /derivatives/Groen2022TemporalDynamicsECoG contains preprocessed data and model fits that can be used to reproduce the results reported in Groen et al., (2022).
 The /derivatives/Brands2024TemporalAdaptationECoG contains preprocessed data and model fits that can be used to reproduce the results reported in Brands et al., (2024).
 

 

 - Quality assessment of the data
 

 Data quality and number of trials per subjects varies considerably across patients, for various reasons.
 

 First, for each recording session, attempts were made to optimize the environment for running visual experiments; e.g. room illumination was stabilized as much as possible by closing blinds when available, the visual display was calibrated (for most patients), and interference from medical staff or visitors was minimized. However, it was not possible to equate this with great precision across patients and sessions/runs. 
 

 Second, implantations were determined based on clinical needs and electrode locations therefore vary across participants. The strength and robustness of the neural responses varies greatly with the electrode location (e.g. early vs higher-level visual cortex), as well as with uncontrolled factors such as how well the electrode made contact with the cortex and whether it was primarily situated on grey matter (surface/grid electrodes) or could be located in white matter (some depth electrodes). Electrodes that were marked as containing epileptic activity by clinicians, or that did not have good signal based on visual inspection of the raw data, are marked as 'bad' in the channels.tsv files. 
 

 Third, patients varied greatly in their cognitive abilities and mental/medical state, which affected their ability to follow task instructions, e.g. to remain alert and fixation. Some patients were able to perform repeated runs of multiple tasks across multiple sessions, while others only managed to do a few runs. 
 

 All patients included in this dataset have sufficiently good responses in some electrodes/tasks as judged by Groen et al., (2022) and Brands et al., (2024). However, when using this dataset to address further research questions, it is advisable to set stringent requirements on electrode and trial selection. See Groen et al., (2022) and associated code repository for an example preprocessing pipeline that selected for robust visual responses to temporally- and contrast-varying stimuli.
 

 ## Methods
 

 - Subjects
 

 All participants were intractable epilepsy patients who were undergoing ECoG for the purpose of monitoring seizures. Participants were included if their implantation covered parts of visual cortex and if they consented to participate in research.
 

 - Apparatus
 

 Data were collected in a clinical setting, i.e. at bedside in the patient's hospital room. Information about iEEG recording apparatus is provided the meta data for each patient. Information about the visual stimulation equipment and behavioral response recordings are provided in Groen et al., (2022), Yuasa et al., (2023) and Brands et al., (2024). 
 

 - Experimental location
 

 Data were collected at NYU University Langone Hospital (New York, USA) or at University Medical Center Utrecht (The Netherlands).
 

 - Missing data
 

 Stimulus files are missing for a few runs of sub-02. These are marked as N/A in the associated event files.
 

 ## Notes
 

 Further participant-specific notes:
 

 - For sub-03 and sub-04 the spatial pattern and temporal pattern stimuli are combined in the soc task runs, for the remaining participants these are split across the spatialpattern and temporalpattern task runs.
 

 - The pRF task from sub-04 has different prf parameters (bar duration and gap). 
 

 - The first two runs of the pRF task from sub-05 are not of good quality (participant repeatedly broke fixation). In addition, the triggers in all pRF runs from sub-05 are not correct due to a stimulus coding problem and will need to be re-interpolated if one wishes to use these data.
 

 - Participants sub-10 and sub-11 have high density grids in addition to clinical grids.
 

 - Note that all stimuli and stimulus parameters can be found in the participant-specific stimulus *.mat files.",1,0,0
ds004196,2022-07-07 13:02:56,Foteini Simistira Liwicki,2.0.2,FMRI-EEG Prestudy,2023-04-26 11:21:50,0,3,10022900000,0.01 TB,109,4,0,0,1.4,CC0,"Foteini Liwicki, Vibha Gupta, Rajkumar Saini, Kanjar De, Nosheen Abid, Sumit Rakesh, Scott Wellington, Holly Wilson, Marcus Liwicki, Johan Eriksson","We would like to thank the Stockholm University Brain Imaging Centre (SUBIC) and more precisely Rita Almeida, Patrik Andersson for giving us access to their facilities and for supporting us in this endeavor.
 Petter Kallioinen and Christoffer Schiehe-Forbes for their valuable support during the EEG data acquisition.
 Finally, we would also like to thank all participants for taking part in this study.","Foteini Simistira Liwicki, Vibha Gupta, Rajkumar Saini, Kanjar De, Nosheen Abid, Sumit Rakesh, Scott Wellington, Holly Wilson, Marcus Liwicki, Johan Eriksson (2022). Bimodal dataset on inner speech. OpenNeuro. [Dataset] doi:10.18112/openneuro.ds004196.v1.0.29",This research was funded by the Grants for excellent research projects proposals of SRT.ai 2022.,https://github.com/LTU-Machine-Learning/Inner\_Speech\_EEG\_FMRI ===NEMAR-SEP=== https://www.biorxiv.org/content/10.1101/2022.05.24.492109v3,doi:10.18112/openneuro.ds004196.v2.0.2,"Etikprövning myndigheden, ID:2021-06710-01","InnerSpeech, Inner Speech",,"MRI, EEG","Bimodal dataset on Inner Speech
 

 Code available: https://github.com/LTU-Machine-Learning/Inner\_Speech\_EEG\_FMRI 
 

 Publication available: https://www.nature.com/articles/s41597-023-02286-w
 

 Abstract:
 The recognition of inner speech, which could give a `voice' to patients that have no ability to speak or move, is a challenge for brain-computer interfaces (BCIs). A shortcoming of the available datasets is that they do not combine modalities to increase the performance of inner speech recognition. Multimodal datasets of brain data enable the fusion of neuroimaging modalities with complimentary properties, such as the high spatial resolution of functional magnetic resonance imaging (fMRI) and the temporal resolution of electroencephalography (EEG), and therefore are promising for decoding inner speech. This paper presents the first publicly available bimodal dataset containing EEG and fMRI data acquired nonsimultaneously during inner-speech production. Data were obtained from four healthy, right-handed participants during an inner-speech task with words in either a social or numerical category. Each of the 8-word stimuli were assessed with 40 trials, resulting in 320 trials in each modality for each participant. 
 The aim of this work is to provide a publicly available bimodal dataset on inner speech, contributing towards speech prostheses.
 

 Short Dataset description:
 The dataset consists of 1280 trials in each modality (EEG, FMRI). 
 The stimuli contain 8 words, selected from 2 different categories (social, numeric):
 Social: child, daughter, father, wife
 Numeric: four, three, ten, six
 

 There are 4 subjects in total: sub-01, sub-02, sub-03, sub-05. Initially, there were 5 participants, however, sub-04 data was rejected due to high fluctuations. Details of valid data are available in the file participants.tsv. 
 

 For questions please contact: foteini.liwicki@ltu.se",1,1,0
ds004200,2022-07-11 11:15:38,Cameron Hassall,1.0.1,Temporal Scaling,2022-07-11 11:47:42,1,1,7740010000,0.008 TB,166,20,20,77,1.1.1,CC0,"Cameron D. Hassall, Jack Harley, Nils Kolling, Laurence T. Hunt",,,,,doi:10.18112/openneuro.ds004200.v1.0.1,,temporalscaling,,EEG,"# Temporal Scaling
 

 Twenty participants learned three temporal intervals. There were two subtasks, randomly interleaved. In the Production task, participants produced either a short, medium, or long temporal interval. In the Perception task, participants judged a computer-produced interval as correct or incorrect (again, for a short, medium, or long temporal interval). In both tasks participants received visual feedback (a checkmark or x).
 

 Preprint: https://doi.org/10.1101/2020.12.11.421180",1,1,0
ds004212,2022-07-14 15:47:41,Oliver Contier,2.0.0,THINGS-MEG,2023-01-05 14:14:13,0,13,255222000000,237.7 GB,15295,4,0,0,1.21,CC0,"Martin N. Hebart, Oliver Contier, Lina Teichmann, Adam H. Rockter, Charles Zheng, Alexis Kidder, Anna Corriveau, Maryam Vaziri-Pashkam, Chris I. Baker",,,,,doi:10.18112/openneuro.ds004212.v2.0.0,NIH Institutional Review Board as part of the study protocol 93-M-0170 (NCT00001360),main,,"MEG, MRI","# THINGS-MEG
 

 Understanding object representations visual and semantic processing of objects requires a broad, comprehensive sampling of the objects in our visual world
 with dense measurements of brain activity and behavior. This densely sampled fMRI dataset is part of THINGS-data, a multimodal collection
 of large-scale datasets comprising functional MRI, magnetoencephalographic recordings, and 4.70 million behavioral judgments in response to thousands of photographic images
 for up to 1,854 object concepts. THINGS-data is unique in its breadth of richly-annotated objects, allowing for testing countless novel hypotheses at scale while assessing
 the reproducibility of previous findings. The multimodal data allows for studying both the temporal and spatial dynamics of object representations and their relationship
 to behavior and additionally provides the means for combining these datasets for novel insights into object processing. THINGS-data constitutes the core release of
 the [THINGS initiative](https://things-initiative.org) for bridging the gap between disciplines and the advancement of cognitive neuroscience.
 

 # Dataset overview
 

 We collected extensively sampled object representations using magnetoencephalography (MEG). To this end, we drew on the THINGS database [(Hebart et al., 2019)](https://doi.org/10.1371/journal.pone.0223792),
 a richly-annotated database of 1,854 object concepts representative of the American English language which contains 26,107 manually-curated naturalistic object images.
 

 During the fMRI experiment, participants were shown a representative subset of THINGS images, spread across 12 separate sessions (N=4, 22,448 unique images of 1,854 objects).
 Images were shown in fast succession (1.5±0.2s), and participants were instructed to maintain central fixation. To ensure engagement, participants performed an oddball detection task
 responding to occasional artificially-generated images. A subset of images (n=200) were shown repeatedly in each session.
 

 Beyond the core functional imaging data in response to THINGS images, we acquired T1-weighted MRI scans to allow for cortical source localization.
 Eye movements were monitored in the MEG to ensure participants maintained central fixation.",1,0,0
ds002885,2020-06-05 12:06:29,Ahmet Levent Kandemir,1.0.1,DBS Phantom Recordings,2020-06-24 7:31:04,1,1,21543200000,20.1 GB,115,2,0,0,1.2,CC0,"Ahmet Levent Kandemir, Vladimir Litvak, Esther Florin","EF gratefully acknowledges support by the Volkswagen Foundation (Lichtenberg program 89387). The Wellcome Centre for Human Neuroimaging is supported by core funding from the Wellcome (203147/Z/16/Z). We thank David Bradbury, Peter Aston, Alphonso Reid and Daniel Bates for technical assistance with the phantom recording. The authors would like to thank Johannes Pfeifer, Jan Hirschmann, and Markus Butz for their critical review of the manuscript and fruitful discussions.","Please acknowledge the authors and cite the following reference: Ahmet Levent Kandemir, Vladimir Litvak, Esther Florin, The comparative performance of DBS artefact rejection methods for MEG recordings, NeuroImage, 2020, 117057, ISSN 1053-8119, https://doi.org/10.1016/j.neuroimage.2020.117057.",Volkswagen Foundation (Lichtenberg program 89387),"Kandemir, A.L., Litvak, V., Florin, E., 2020. The comparative performance of DBS artefact rejection methods for MEG recordings, NeuroImage, 2020, https://doi.org/10.1016/j.neuroimage.2020.117057.",10.18112/openneuro.ds002885.v1.0.1,,"DSMW, EmptyRoom, Reference, DMW",,"coordsystem, channels, meg, 14, 13, 02, 01","This dataset is a part of the data used for the study: 'Kandemir, A.L., Litvak, V., Florin, E., 2020. The comparative performance of DBS artefact rejection methods for MEG recordings, NeuroImage, 2020, https://doi.org/10.1016/j.neuroimage.2020.117057.'
 

 

 Please use the latest version of the dataset.
  
 

 For detailed information about measurement protocol please refer to https://doi.org/10.1016/j.neuroimage.2020.117057. Additional information about CTF Phantom measurement is provided below. 
 The customized Matlab code for artefact rejection methods is available at: https://gitlab.com/lkandemir/dbs-artefact-rejection.
 

 --------------
 CTF Phantom Measurement 
 Stimulation reference signal is captured with EEG001
 Movement trigger is captured with UPPT001
 Dipole activity is captured with HADC006",1,0,0
ds000246,2018-03-30 10:34:05,Julia Guiomar Niso Galán,1.0.1,MEG-BIDS Brainstorm data sample,2018-03-30 10:34:05,1,1,2457670000,2.3 GB,54,1,25,0,1.4.1,CC0,"Elizabeth Bock, Peter Donhauser, Francois Tadel, Guiomar Niso, Sylvain Baillet",,"If you reference this dataset in your publications, please aknowledge its authors and cite Brainstorm as indicated on the website (http://neuroimage.usc.edu/brainstorm/CiteBrainstorm). 
 

 Also, include the following message: ' This data was obtained from the OpenNeuro database. Its accession number is ds000246'",NIH (2R01EB009048-05),https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3090754 ===NEMAR-SEP=== https://doi.org/10.3389/fnins.2019.00076 ===NEMAR-SEP=== http://neuroimage.usc.edu/brainstorm/ ===NEMAR-SEP=== https://openfmri.org/dataset/ds000246/,doi:10.18112/openneuro.ds000246.v1.0.1,,"AEF, noise",,"MEG, MRI","# Brainstorm - Auditory Dataset
 

 ## License
 

 This dataset (MEG and MRI data) was collected by the MEG Unit Lab, McConnell Brain Imaging Center, Montreal Neurological Institute, McGill University, Canada. The original purpose was to serve as a tutorial data example for the Brainstorm software project (http://neuroimage.usc.edu/brainstorm). It is presently released in the Public Domain, and is not subject to copyright in any jurisdiction.
 

 We would appreciate though that you reference this dataset in your publications: please acknowledge its authors (Elizabeth Bock, Peter Donhauser, Francois Tadel and Sylvain Baillet) and cite the Brainstorm project seminal publication (also in open access): 
 http://www.hindawi.com/journals/cin/2011/879716/
  
 

 

 ## Presentation of the experiment
 

 #### Experiment
 

 * One subject, two acquisition runs of 6 minutes each
 * Subject stimulated binaurally with intra-aural earphones (air tubes+transducers)
 * Each run contains:
  * 200 regular beeps (440Hz)
  * 40 easy deviant beeps (554.4Hz, 4 semitones higher)
 * Random inter-stimulus interval: between 0.7s and 1.7s seconds, uniformly distributed
 * The subject presses a button when detecting a deviant with the right index finger
 * Auditory stimuli generated with the Matlab Psychophysics toolbox
 * The specifications of this dataset were discussed initially on [the FieldTrip bug tracker][2]
 

 #### MEG acquisition
 

 * Acquisition at **2400Hz**, with a **CTF 275** system, subject in seating position
 * Recorded at the Montreal Neurological Institute in December 2013
 * Anti-aliasing low-pass filter at 600Hz, files saved with the 3rd order gradient
 * Recorded channels (340):
  * 1 Stim channel indicating the presentation times of the audio stimuli: UPPT001 (#1)
  * 1 Audio signal sent to the subject: UADC001 (#316)
  * 1 Response channel recordings the finger taps in response to the deviants: UDIO001 (#2)
  * 26 MEG reference sensors (#5-#30)
  * 274 MEG axial gradiometers (#31-#304)
  * 2 EEG electrodes: Cz, Pz (#305 and #306)
  * 1 ECG bipolar (#307)
  * 2 EOG bipolar (vertical #308, horizontal #309)
  * 12 Head tracking channels: Nasion XYZ, Left XYZ, Right XYZ, Error N/L/R (#317-#328)
  * 20 Unused channels (#3, #4, #310-#315, #329-340)
 * 3 datasets:
  * **S01_AEF_20131218_01.ds**: Run #1, 360s, 200 standard + 40 deviants
 

  * **S01_AEF_20131218_02.ds**: Run #2, 360s, 200 standard + 40 deviants
 

  * **S01_Noise_20131218_01.ds**: Empty room recordings, 30s long
 

  * File name: S01=Subject01, AEF=Auditory evoked field, 20131218=date(Dec 18 2013), 01=run
 * Use of the .ds, not the AUX (standard at the MNI) because they are easier to manipulate in FieldTrip
 

 #### Stimulation delays
 

 * **Delay #1**: Production of the sound. 
 Between the stim markers (channel UDIO001) and the moment where the sound card plays the sound (channel UADC001). This is mostly due to the software running on the computer (stimulation software, operating system, sound card drivers, sound card electronics). The delay can be measured from the recorded files by comparing the triggers in the two channels: Delay **between 11.5ms and 12.8ms** (std = 0.3ms) This delay is **not constant**, we will need to correct for it.
 * **Delay #2**: Transmission of the sound. 
 Between when the sound card plays the sound and when the subject receives the sound in the ears. This is the time it takes for the transducer to convert the analog audio signal into a sound, plus the time it takes to the sound to travel through the air tubes from the transducer to the subject's ears. This delay cannot be estimated from the recorded signals: before the acquisition, we placed a sound meter at the extremity of the tubes to record when the sound is delivered. Delay **between 4.8ms and 5.0ms** (std = 0.08ms). At a sampling rate of 2400Hz, this delay can be considered **constant**, we will not compensate for it.
 * **Delay #3**: Recording of the signals. 
 The CTF MEG systems have a constant delay of **4 samples** between the MEG/EEG channels and the analog channels (such as the audio signal UADC001), because of an anti-aliasing filtered that is applied to the first and not the second. This translate here to a **constant delay** of **1.7ms**.
 * **Delay #4**: Over-compensation of delay #1. 
 When correcting of delay #1, the process we use to detect the beginning of the triggers on the audio signal (UADC001) sets the trigger in the middle of the ramp between silence and the beep. We ""over-compensate"" the delay #1 by 1.7ms. This can be considered as **constant delay** of about **-1.7ms**.
 * **Uncorrected delays**: We will correct for the delay #1, and keep the other delays (#2, #3 and #4). After we compensate for delay #1 our MEG signals will have a **constant delay** of about 4.9 + 1.7 - 1.7 = **4.9 ms**. We decide not to compensate for th3se delays because they do not introduce any jitter in the responses and they are not going to change anything in the interpretation of the data.
 

 #### Head shape and fiducial points
 

 * 3D digitization using a Polhemus Fastrak device driven by Brainstorm (S01_20131218_*.pos)
 * More information: [Digitize EEG electrodes and head shape][3]
 * The output file is copied to each .ds folder and contains the following entries:
  * The position of the center of CTF coils
  * The position of the anatomical references we use in Brainstorm: Nasion and connections tragus/helix, as illustrated [here][4].
 

 * Around 150 head points distributed on the hard parts of the head (no soft tissues)
 

 #### Subject anatomy
 * Subject with 1.5T MRI
 * Marker on the left cheek
 * Processed with FreeSurfer 5.3
 

 

 [1]: http://neuroimage.usc.edu/brainstorm/CiteBrainstorm
 [2]: http://bugzilla.fcdonders.nl/show_bug.cgi?id=2300
 [3]: http://neuroimage.usc.edu/brainstorm/Tutorials/TutDigitize
 [4]: http://neuroimage.usc.edu/brainstorm/CoordinateSystems#Pre-auricular_points_.28LPA.2C_RPA.29
 [5]: http://neuroimage.usc.edu/brainstorm/Tutorials/Auditory",1,0,0
ds005065,2024-04-07 2:26:32,Evan Russek,1.0.0,Heuristics in risky decision-making relate to preferential representation of information MEG data,2024-04-09 14:11:08,0,1,457153000000,425.8 GB,3332,21,0,0,v1.5.0,CC0,"Evan M. Russek, Rani Moran, Yunzhe Liu, Ray Dolan, Quentin Huys",We thank Daniel Bates for assistance with data collection. We acknowledge funding from the Open Research Fund of the State Key Laboratory of Cognitive Neuroscience and Learning to Y.L. and a Wellcome Trust Investigator Award (098362/Z/12/Z) to R.J.D. This work was carried out whilst R.J.D. was in receipt of a Lundbeck 20 Visiting Professorship (R290-2018-2804) to the Danish Research Centre for Magnetic Resonance. The Max Planck UCL Centre is supported by UCL and the Max Planck Society. The Wellcome Centre for Human Neuroimaging (WCHN) is supported by core funding from the Wellcome Trust (203147/Z/16/Z).,,,https://osf.io/preprints/psyarxiv/kb6ew,doi:10.18112/openneuro.ds005065.v1.0.0,UCL ethics board (ID: 9929/002),RiskyDecision,,MEG,"The task consisted of 13 scanner runs (except for subject 1 who completed 5 rather than 3 localizer runs). Runs 1-3 (1-5 for subject 1) are the localizer task. Runs 4-5 are non-analyzed data from the 'probability learning' task. Runs 6-13 (8-15 for subject 1) are the risky decision-making task.
 

 Event times were recorded with a photodiode, which is accessible as a MEG channel. This has been processed so that event times are listed in derivatives/Event_Info_Tables. Raw times of events in the scan are in column ""onset_time"". The corresponding index into the unprocessed MEG data is in column ""scanner_onset_idx"". The onset into the downsampled data is in ""onset_idx_ds"". In the table, each row corresponds to an event. Block number denotes which scanner run that event belongs to. For the localizer task (denoted in phase column), events are image onsets. ""image_type"" specifies the role of that image in the task (""CHOICE"" or ""OUTCOME"") and ""image_number"" denotes which choice or outcome it is (see paper Fig. 1). Finally, ""image_name"" denotes which image category was shown (e.g. ""Hand""). For the task, events correspond to gamble information onset (Info), Probability stimulus presentation (""Choice""), response (""Gamble Response"") and outcome onset (""Outcome""). Columns denote which image was shown and what the response was (accept).
 

 derivatives/Epoched_Data contains epoched preprocessed data for each subject for the localizer task and then around each choice in the main choice task. Both are epoched from from 0-500 ms following the event.
 

 Code to analyze the data along with additional behavioral data is available at https://github.com/evanrussek/MEG_Heuristics_Risk_Preferential_Information",1,0,0
ds004080,2022-07-21 15:35:16,Epilab UMCU,1.2.4,CCEP ECoG dataset across age 4-51,2023-02-23 19:33:50,0,2,288953000000,0.289 TB,930,74,4,51,Brain Imaging Data Structure Specification v1.6.0,CC0,"D. van Blooijs, M.A. van den Boom, J.F. van der Aar, G.J.M. Huiskamp, G. Castegnaro, M. Demuru, W.J.E.M. Zweiphenning, P. van Eijsden, K. J. Miller, F.S.S. Leijten, D. Hermes","The SEIN-UMCU RESPect database group: C.J.J. van Asch, L. van de Berg, S. Blok, M.D. Bourez, K.P.J. Braun, J.W. Dankbaar, C.H. Ferrier, T.A. Gebbink, P.H. Gosselaar, R. van Griethuysen, M.G.G. Hobbelink, F.W.A. Hoefnagels, N.E.C. van Klink, M.A. van ‘t Klooster, G.A.P. de Kort, M.H.M. Mantione, A. Muhlebner, J.M. Ophorst, P.C. van Rijen, S.M.A. van der Salm, E.V. Schaft, M.M.J. van Schooneveld, H. Smeding, D. Sun, A. Velders, M.J.E. van Zandvoort, G.J.M. Zijlmans, E. Zuidhoek and J. Zwemmer","D. van Blooijs, M.A. van den Boom, J.F. van der Aar, G.J.M. Huiskamp, G. Castegnaro, M. Demuru, W.J.E.M. Zweiphenning, P. van Eijsden, K. J. Miller, F.S.S. Leijten, D. Hermes, Developmental trajectory of transmission speed in the human brain, Nature Neuroscience, 2023, https://doi.org/10.1038/s41593-023-01272-0",NIMH R01MH122258 ===NEMAR-SEP=== Alexandre Suerman Stipendium 2015 ===NEMAR-SEP=== EpilepsieNL #17-07,see HowToAcknowledge,doi:10.18112/openneuro.ds004080.v1.2.4,"This study was approved by the Medical Ethical Committee from the UMC Utrecht, The Netherlands",task-SPESclin,,iEEG,"# Dataset description
 This dataset consists of 74 patients age 4-51 years old where Cortico-Cortical Evoked Potentials (CCEPs) were measured with Electro-CorticoGraphy (ECoG) during single pulse electrical stimulation. For a detailed description see:
 

 - Developmental trajectory of transmission speed in the human brain. D. van Blooijs¹, M.A. van den Boom¹, J.F. van der Aar, G.J.M. Huiskamp, G. Castegnaro, M. Demuru, W.J.E.M. Zweiphenning, P. van Eijsden, K. J. Miller, F.S.S. Leijten, D. Hermes, Nature Neuroscience, 2023, https://doi.org/10.1038/s41593-023-01272-0  
  ¹ these authors contributed equally.
 

 This dataset is part of the RESPect (Registry for Epilepsy Surgery Patients) database, a dataset recorded at the University Medical Center of Utrecht, the Netherlands. The study was approved by the Medical Ethical Committee from the UMC Utrecht.
 

 ## Contact 
 - Dorien van Blooijs: D.vanBlooijs@umcutrecht.nl
 - Frans Leijten: F.S.S.leijten@umcutrecht.nl
 - Dora Hermes: hermes.dora@mayo.edu
 

 # Data organization
 This data is organized according to the Brain Imaging Data Structure specification. A community-driven specification for organizing neurophysiology data along with its metadata. For more information on this data specification, see https://bids-specification.readthedocs.io/en/stable/ 
 

 Each patient has their own folder (e.g., `sub-ccepAgeUMCU01` to `sub-ccepAgeUMCU74`) which contains the iEEG recordings data for that patient, as well as the metadata needed to understand the raw data and event timing.
 

 Data are logically grouped in the same BIDS session and stored across runs that indicating the day and time point of recording during the monitoring period.
 If extra electrodes were added/removed during this period, the session was divided into different sessions (e.g. ses-1a and ses-1b). 
 We use the optional run key-value pair to specify the day and the start time of the recording (e.g. run-021315, day 2 after implantation, which is day 1 of the monitoring period, at 13:15).
 The task key-value pair in long-term iEEG recordings describes the patient's state during the recording of this file. The task label is “SPESclin“ since these files contain data collected during clinical single pulse electrical stimulation (SPES). 
 

 Electrode positions include Destrieux atlas labels that were estimated by running Freesurfer on the individual subject MRI scan and taking the most common surface label within a sphere around the electrode. All shared electrode positions were then converted to MNI152 space using the Freesurfer surface based non-linear transformation. We note that this surface based transformation distorts the dimensions of the grids, but maintains the gyral anatomy.
 

 # License
 This dataset is made available under the Public Domain Dedication and License CC v1.0, whose full text can be found at 
 https://creativecommons.org/publicdomain/zero/1.0/. 
 We hope that all users will follow the ODC Attribution/Share-Alike Community Norms (http://www.opendatacommons.org/norms/odc-by-sa/); 
 in particular, while not legally required, we hope that all users of the data will acknowledge by citing the following in any publication:
 Developmental trajectory of transmission speed in the human brain, D. van Blooijs, M.A. van den Boom, J.F. van der Aar, G.J.M. Huiskamp, G. Castegnaro, M. Demuru, W.J.E.M. Zweiphenning, P. van Eijsden, K. J. Miller, F.S.S. Leijten, D. Hermes, Nature Neuroscience, 2023, https://doi.org/10.1038/s41593-023-01272-0
 

 # Code
 Code to analyses these data is available at: https://github.com/MultimodalNeuroimagingLab/mnl\_ccepAge
 

 # Acknowledgements 
 We thank the SEIN-UMCU RESPect database group (C.J.J. van Asch, L. van de Berg, S. Blok, M.D. Bourez, K.P.J. Braun, J.W. Dankbaar, C.H. Ferrier, T.A. Gebbink, P.H. Gosselaar, R. van Griethuysen, M.G.G. Hobbelink, F.W.A. Hoefnagels, N.E.C. van Klink, M.A. van ‘t Klooster, G.A.P. deKort, M.H.M. Mantione, A. Muhlebner, J.M. Ophorst, P.C. van Rijen, S.M.A. van der Salm, E.V. Schaft, M.M.J. van Schooneveld, H. Smeding, D. Sun, A. Velders, M.J.E. van Zandvoort, G.J.M. Zijlmans, E. Zuidhoek and J. Zwemmer) for their contributions and help in collecting the data, and G. Ojeda Valencia for proofreading the manuscript. 
 

 # Funding
 Research reported in this publication was supported by the National Institute of Mental Health of the National Institutes of Health under Award Number R01MH122258 (DH, FSSL, the content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health), the EpilepsieNL under Award Number NEF17-07 (DvB) and the UMC Utrecht Alexandre Suerman MD/PhD Stipendium 2015 (WZ).",1,1,0
ds004229,2022-08-01 18:25:31,Eric Larson,1.0.3,amnoise,2022-08-02 18:33:45,1,1,1908940000,0.002 TB,18,1,0,0,1.7.0,CC0,"Maria Mittag, Eric Larson, Maggie Clarke, Samu Taulu, Patricia K. Kuhl",,"If you use this data, please cite the references provided in this dataset description.",,"Mittag, M., Larson, E., Clarke, M., Taulu, S., & Kuhl, P. K. (2021). Auditory deficits in infants at risk for dyslexia during a linguistic sensitive period predict future language. NeuroImage: Clinical, 30, 102578. https://doi.org/10.1016/j.nicl.2021.102578 ===NEMAR-SEP=== Mittag, M., Larson, E., Taulu, S., Clarke, M., & Kuhl, P. K. (2022). Reduced Theta Sampling in Infants at Risk for Dyslexia across the Sensitive Period of Native Phoneme Learning. International Journal of Environmental Research and Public Health, 19(3), 1180. https://doi.org/10.3390/ijerph19031180",doi:10.18112/openneuro.ds004229.v1.0.3,Human Subjects Division at the University of Washington,"amnoise, noise",,MEG,"ILABS amnoise MEG BIDS dataset
 ==============================
 

 This dataset contains MEG data from a single infant subject. For more
 information, see the following publications, which should be cited if you use
 this data:
 

 - Mittag, M., Larson, E., Clarke, M., Taulu, S., & Kuhl, P. K. (2021). Auditory deficits in infants at risk for dyslexia during a linguistic sensitive period predict future language. NeuroImage: Clinical, 30, 102578. https://doi.org/10.1016/j.nicl.2021.102578
 - Mittag, M., Larson, E., Taulu, S., Clarke, M., & Kuhl, P. K. (2022). Reduced Theta Sampling in Infants at Risk for Dyslexia across the Sensitive Period of Native Phoneme Learning. International Journal of Environmental Research and Public Health, 19(3), 1180. https://doi.org/10.3390/ijerph19031180
 

 The data were converted with MNE-BIDS:
 

 - Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 - Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., Moreau, J., Oostenveld, R., Schoffelen, J., Tadel, F., Wexler, J., Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5, 180110. https://doi.org/10.1038/sdata.2018.110",1,1,0
ds004252,2022-08-16 15:48:03,Denise Moerel,1.0.2,Rotation-tolerant representations elucidate the time course of high-level object processing,2022-08-17 9:52:46,1,1,1392180000,0.001 TB,7,1,0,0,1.6.0,CC0,"Denise Moerel, Tijl Grootswagers, Amanda K. Robinson, Patrick Engeler, Alex O. Holcombe, Thomas A. Carlson",The authors acknowledge the University of Sydney HPC service for providing High Performance Computing resources.,,,"Moerel D., Grootswagers T., Robinson A.K., Engeler P., Holcombe A.O., Carlson T.A. (2022). Rotation-tolerant representations elucidate the time-course of high-level object processing. PsyArXiv, wp73u ===NEMAR-SEP=== Pre-print: https://doi.org/10.31234/osf.io/wp73u ===NEMAR-SEP=== OSF: https://osf.io/r93es",doi:10.18112/openneuro.ds004252.v1.0.2,,,,EEG,"Note: only the data for participant 1 has been uploaded. The rest of the dataset will be released upon publication. 
 

 The pre-print can be found here: https://doi.org/10.31234/osf.io/wp73u
 The analysis codes, results, and figures can be found on OSF: https://osf.io/r93es. 
 

 The main folder contains the raw EEG data in standard bids format.  
 

 The ‘derivatives’ folder contains the pre-processed & epoched EEG data, formatted in line with cosmomvpa.
 

 For codes, results, & figures, see OSF: Engeler, P., Grootswagers, T., Robinson, A. K., Holcombe, A. O., Carlson, T. A., & Moerel, D. (2022, August 17). Rotation-tolerant representations elucidate the time course of high-level object processing. Retrieved from osf.io/r93es",1,1,0
ds004256,2022-08-29 1:25:10,Ole Bialas,1.0.5,Encoding of Sound Source Elevation in Human Cortex,2022-09-07 1:10:54,1,1,19516300000,0.02 TB,584,53,0,0,1.6.0,CC0,"Ole Bialas, Marc Schoenwiesner, Burkhard Maess",,,,,doi:10.18112/openneuro.ds004256.v1.0.5,,"deviantdetection, oneback",,EEG,"# Overview
 The dataset consists of data from two experiments in which subjects were presented bursts of noise from loudspeakers at different elevations. Subjects who participated in either experiment were initially tested in their ability to localize elevated sound sources. Both experiments were conducted in a hemi-anechoic chamber.
 

 # Localization Tests
 Bursts of pink noise were presented from loudspeakers at different elevations and 10° azimuth (to the listeners right). In the localization test preceding experiment I, these loudspeakers were positioned at elevations of +50°, +25°, 0° and -25° while the localization test preceding experiment II also included a loudspeaker at -50° elevation. Localization test data is missing for sub-001, sub-002 and sub-003
 

 # Deviant Detection (Experiment 1)
 Subjects 001-023 participated in this experiment. Subjects heard a long trail of noise from one loudspeaker (adapter), followed by a short burst of noise from another loudspeaker (probe). The elevation of the adapter and probe are encoded in the event values:
 2: adapter at 37.5°, probe at 12.5°
 3: adapter at 37.5°, probe at -12.5°
 4: adapter at 37.5°, probe at -37.5°
 5: adapter at -37.5°, probe at 37.5°
 6: adapter at -37.5°, probe at 12.5°
 7: adapter at -37.5°, probe at -12.5°
 8: no adapter, any non-target location (deviant)
 The behavioral data contains the trial numbers where a deviant was presented and weather the subject responded within one second by pressing a button.
 

 # One-Back (Experiment II)
 Subjects 100-129 participated in this experiment. Subjects heard a long trail of white noise through open headphones followed by a short burst of noise from one of the loudspeakers. The loudspeaker's elevation is encoded in the event values:
 1: 37.5°, 2: 12.5°, 3:-23.5°, 4:-37.5°
 Roughly five percent of trials were targets where subjects heard a beep after the trial, prompting them to localize the previously heard sound. The number of those target trials, as well as the target's elevation and the subject's response can be found in thee behavioral data.",1,1,0
ds004262,2022-09-07 11:58:29,Cameron Hassall,1.0.0,Continuous Feedback Processing,2022-09-07 12:52:02,1,1,3728370000,0.004 TB,194,21,21,41,1.2.1,CC0,"Cameron D. Hassall, Yan Yan, Laurence T. Hunt",,,,,doi:10.18112/openneuro.ds004262.v1.0.0,,gnomes,,EEG,"# Continuous Feedback Processing
 

 Twenty-one participants learned to predict the final level of an animated rising bar. Following the appearance of a fixation cross, participants used the mouse to indicate their guess (i.e., how high they thought the bar would rise). After a delay, participants watched the bar rise to its final level. Points were awarded based on the distance between their guess and the actual level. Each round was cued by the appearance of a gnome (cover story: the gnomes are playing a strongman game while visiting a fair). Cues varied in the degree to which the outcome was predictable (highly predictable, somewhat predictable, unpredictable).  
 

 Participant 11 was excluded from the analysis due to excessive artifacts.  
 

 Timing  
 fixation cross (400-600 ms) -> gnome cue (1500 ms) -> bar outline (until response) -> animation (1 degree per second until complete) -> final outcome (1000 ms)
 

 Conditions (Gnome Types)  
 1: highly predictable - consistently low  
 2: highly predictable - consistently high  
 3: unpredictable - low or high with equal probability  
 4: somewhat predictable - usually (80\%) low, sometimes high  
 5: somewhat predictable - usually (80\%) high, sometimes low  
 6: unpredictable - random uniform distribution  
 

 Trigger Modifiers  
 Add 0: Fixation cross  
 Add 10: Cue (gnome) onset  
 Add 20: Bar outline appears  
 Add 30: Participant response  
 Add 40: Start of animation  
 Add 50: End of animation (and start of 1-second delay)",1,1,0
ds004264,2022-09-13 14:39:12,Cameron Hassall,1.1.0,Steer the Ship,2022-09-13 14:57:06,1,1,3546000000,0.004 TB,194,21,21,41,1.2.1,CC0,"Cameron D. Hassall, Yan Yan, Laurence T. Hunt",,,,,doi:10.18112/openneuro.ds004264.v1.1.0,,steertheship,,EEG,"# Steer the Ship
 

 Twenty-one participants learned to control the trajectory of a ship, represented by centrally presented rotating arrow. Prior to each round participants were cued about the degree of controller and environmental noise (""wind"") they would experience. During the round, participants pressed the 'f' and 'j' keys to apply angular force in either a clockwise or counterclockwise direction. The goal of the task was to keep the ship closely oriented towards a target. Points were accumulated depending on the mean distance to target. The ship would crash if it strayed too far from the target (and the round would end). Each round lasted up to 1 minute. The underlying physics were based on the pole-and-cart problem (i.e., unstable).  
 

 There were four noise conditions:  
 1: No noise  
 2: Environmental noise only (ship occasionally moved on its own)  
 3: Controller noise only (amount of force varied)  
 4: Environmental and controller noise  
 

 Participant 12 should be excluded from event-locked analyses due to bad triggers (trigger cable was partially disconnected).  
 

 Also note that the RT for the first button press in each round is not recorded (but is recorded in the participantActions column).
 

 Trigger Modifiers (added to condition numbers)  
 Add 0: Condition cue  
 Add 10: Start of round  
 Add 20: Left button press  
 Add 30: Right button press  
 Add 40: Left button press (computer)  
 Add 50: Right button press (computer)  
 Add 60: Crash  
 Add 70: Success (reached 1 minute of play)  
 Add 80: Points displayed",1,1,0
ds004276,2022-09-23 14:41:43,Christian Brodbeck,1.0.0,Auditory single word recognition in MEG,2022-10-07 15:21:12,0,1,12467000000,0.012 TB,135,18,18,30,1.6.0,CC0,"Phoebe Gaston, Christian Brodbeck, Colin Phillips, Ellen Lau",,,NSF BCS-1749407,,doi:10.18112/openneuro.ds004276.v1.0.0,,"words, noise",,MEG,"# Auditory single word recognition in MEG
 

 This dataset is described in Gaston et al. (2022).
 

 Stimuli and TextGrids are available from the Massive Auditory Lexical Decision database (Tucker et al., 2019).
 

 Converted to BIDS using MNE-BIDS (Appelhoff et al., 2019; Niso et al., 2018).
 

 

 # References
 

 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Gaston, P., Brodbeck, C., Phillips, C., & Lau, E. (2022). Auditory word comprehension is less incremental in isolated words. Neurobiology of Language
 

 Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., Moreau, J., Oostenveld, R., Schoffelen, J., Tadel, F., Wexler, J., Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5, 180110. https://doi.org/10.1038/sdata.2018.110
 

 Tucker, B. V., Brenner, D., Danielson, D. K., Kelley, M. C., Nenadic, F., & Sims, M. (2019). The Massive Auditory Lexical Decision (MALD) database. Behavior Research Methods, 51(3), 1187–1204. https://doi.org/10.3758/s13428-018-1056-1",1,1,0
ds002908,2020-06-22 13:05:54,Eva Zita Patai,1.0.0,Human MEG recordings during sequential conflict task,2020-09-07 8:38:13,1,6,64199900000,59.8 GB,536,13,0,0,1.0.1,CC0,"Rafal Bogacz, Vladimir Litvak","We thank Dr. Ashwini Oswal, Dr. Simon Little, Dr. David Pedrosa, Dr. Damian Herz and Dr.Viswas Dayal for clinical support of patient recordings, as well as the patients and their families.","Please cite this paper: Patai, Z. E., Foltynie, T., Limousin, P., Hariz, M. I., Zrinzo, L.,Bogacz, R., Litvak, V. (2020). Conflict detection in a sequential decision task is associated with increased cortico-subthalamic coherence and prolonged subthalamic oscillatory response in the beta band.",MC_UU_12024/5 ===NEMAR-SEP=== MC_UU_00003/1 ===NEMAR-SEP=== BB/S006338/1 ===NEMAR-SEP=== 203147/Z/16/Z ===NEMAR-SEP=== MR/K005464/1,,10.18112/openneuro.ds002908.v1.0.0,University College London Ethics Committee approval for minimum risk magnetoencephalography studies of healthy human cognition,mouse,,MEG,,1,0,0
ds004279,2022-09-26 14:52:36,CARLOS VALLE ARAYA,1.1.1,EEG: silent and perceive speech on 30 spanish sentences,2023-08-02 22:50:05,0,4,27082300000,0.027 TB,305,56,19,29,1.6.0,CC0,"Carlos Valle Araya, Carolina Mendez-Orellana, Maria Rodriguez-Fernandez",We thank Andrea Sanchez and Amaro silva for their help during data collection.,,Doctoral scholarship N 21211883 (2021)383 by Agencia Nacional de Investigacion y Desarrollo (ANID Chile),,doi:10.18112/openneuro.ds004279.v1.1.1,,sentences,,EEG,"EEG: silent and perceive speech on 30 spanish sentences
 Large Spanish Speech EEG dataset 
 

 Authors
 <ul>
  <li>Carlos Valle</li>
  <li>Carolina Mendez-Orellana</li>
  <li>María Rodríguez-Fernández</li>
 </ul>
 

 

 Resources:
 <ul>
  <li>Code availaible at: https://github.com/cgvalle/Large\_Spanish\_EEG</li>
  <li>Publication: </li>
 </ul>
 

 Abstract:
 Decoding speech from brain activity can enable communication for individuals with speech disorders. Deep neural networks have shown great potential for speech decoding applications, but the large data sets required for these models are usually not available for neural recordings of speech impaired subjects. Harnessing data from other participants would thus be ideal to create speech neuroprostheses without the need of patient-specific training data.
 In this study, we recorded 60 sessions from 56 healthy participants using 64 EEG channels and developed a neural network capable of subject-independent classification of perceived sentences. We found that sentence identity can be decoded from subjects without prior training achieving higher accuracy than mixed-subject models.
 The development of subject-independent models eliminates the need to collect data from a target subject, reducing time and data collection costs during deployment. These results open new avenues for creating speech neuroprostheses when subjects cannot provide training data.  
 

 

 Please contact us at this e-mail address if you have any question: cgvalle@uc.cl",1,1,0
ds004284,2022-10-03 22:02:01,John Veillette,1.0.0,eeg-neuroforecasting,2023-06-13 21:09:06,0,1,17562700000,0.018 TB,167,18,0,0,1.6.0,CC0,"Veillette, J., Heald, S., Wittenbrink, B., Nusbaum, H.",,,,https://github.com/john-veillette/eeg-neuroforecasting,doi:10.18112/openneuro.ds004284.v1.0.0,,kickstarter,,EEG,"References
 ----------
 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G., Phillips, C., Delorme, A., Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Scientific Data, 6, 103. https://doi.org/10.1038/s41597-019-0104-8",1,1,0
ds004295,2022-10-15 6:53:16,Christopher Stolz,1.0.0,Reward gain and punishment avoidance reversal learning,2022-10-15 16:44:29,0,1,33829400000,0.034 TB,158,26,0,0,1.1.1,CC0,"Christopher Stolz, Alan Pickering, Erik M. Mueller",,,,,doi:10.18112/openneuro.ds004295.v1.0.0,,task,,EEG,"Two reversal learning tasks with different reinforcer (monetary reward vs. primary threat reinforcer). Positive feedback in the reward task indicated monetary reward (+10 Cent) and negative feedback monetary non-reward (+0 Cent). In the punishment task, positive feedback signaled successful avoidance of a loud white noise burst and negative feedback the application of the noise burst. The white noise burst intensity was titrated to match monetary reward (+10 Cent) for every participant (81 dB, 84 dB, 87, dB, 90 dB).",1,1,0
ds004306,2022-10-25 17:34:10,Holly Wilson,1.0.2,MultisensoryDataset,2023-01-24 22:01:03,0,3,686761000,0.64 GB,514,12,0,0,1.7.0,CC0,"Holly Wilson, Mohammad Golbabaee, Michael Proulx, Eamonn O'Neill",,,EPSRC,,doi:10.18112/openneuro.ds004306.v1.0.2,PREC,experiment,,EEG,"This dataset consists of electroencephalography (EEG) signals acquired with a 124 EEG ANT-Neuro device. 
 

 ### Participants and Sessions
 There are 13 participants included, ten performed one session and three performed two sessions. All participants had normal or corrected vision and hearing, apart from sub-16.
 

 ### Task
 The task consisted of imagining and perceiving stimuli from three modalities; visual pictorial, visual orthographic (writing) or auditory. Each of the stimuli belonged to one of three categories: guitar, flower and penguin. These categories were selected based on being semantically dissimilar to one another, and because there were all of 2 syllables. 
 

 ### Dataset Versions
 The dataset provided consists of the raw EEG data, a pre-processed version, and an epoched version.",1,0,0
ds004315,2022-10-27 0:49:48,Trevor Clinton James Jackson,1.0.0,"Mood Manipulation and PST, Experiment 1",2022-10-27 5:10:59,0,1,10532300000,0.011 TB,463,50,19,55,1.1.1,CC0,"James F Cavanagh, Trevor C J Jackson",,,,DOI:,doi:10.18112/openneuro.ds004315.v1.0.0,,PST,,EEG,"Reinforcement learning task with 50 healthy controls (25 after a sad mood manipulation, 25 after a neutral mood manipulation) Task with a training section and testing section.  Task adapted from here: https://doi.org/10.1126/science.1102941. Separate mood manipulation occurred earlier. Task included in Matlab programming language.  Data collected from 2019-2021 in Cognitive Rhythms and Computation Lab at University of New Mexico. Check the .xls sheet under code folder for more meta data.  - Trevor CJ Jackson 06/25/2021",1,1,0
ds004317,2022-10-27 5:08:53,Trevor Clinton James Jackson,1.0.3,"Mood Manipulation and PST, Experiment 2",2022-10-27 16:57:51,0,1,19639200000,0.02 TB,425,50,18,53,1.1.1,CC0,"James F Cavanagh, Trevor C J Jackson",,,,DOI:,doi:10.18112/openneuro.ds004317.v1.0.3,,PST,,EEG,"Reinforcement learning task with 50 healthy controls (25 after a sad mood manipulation, 25 after a happy mood manipulation) Task with a training section and testing section.  Task adapted from here: https://doi.org/10.1126/science.1102941. Mood Manipulation occurs during task before each training block. Task included in Matlab programming language.  Data collected from 2019-2021 in Cognitive Rhythms and Computation Lab at University of New Mexico. Check the .xls sheet under code folder for more meta data. - Trevor CJ Jackson 10/27/2022",1,1,0
ds004324,2022-11-02 10:34:06,Alberto Barradas,1.0.0,ToonFaces,2023-09-14 7:37:42,0,1,2637690000,0.003 TB,161,26,0,0,1.6.0,CC0,"Luis Alberto Barradas Chacón, Selina C. Wriessnegger",,,,,doi:10.18112/openneuro.ds004324.v1.0.0,,RSVP,,EEG,"# Images of stylized faces improve ERP features used for emotion detection
 

 For their ease of accessibility and low cost, current Brain-Computer Interfaces (BCI) used to detect subjective emotional and affective states rely largely on electroencephalographic (EEG) signals. Numerous datasets are publicly available for any researcher to design models for affect detection from EEG. However, few designs focus on optimally exploiting the nature of the stimulus elicitation to improve accuracy.
 We found that artificially enhanced human faces with exaggerated visual features significantly improve some commonly used neural correlates of emotion as measured by event-related potentials (ERPs). These images elicit an enhanced N170 component, well known in facial recognition encoding. Our findings suggest that the study of emotion elicitation could exploit consistent stimuli transformations to study the characteristics of ERPs related to specific affective stimuli. Furthermore, this specific result might be useful in the context of affective BCI design, where a higher accuracy in affect detection from EEG can improve the experience of a user.
 

 Participant information has been removed for annonimation reasons.
 

 References
 ----------
 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G., Phillips, C., Delorme, A., Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Scientific Data, 6, 103. https://doi.org/10.1038/s41597-019-0104-8",1,1,0
ds004330,2022-11-07 14:41:49,Johannes Janek Daniel Singer,1.0.0,The spatiotemporal neural dynamics of object recognition for natural images and line drawings (MEG),2022-11-08 11:21:52,0,1,165027000000,0.165 TB,1145,30,0,0,1.7.0,CC0,"Johannes J.D. Singer, Radoslaw M. Cichy, Martin N. Hebart",,,ERC-StG-2021-101039712 ===NEMAR-SEP=== CI241/1-1 ===NEMAR-SEP=== CI241/3-1 ===NEMAR-SEP=== CI241/7-1 ===NEMAR-SEP=== ERC-StG-2018-803370,https://biorxiv.org/cgi/content/short/2022.08.12.503484v1,doi:10.18112/openneuro.ds004330.v1.0.0,,main,,MEG,"This dataset contains the raw MEG data accompanying the paper ""The spatiotemporal neural dynamics of object recognition for natural images and line drawings"" (Link to preprint: https://biorxiv.org/cgi/content/short/2022.08.12.503484v1). Please cite the above paper if you use this data.
 

 The dataset includes:
 

 MEG data for 9 runs for each subjects. Events files that contain the onsets, durations and trial types for each trial in the experiment (excluding catch trials).
 

 For a full description of the paradigm and the employed procedures please see the manuscript.  
 

 Results for the first-level analyses for this data can be found on OSF (https://osf.io/vsc6y/). Code for the analysis of the data can be found on Github (https://github.com/Singerjohannes/object\_drawing\_dynamics/).",1,1,0
ds004346,2022-11-28 16:33:17,Tara Ghafari,1.0.8,FLUX: A pipeline for MEG analysis,2022-11-30 17:54:29,0,1,3857850000,3.6 GB,19,1,0,0,1.7.0,CC0,"Oscar Ferrante, Ling Liu, Tamas Minarik, Urszula Gorska, Tara Ghafari, Huan Luo, Ole Jensen",,,,,doi:10.18112/openneuro.ds004346.v1.0.8,,SpAtt,,"MEG, MRI","References
 

 ----------
 

 Ferrante, O., Liu, L., Minarik, T., Gorska, U., Ghafari, T., Luo, H., & Jensen, O. (2022). FLUX: A pipeline for MEG analysis. NeuroImage, 253, 119047. https://doi.org/10.1016/j.neuroimage.2022.119047
 

 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., Moreau, J., Oostenveld, R., Schoffelen, J., Tadel, F., Wexler, J., Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5, 180110. https://doi.org/10.1038/sdata.2018.110",1,0,0
ds004347,2022-11-28 17:02:27,John Tyson-Carr,1.0.0,Symmetry perception and affective responses: a combined EEG/EMG study,2022-11-28 17:08:56,0,1,2574270000,0.003 TB,173,24,0,0,1.1.1,CC0,"Makin, A. D. J, Wilton, M. M, Pecchinenda, A., Bertamini, M.",,,,10.1016/j.neuropsychologia.2012.10.003,doi:10.18112/openneuro.ds004347.v1.0.0,,jacobsen,,EEG,"SPN1 Experiment 1 Project 1. After stimulus offset, participants reported whether patterns were regular or random.
 

 For full catalogue, see https://osf.io/2sncj/",1,1,0
ds004348,2022-11-29 8:54:03,kaare mikkelsen,1.0.4,Ear-EEG Sleep Monitoring 2017 (EESM17),2022-12-01 8:29:38,0,1,8793610000,8.2 GB,113,9,0,0,1.7.0,CC0,"Kaare B. Mikkelsen, David B. Villadsen, Laura Birch, Marit Otto, Preben Kidmose",,Please cite Mikkelsen et al 2017: https://doi.org/10.1186/s12938-017-0400-5,,,doi:10.18112/openneuro.ds004348.v1.0.4,,Sleep,,EEG,"Ear-EEG Sleep Monitoring 2017 (EESM17) data set
 

 **Overview**
 

 This dataset was collected as part of a research project on ear-EEG sleep monitoring which took place in 2017.
 

 The data set contains nightly EEG recordings from 9 healthy participants ('subjects'). The recordings consist of 'partial polysomnography' (PSG) measurements, including EEG, EOG and chin EMG combined with 14 ear-EEG electrodes.
 

 **Format**
 

 The dataset is formatted according to the Brain Imaging Data Structure. See the 'dataset_description.json' file for the specific BIDS version used. The EEG data format chosen is the '.set' format of EEGLAB.
 

 For more information, see the following link:
 https://bids-specification.readthedocs.io/en/stable/01-introduction.html
 

 

 **Task description**
 

 The subjects were instructed to perform two recordings. In the first recording, they had to simply relax in a chair either reading or watching television, prior to going to bed. These recordings are labeled as 'wake' task. After this, the real recording started, which took place during the night and began when the subject went to bed. These recordings are labeled as having task 'sleep'. 
 The recording equipment was mounted in the afternoon, and the recordings took place at the subject's home.
 

 The data set was previously described in the paper: https://doi.org/10.1186/s12938-017-0400-5
 When citing this data set, please refer to this paper.
 

 

 Please note that for all subjects, the sleep scoring begins at 'Lights out'.
 

 ** Notes **
 

 Due to a miscommunication in the original sleep study, two ear-EEG channels, ERB1 and ELB1, were not used. However, they are included in the data set. Both electrode positions were very close to the ERB and ELB positions. 
 

 **Contact**
 

 For questions regarding this data set, contact: 
 Kaare Mikkelsen, Mikkelsen.kaare@ece.au.dk, https://orcid.org/0000-0002-7360-8629",1,0,0
ds004350,2022-12-04 7:25:37,Liana Pakingan,1.1.1,Executive Functioning Data,2022-12-06 3:04:16,0,2,22949200000,0.023 TB,960,24,0,0,1.8.0,CC0,"Tracy Brandmeyer, Arnaud Delorme",,,,https://doi.org/10.3389/fnhum.2020.00246,doi:10.18112/openneuro.ds004350.v1.1.1,,"LG, NB, SART",8.1.0,EEG,"## Executive Functioning Tasks
 

 The data of this dataset was collected as part of an executive functioning battery consisting of three separate tasks: 
 

 1) N-Back (NB)
 

 2) Sustained Attention to Response Task (SART)
 

 3) Local Global (LG)
 

 The original experiment details in which these tasks were conducted in addition to can be read about here (https://doi.org/10.3389/fnhum.2020.00246).
 

 **Experiment Design:** Two sessions of each task were conducted on the first and last day of the neurofeedback experiment with 24 participants (mentioned above).
 

 **[N-Back (NB)]** Participants performed a visual sequential letter n-back working memory task, with memory load ranging from 1-back to 3-back. The visual stimuli consisted of a sequence of 4 letters (A, B, C, D) presented black on a gray background. Participants observed stimuli on a visual display and responded using the spacebar on a provided keyboard. In the 1-back condition, the target was any letter identical to the trial immediately preceding one. In the 2-back and 3-back conditions, the target was any letter that was presented two or three trials back, respectively. The stimuli were presented on a screen for a duration of 1 s, after which a fixation cross was presented for 500 ms. Participants responded to each stimulus by pressing the spacebar with their right hand upon target presentation. If no spacebar was pressed within 1500 ms of the stimulus presentation, a new stimulus was presented. Each n-back condition (1, 2, and 3-back) consisted of the presentation of 280 stimuli selected randomly in the 4-letter pool.
 

 **[Sustained Attention to Response Task (SART)]** Participants were presented with a series of single numerical digits (randomly selected from 0 to 9 - the same digit could not be presented twice in a row) and instructed to press the spacebar for each digit, except for when presented with the digit 3. Each number was presented for 400 ms in white on a gray background. The inter-stimulus interval was 2 s irrespective of the button press and a fixation cross was present at all times except for when the digits were presented. Participants performed the SART for approximately 10 minutes corresponding to 250 digit presentations.
 

 **[Local Global (LG)]** Participants were shown large letters (H and T) on a computer screen. The large letters were made up of an aggregate of smaller letters that could be congruent (i.e large H made of small Hs or large T made of small Ts) or incongruent (large H made of small Ts or large T made of small Hs) with respect to the large letter. The small letters were 0.8 cm high and the large letters were 8 cm high on the computer screen. A fixation cross was present at all times except when the stimulus letters were presented. Letters were shown on the computer screen until the subject responded. After each subject's response, there was a delay of 1 s before the next stimulus was presented. Before each sequence of letters, instructions were shown on a computer screen indicating to participants whether they should respond to the presence of small (local condition) or large (global condition) letters. The participants were instructed to categorize specifically large letters or small letters and to press the letter H or T on the computer keyboard to indicate their choice. 
 

 **Data Processing:** Data processing was performed in Matlab and EEGLAB. The EEG data was average referenced and down-sampled from 2048 to 256 Hz. A high-pass filter at 1 HZ using an elliptical non-linear filter was applied and the data was then average referenced.
 

 **Note:** The data files in this dataset were converted into the .set format for EEGLAB. The .bdf files that were converted for each of the tasks can be found in the sourcedata folder.
 

 **Exclusion Note:** The second run of NB in session 1 of sub-11 and the run of SART in session 1 of sub-18 were both excluded due to issues with conversion to .set format. However, the .bdf files of these runs can be found in the sourcedata folder.",1,1,1
ds004356,2022-12-06 20:13:46,Tong Shan (TeenagingCu),2.2.1,Music and Speech Elicit Similar Subcortical Responses in Human Listeners,2022-12-12 17:34:53,0,1,228796000000,213.1 GB,639,22,19,37,1.7.0,CC0,"Tong Shan, Madeline S. Cappelloni, Ross K. Maddox",,,the Schmitt Program in Neuroscience,"Shan, T., Cappelloni, M.S. & Maddox, R.K. Subcortical responses to music and speech are alike while cortical responses diverge. Sci Rep 14, 789 (2024). https://doi.org/10.1038/s41598-023-50438-0",doi:10.18112/openneuro.ds004356.v2.2.1,approved by the University of Rochester Subjects Review Board (#66988),MusicvsSpeech,,EEG,"# README
 

 ## Details related to access to the data
 

 Please contact the following authors for further information:
 - Tong Shan (email: tshan@ur.rochester.edu)
 - Ross K. Maddox (email: rmaddox@ur.rochester.edu)
 

 ## Overview
 

 The goal of this study is to derive Auditory Brainstem Response (ABR) from continuous music and speech stimuli using deconvolution method. Data collected from Jun to Aug, 2021.
 

 The details of the experiment can be found at Shan et al. (2024). There were two phases in this experiment. For the first phase, ten trials of one-minute clicks were presented to the subjects. For the second phase, the 12 types (six genres of music and six types of speech) of 12 s stimuli clips were presented. There were 40 trials 
 for each type with shuffled order. Between trials, there was a 0.5 s pause. 
 

 The code for stimulus preprocessing and EEG analysis is available on Github: 
 

 https://github.com/maddoxlab/Music_vs_Speech_abr
 

 

 ## Format
 

 This dataset is formatted according to the EEG Brain Imaging Data Structure. It includes EEG recording from subject 001 to subject 024 (excluding subject 014 and subject 021) in raw brainvision format (including `.eeg`, `.vhdr`, and `.vmrk` triplet) and stimuli files in format of `.wav`. 
 

 For some subjects (sub-03 & sub-19), there are 2 ""runs"" of data that the first run (`run-01`) only contains the click phase (phase 1), and the second run includes the data for the ABR analysis. 
 

 Triggers with values of ""1"" were recorded to the onset of the stimulus, and shortly after triggers with values of ""4"" or ""8"" were stamped to indicate the stimulus types and the trial number out of 40. This was done by converting the decimal trial number to bits, denoted b, then calculating 2 ** (b + 2). Triggers of ""999"" denote the start of a new segment of EEG. We've specified these trial numbers and more metadata of the events in each of the `*_eeg_events.tsv` file, which is sufficient to know which trial corresponded to which type of stimulus and which file.
 

 

 ## Subjects
 

 24 subjects participated in this study.
 

 **Subject inclusion criteria**
 1. Age between 18-40.
 2. Normal hearing: audiometric thresholds of 20 dB HL or better from 500 to 8000 Hz.
 3. Speak English as their primary language.
 4. Self-reported normal or correctable to normal vision.
 

 **Subject exclusion criteria**
 1. Subject 014 self-withdrew partway through the experiment.
 2. Subject 021 was excluded because of technical problems during data collection that led to unusable data.
 

 Therefore, after excluding the two subjects, there were 22 subjects (11 male and 11 female) with an age of 22.7 ± 5.1 (mean ± SD) years that we included in the analysis. Please see `subjects.tsv` for more demography.
 

 ## Apparatus
 

 Subjects were seated in a sound-isolating booth on a chair in front of a 24-inch BenQ monitor with a viewing distance of approximately 60 cm. Stimuli were presented at an average level of 65 dB SPL and a sampling rate of 48000 Hz through ER-2 insert earphones plugged into an RME Babyface Pro digital sound card. The stimulus presentation for the experiment was controlled by a python script using a custom package, `expyfun`.",1,0,0
ds004357,2022-12-08 4:57:17,Tijl Grootswagers,1.0.0,Features-EEG,2023-04-26 6:43:02,0,1,20769800000,0.021 TB,71,16,0,0,1.0.2,CC0,"Grootswagers, Tijl, Robinson, Amanda, Shatek, Sofia, Carlson, Thomas",,,,,doi:10.18112/openneuro.ds004357.v1.0.0,,rsvp,,EEG,"Experiment Details
 Electroencephalography recordings from 16 subjects to fast streams of gabor-like stimuli.
 Images were presented in rapid serial visual presentation streams at 6.67Hz and 20Hz rates. Participants performed an orthogonal fixation colour change detection task.
 

 Experiment length: 1 hour",1,1,0
ds004362,2022-12-12 20:42:11,Liana Pakingan,1.0.0,EEG Motor Movement/Imagery Dataset,2022-12-15 16:24:41,0,1,8347280000,0.008 TB,9162,109,0,0,1.8.0,CC0,"Gerwin Schalk, Dennis J McFarland, Thilo Hinterberger, Niels Birbaumer, Jonathan R Wolpaw",,"When using this resource, please cite the original publication:
 

 Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw, J.R. BCI2000: A General-Purpose Brain-Computer Interface (BCI) System. IEEE Transactions on Biomedical Engineering 51(6):1034-1043, 2004.",,"Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw, J.R. BCI2000: A General-Purpose Brain-Computer Interface (BCI) System. IEEE Transactions on Biomedical Engineering 51(6):1034-1043, 2004.",doi:10.18112/openneuro.ds004362.v1.0.0,,motion,8.1.0,EEG,"##Acknowledgements
 This data set was originally created and contributed to PhysioBank by Gerwin Schalk (schalk at wadsworth dot org) and his colleagues at the BCI R&D Program, Wadsworth Center, New York State Department of Health, Albany, NY. W.A. Sarnacki collected the data. Aditya Joshi compiled the dataset and prepared the documentation. D.J. McFarland and J.R. Wolpaw were responsible for experimental design and project oversight, respectively. This work was supported by grants from NIH/NIBIB ((EB006356 (GS) and EB00856 (JRW and GS)). 
 

 **To access the initial publication of this dataset, please visit this link to PhysioBank: https://physionet.org/content/eegmmidb/1.0.0/**
 

 ## Experiment Protocol
  This data set consists of over 1500 one- and two-minute EEG recordings, obtained from 109 volunteers, as described below.  
  
 Subjects performed different motor/imagery tasks while 64-channel EEG were recorded using the BCI2000 system (http://www.bci2000.org). Each subject performed 14 experimental runs: two one-minute baseline runs (one with eyes open, one with eyes closed), and three two-minute runs of each of the four following tasks:
  
 **[Task 1]** A target appears on either the left or the right side of the screen. The subject opens and closes the corresponding fist until the target disappears. Then the subject relaxes. 
 **[Task 2]** A target appears on either the left or the right side of the screen. The subject imagines opening and closing the corresponding fist until the target disappears. Then the subject relaxes.  
 **[Task 3]** A target appears on either the top or the bottom of the screen. The subject opens and closes either both fists (if the target is on top) or both feet (if the target is on the bottom) until the target disappears. Then the subject relaxes. 
 **[Task 4]** A target appears on either the top or the bottom of the screen. The subject imagines opening and closing either both fists (if the target is on top) or both feet (if the target is on the bottom) until the target disappears. Then the subject relaxes. 
  
 In summary, the experimental runs were:  
  1. Baseline, eyes open  
  2. Baseline, eyes closed 
  3. Task 1 (open and close left or right fist)  
  4. Task 2 (imagine opening and closing left or right fist) 
  5. Task 3 (open and close both fists or both feet) 
  6. Task 4 (imagine opening and closing both fists or both feet)  
  7. Task 1  
  8. Task 2  
  9. Task 3  
  10. Task 4 
  11. Task 1 
  12. Task 2 
  13. Task 3 
  14. Task 4 
 

 Each event code includes an event type indicator (T0, T1, or T2) that is concatenated to the Task # it belongs with (i.e TASK1T2). The event type indicators change definition depending on the Task # it is associated with. For example, TASK1T2 would correspond to the onset of real motion in the right fist, while TASK3T2 would correspond to onset of real motion in both feet:
 

 **[T0]** corresponds to rest
 

 **[T1]** corresponds to onset of motion (real or imagined) of:
 

 - the left fist (in runs 3, 4, 7, 8, 11, and 12; for Task 1 (real) and Task 2 (imagined))
 - both fists (in runs 5, 6, 9, 10, 13, and 14; for Task 3 (real) and Task 4 (imagined))
 

 **[T2]** corresponds to onset of motion (real or imagined) of:
 

 - the right fist (in runs 3, 4, 7, 8, 11, and 12; Task 1 (real) and Task 2 (imagined))
 - both feet (in runs 5, 6, 9, 10, 13, and 14; for Task 3 (real) and Task 4 (imagined))
 

 **Note:** The data files in this dataset were converted into the .set format for EEGLAB. The event codes in the .set files of this dataset will contain the concatenated event codes above for all event files for clarity purposes. The non-converted .edf files along with the accompanying PhysioBank-compatible annotation files for all the runs of each subject can be found in the sourcedata folder. In the non-converted .edf files the event codes will only be shown as T0, T1, and T2 regardless of task type. All the Matlab scripts used for the .set conversion and renaming of event codes of the PhysioBank .edf files can be found in the code folder.
 

 ## Montage
 The EEGs were recorded from 64 electrodes as per the international 10-10 system (excluding electrodes Nz, F9, F10, FT9, FT10, A1, A2, TP9, TP10, P9, and P10), as shown in the figure in the code folder. The numbers below each electrode name indicate the order in which they appear in the records; note that signals in the records are numbered from 0 to 63, while the numbers in the figure range from 1 to 64.",1,1,1
ds004367,2022-12-15 11:42:01,Martin Rouy,1.0.2,,2022-12-15 12:45:27,0,1,30039300000,0.03 TB,206,40,0,0,1.1.1,CC0,"Martin Rouy, Matthieu Roger, Dorian Goueytes, Michael Pereira, Paul Roux, Nathan Faivre",,,,,doi:10.18112/openneuro.ds004367.v1.0.2,,rdk,,EEG,,1,1,0
ds004368,2022-12-15 12:51:06,Martin Rouy,1.0.2,Meta-rdk,2022-12-15 13:00:15,0,2,1045570000,0.001 TB,206,39,0,0,1.1.1,CC0,"Martin Rouy, Matthieu Roger, Dorian Goueytes, Michael Pereira, Paul Roux, Nathan Faivre",,,,,doi:10.18112/openneuro.ds004368.v1.0.2,,task,,EEG,,1,1,0
ds004369,2022-12-16 13:02:02,Björn Holtze,1.0.1,Blink-Pause-Relation (Competing Speaker Paradigm),2023-02-06 14:16:24,0,1,2182340000,0.002 TB,345,41,18,33,v5.2,CC0,"Bjoern Holtze, Marc Rosenkranz, Martin Bleichner, Stefan Debener",,,,tba,doi:10.18112/openneuro.ds004369.v1.0.1,,attendedSpeakerParadigm,,EEG,,1,1,0
ds004381,2022-12-20 10:56:51,Vasileios S. Dimakopoulos,1.0.1,Intraoperative EEG dataset during medianus-tibialis stimulation with 8 different rates,2023-04-02 15:14:21,0,4,4978750000,0.005 TB,1069,14,29,87,1.4.0,CC0,"Giorgio Selmin, Vasileios Dimakopoulos, Luca Regli, Johannes Sarnthein",We acknowledge a grant awarded by the Swiss National Science Foundation(funded by the SNSF 204651 to J.S.) and a scholarship awarded by the Alexander S. Onassis Public Benefit Foundation (to V.D.). The funders had no role in the design or analysis of the study. We thank two anonymous reviewers for their constructive comments on earlier versions of the manuscript.,"If you use this dataset please cite this paper:
 Dimakopoulos V, Selmin G, Regli L, Sarnthein J,
 Optimization of signal-to-noise ratio in short-duration SEP recordings by variation of stimulation rate, Clinical Neurophysiology, 2023, ISSN 1388-2457, https://doi.org/10.1016/j.clinph.2023.03.008.","Swiss National Science Foundation, SNSF 204651 ===NEMAR-SEP=== Alexander S. Onassis Public Benefit Foundation Scholarship",https://hfozuri.ch/,doi:10.18112/openneuro.ds004381.v1.0.1,2019-01977,sepRate,,EEG,"# Intraoperative SEP with stimulation rates 2.7-28.7 Hz
 This dataset of medianus and tibial nerve SEP was first analyzed in publication [1]. There we investigated how the signal-to-noise ratio (SNR) depends on stimulus repetition rate (2.7 - 28.7 Hz) and recording duration (4 – 20 s).
 

 There are 14 subjects with continuous EEG data split in sessions (tibial left/right, medianus left/right) and runs (1 run for each stimulation rate).
 We also provide processed data (derivatives) for all the sessions. 
 In total there are 26 medianus SEP and 24 tibial SEP sessions.
  
 ## Repository structure
 

 ### Main directory (SEP rate/)
 Contains metadata files in the BIDS standard about the participants and the study. Folders are explained below.
 

 ### Subfolders
 * SEP rate/sub-**/
 Contains folders for each subject, named sub-<subject number> and session information.
 * SEP rate/sub-**/ses-01/eeg
 Contains the raw eeg data in .edf format for each subject. 
 Each *eeg.edf file contains EEG data from one stimulation rate (see scans.tsv column stimRate).
 Details about the channels are given in the corresponding .tsv file. 
 * SEP rate/derivatives
 Contains folders for each subject,named sub-<subject number> and session information that include processed data
 * SEP rate/derivatives/sub-**/ses-01/eeg/
 Contains processed data for each subject.
 

 

 # Note from the paper
 ""The offline data processing used the continuous EEG that was recorded in parallel to the SEP recordings. 
 Data analysis was performed with custom scripts in Matlab (www.mathworks.com). To detect the SEP stimulation artefact, 
 we first filtered the EEG (high pass cutoff = 200 Hz) and performed local peak detection (minimum peak prominence between peaks = 30 ms,
 minimum peak width = 4 ms, samples = 0.2 ms). We used the times of the detected stimulus artifact as triggers to define sweeps with 
 post-stimulus recording sweep length 50 ms for medianus SEP and 100 ms for tibial SEP. We resampled the data to sampling rate 1200 Hz before
 further processing. We classified sweeps with amplitude > 10 µV as artefact-ridden and excluded them from further analysis.""
 

 ## BIDS Conversion
 bids-starter-kid and custom Matlab scripts were used to convert the dataset into BIDS format. 
 

 

 ## References
 - [1] Dimakopoulos V, Selmin G, Regli L, Sarnthein J, Optimization of signal-to-noise ratio in short-duration SEP recordings by variation of stimulation rate, Clinical Neurophysiology, 2023, ISSN 1388-2457, https://doi.org/10.1016/j.clinph.2023.03.008.",1,1,0
ds004395,2023-01-10 18:29:34,Joseph Rudoler,2.0.0,Penn Electrophysiology of Encoding and Retrieval Study (PEERS),2023-06-01 3:00:26,0,24,9583320000000,9.6 TB,62638,364,0,0,1.6.0,CC0,"Michael J. Kahana, Joseph H. Rudoler, Lynn J. Lohnas, Karl Healey, Ada Aka, Adam Broitman, Elizabeth Crutchley, Patrick Crutchley, Kylie H. Alm, Brandon S. Katerman, Nicole E. Miller, Joel R. Kuhn, Yuxuan Li, Nicole M. Long, Jonathan Miller, Madison D. Paron, Jesse K. Pazdera, Isaac Pedisich, Christoph T. Weidemann",,,,,doi:10.18112/openneuro.ds004395.v2.0.0,,"ltpFR, ltpFR2, VFFR",,EEG,"The Penn Electrophysiology of Encoding and Retrieval Study (PEERS) aimed to characterize the behavioral and electrophysiological (EEG) correlates of memory encoding and retrieval in highly practiced individuals. Across five PEERS experiments, 300+ subjects contributed more than 7,000 90 minute memory testing sessions with recorded EEG data.
 

 See the Computational Memory Lab's [wiki page](https://memory.psych.upenn.edu/PEERS) for more detailed information, and [this paper](https://psyarxiv.com/bu5x8/) for a discussion of the main findings and lessons learned from this large-scale study.
 

 This dataset contains 3 experiments:
 * ltpFR (a.k.a. PEERS1-3)
 * ltpFR2 (a.k.a. PEERS4)
 * VFFR (a.k.a. PEERS5)
 

 Electroencephalogram (EEG) data were recorded with either a 129-channel Geodesic Sensor Net (either GSN 200 model or HydroCel GSN model) using the Netstation acquisition environment (Electrical Geodesics, Inc.; EGI) or with a 128-channel BioSemi headcap using the Biosemi ActiveTwo acquisition system. 
 **Note:** subject-specific electrode layouts were NOT recorded. Despite being labeled as ""CapTrak"" space, the coordinates reflect a generic electrode layout for a given headcap and do NOT represent any individual's head shape. 
 

 

 References
 ----------
 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G., Phillips, C., Delorme, A., Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Scientific Data, 6, 103. https://doi.org/10.1038/s41597-019-0104-8",1,1,0
ds003568,2021-03-16 15:16:54,Lucrezia Liuzzi,1.0.4,Mood induction in MDD and healthy adolescents,2021-03-16 21:47:34,1,1,132520000000,123.4 GB,3707,51,0,0,1.2.0,CC0,"Lucrezia Liuzzi, Katharine Chang, Hanna Keren, Charles Zheng, Dipta Saha, Dylan Nielson, Argyris Stringaris",This research was supported by the Intramural Research Program of the National Institute of Mental Health National Institutes of Health (NIH). This work used the computational resources of the NIH HPC (high-performance computing) Biowulf cluster (http://hpc.nih.gov).,"Please cite: Lucrezia Liuzzi, Katharine Chang, Charles Zheng, Hanna Keren, Dipta Saha, Dylan Nielson and Argyris Stringaris, Magnetoencephalographic Correlates of Mood and Reward Dynamics in Human Adolescents, Cerebral Cortex 2021",Intramural Research Program of the National Institute of Mental Health National Institutes of Health (NIH) (Grant No. ZIA-MH002957-01 [to AS]),https://doi.org/10.1093/cercor/bhab417,doi:10.18112/openneuro.ds003568.v1.0.4,NIH Institutional Review Board,"mmi3, rest",,"MEG, MRI","This dataset contains the MEG and structural MRI data from the ""Electrophysiological correlates of mood and reward dynamics in human adolescents"" pre-registered analysis (https://www.biorxiv.org/content/10.1101/2021.03.04.433969v1).
  
 The task-mmi3 data corresponds to the monetary gambling mood induction task described in the paper. Task-mmi3 data has been pre-processed marking bad channels and bad segments (motion > 5mm or/and noise artifacts). 
 Task-rest data is unprocessed 10 minutes resting state scan acquired during the same scanning session. 
 

 Anatomical MRIs have been defaced and co-registered fiducial coordinates are available in the anatomical json files. 
 

 Data from four confirmatory subjects are not made available because of missing data sharing consent.
 sub-22658 and sub-24247 do not have an available structural scan.",1,0,0
ds004408,2023-01-17 16:49:36,Ole Bialas,1.0.8,Invariant encoding of phonemes in neural responses to continuous speech,2023-01-20 15:17:31,0,1,20083200000,0.02 TB,1946,19,0,0,1.7.0,CC0,"Giovanni M Di Liberto, Michael P Broderick, Ole Bialas, Edmund C Lalor",,,,,doi:10.18112/openneuro.ds004408.v1.0.8,,listening,,EEG,"The data in one study [^1] and then added to by another [^2] and contains EEG responses of healthy, neurotypical adults who listened to naturalistic speech. The subjects listened to segments from an audio book version of ""The Old Man and the Sea"" and their brain activity was recorded using a 128-channel ActiveTwo EEG system (BioSemi). 
 

 The stimuli folder contains .wav files of the presented audiobook segments as well as a .TextGrid file for each segment, containng the timing of words and phonemes in that segment. The text grids were generated using the forced-alignment software Prosodylab-Aligner [^3] and inspected by eye. Each subject's folder contains one EEG-recording per audio segment and their starts are aligned (the EEG recordings are longer than the audio to a varying extent). The recordings are unfiltered, unreferenced and sampled at 512 Hz.
 

 [^1]: Di Liberto, G. M., O’sullivan, J. A., & Lalor, E. C. (2015). Low-frequency cortical entrainment to speech reflects phoneme-level processing. Current Biology, 25(19), 2457-2465.
 

 [^2]: Broderick, M. P., Anderson, A. J., Di Liberto, G. M., Crosse, M. J., & Lalor, E. C. (2018). Electrophysiological correlates of semantic dissimilarity reflect the comprehension of natural, narrative speech. Current Biology, 28(5), 803-809.
 

 [^3]: Gorman, K., Howell, J., & Wagner, M. (2011). Prosodylab-aligner: A tool for forced alignment of laboratory speech. Canadian Acoustics, 39(3), 192-193.",1,1,0
ds004444,2023-01-25 11:39:51,Seitaro Iwama,1.0.1,The BMI-HDEEG dataset 1,2023-01-25 12:35:02,0,16,52205000000,0.052 TB,2332,30,18,27,1.7.0,CC0,"Seitaro Iwama, Masumi Morishige, Yoshikazu Takahashi, Ryotaro Hirose, Midori Kodama, Junichi Ushiba",,Reference,,,doi:10.18112/openneuro.ds004444.v1.0.1,,smrbmi,,EEG,"Data Descriptor Article
 

 Iwama, S., Morishige, M., Kodama, M. et al. High-density scalp electroencephalogram dataset during sensorimotor rhythm-based brain-computer interfacing. Sci Data 10, 385 (2023). https://doi.org/10.1038/s41597-023-02260-6
 

 Sample code
 https://github.com/Junichi-Ushiba-Laboratory/pj-hd-smrbmi",1,1,0
ds004446,2023-01-25 13:06:48,Seitaro Iwama,1.0.1,The BMI-HDEEG dataset 2,2023-01-25 16:30:22,0,8,31383000000,0.031 TB,1192,30,18,27,1.7.0,CC0,"Seitaro Iwama, Masumi Morishige, Yoshikazu Takahashi, Ryotaro Hirose, Midori Kodama, Junichi Ushiba",,Reference,JST Moonshot R&D JPMJMS2012,,doi:10.18112/openneuro.ds004446.v1.0.1,,smrbmi,,EEG,"Data Descriptor Article
 

 

 Iwama, S., Morishige, M., Kodama, M. et al. High-density scalp electroencephalogram dataset during sensorimotor rhythm-based brain-computer interfacing. Sci Data 10, 385 (2023). https://doi.org/10.1038/s41597-023-02260-6
 

 Sample code
 

 

 https://github.com/Junichi-Ushiba-Laboratory/pj-hd-smrbmi",1,1,0
ds004447,2023-01-25 15:44:37,Seitaro Iwama,1.0.1,The BMI-HDEEG dataset 3,2023-01-26 5:19:08,0,20,22253500000,0.022 TB,2094,22,18,27,1.7.0,CC0,"Seitaro Iwama, Masumi Morishige, Yoshikazu Takahashi, Ryotaro Hirose, Midori Kodama, Junichi Ushiba",,Reference,JST Moonshot R&D JPMJMS2012,,doi:10.18112/openneuro.ds004447.v1.0.1,,smrbmi,,EEG,"Data Descriptor Article
 

 

 Iwama, S., Morishige, M., Kodama, M. et al. High-density scalp electroencephalogram dataset during sensorimotor rhythm-based brain-computer interfacing. Sci Data 10, 385 (2023). https://doi.org/10.1038/s41597-023-02260-6
 

 Sample code
 

 

 https://github.com/Junichi-Ushiba-Laboratory/pj-hd-smrbmi",1,1,0
ds004448,2023-01-25 16:31:27,Seitaro Iwama,1.0.2,The BMI-HDEEG dataset 4,2023-01-26 5:19:30,0,5,40980900000,0.041 TB,1407,56,19,58,1.7.0,CC0,"Seitaro Iwama, Masumi Morishige, Yoshikazu Takahashi, Ryotaro Hirose, Midori Kodama, Junichi Ushiba",,Reference,,,doi:10.18112/openneuro.ds004448.v1.0.2,,smrbmi,,EEG,"Data Descriptor Article
 

 

 Iwama, S., Morishige, M., Kodama, M. et al. High-density scalp electroencephalogram dataset during sensorimotor rhythm-based brain-computer interfacing. Sci Data 10, 385 (2023). https://doi.org/10.1038/s41597-023-02260-6
 

 Sample code
 

 

 https://github.com/Junichi-Ushiba-Laboratory/pj-hd-smrbmi",1,1,0
,0000-00-00 00:00:00,,,,0000-00-00 00:00:00,0,1,0,0 GB,0,0,0,0,,,,,,,,,,,,,,1,0,0
ds003708,2021-06-23 18:46:01,Dora Hermes,1.0.4,Basis profile curve identification to understand electrical stimulation effects in human brain networks,2021-06-23 18:53:45,0,1,650274000,0.606 GB,278,1,0,0,v 1.3.0,CC0,"Dora Hermes, Gabriella Ojeda, Kai J. Miller, Multimodal Neuroimaging Laboratory at Mayo Clinic","Nicholas Gregg, Brian Lundstom, Kai Miller, Gregory Worrell, Cindy Nelson","This dataset is part of the paper on 'Basis profile curve identification to understand electrical stimulation effects in human brain networks' by Miller, Mueller and Hermes, 2021, https://www.biorxiv.org/content/10.1101/2021.01.24.428020v1.full. This project is funded by the National Institute Of Mental Health of the National Institutes of Health under Award Number R01MH122258 to Dora Hermes (Mayo Clinic). The data was collected by Dora Hermes (Mayo Clinic), Nick Gregg (Mayo Clinic), Brian Lundstrom (Mayo Clinic) and Cindy Nelson (Mayo Clinic). The BIDS formatting was performed by Dora Hermes (Mayo Clinic) and Gabriella Ojeda Valencia (Mayo Clinic). The iEEG data collection was facilitated by Gregory Worrell and Kai J. Miller (Mayo Clinic)",R01 MH122258 CRCNS: Processing speed in the human connectome across the lifespan,https://doi.org/10.1101/2021.01.24.428020,doi:10.18112/openneuro.ds003708.v1.0.4,,rest,,iEEG,"--------
 This dataset contains intracranial EEG recordings from one patient during single pulse electrical stimulation. These data were recorded at the Mayo Clinic in Rochester, MN, as part of the NIH Brain Initiative supported project R01 MH122258 ""CRCNS: Processing speed in the human connectome across the lifespan"". 
 

 The overarching goal of this project is to develop a large database of single pulse stimulation data and develop tools to advance our understanding of the human connectome across the lifespan. 
 

 

 Citing this dataset
 -------------------
 This dataset is part of the paper on 'Basis profile curve identification to understand electrical stimulation effects in human brain networks' by Miller, Mueller and Hermes, 2021, https://www.biorxiv.org/content/10.1101/2021.01.24.428020v1.full. Research reported in this publication was supported by the National Institute Of Mental Health of the National Institutes of Health under Award Number R01MH122258. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. The data was collected by Dora Hermes, Nick Gregg, Brian Lundstrom, Cindy Nelson, Gregg Worrell and Kai J. Miller. The BIDS formatting was performed by Dora Hermes and Gabriella Ojeda Valencia. 
 

 

 

 Format
 ------
 It is formatted according to BIDS version 1.3.0
 

 

 Details about the single pulse stimulation experiment
 ----------------------------
 Patients were resting in the hospital bed, while single pulse stimulation was performed with a frequency of ~0.2 Hz. The stimulation had a duration of 200 microseconds, was biphasic and had an amplitude of 6mA. On the motor cortex stimulation amplitude was sometimes reduced to 1 or 2mA to minimize movement artifacts.  
 

 

 Contact
 ----------------------------
 Please contact Dora Hermes (hermes.dora@mayo.edu) for questions.",1,0,0
ds004460,2023-02-02 13:27:50,Sein Jeung,1.1.0,EEG and motion capture data set for a full-body/joystick rotation task,2023-02-03 9:56:13,0,2,63428700000,59.1 GB,346,20,21,34,n/a,CC0,"Gramann, K., Hohlefeld, F.U., Gehrke, L., Klug, M",We gratefully acknowledge the help of Jonna Jürs and Yiru Chen with the data acquisition and pre-processing.,"Please cite as ""Gramann, K., Hohlefeld, F. U., Gehrke, L., & Klug, M. (2021). Human cortical dynamics during full-body heading changes. Scientific Reports, 11(1), 18186""","the German Research Foundation (DFG) Grant No. GR 2627/8-1, Open Access funding enabled and organized by Projekt DEAL.",Human cortical dynamics during full-body heading changes ===NEMAR-SEP=== https://doi.org/10.1038/s41598-021-97749-8,doi:10.18112/openneuro.ds004460.v1.1.0,"The experimental procedures were approved by the local ethics committee (Technische Universität Berlin, Germany) and the research was performed in accordance with the ethics guidelines. The study was conducted in accordance to the Declaration of Helsinki and all participants signed a written informed consent.",Rotation,,EEG,"An EEG + motion capture data set, analyzed and published in ""Gramann, K., Hohlefeld, F. U., Gehrke, L., & Klug, M. (2021). Human cortical dynamics during full-body heading changes. Scientific Reports, 11(1), 18186"".
 

 Used as a BIDS-example data set for EEG + motion : https://github.com/bids-standard/bids-examples/tree/master/motion_spotrotation
 

 Overview
 --------
 This is the ""Spot rotation"" dataset.
 It contains EEG and motion data collected from 20 subjects
 collected at the Berlin Mobile Brain-Body Imaging Lab,
 while they rotated their heading in physical space or on flat screen using a joystick.
 Detailed description of the paradigm can be found in the following reference:
 

 Gramann.K, Hohlefeld, F. U., Gehrke, L., and Klug, M. 
 ""Human cortical dynamics during full-body heading changes"".
 Scientific Reports 11, 18186 (2021). 
 https://doi.org/10.1038/s41598-021-97749-8
 

 

 

 Citing this dataset
 -------------------
 Please cite as follows:
 

 Gramann, K., Hohlefeld, F.U., Gehrke, L. et al. Human cortical dynamics during full-body heading changes. Sci Rep 11, 18186 (2021). https://doi.org/10.1038/s41598-021-97749-8
 For more information, see the `dataset_description.json` file.
 

 

 License
 -------
 This motion_spotrotation dataset is made available under the Creative Commons CC0 license. 
 Information on CC0 can be found here : https://creativecommons.org/share-your-work/public-domain/cc0/
 

 

 Format
 ------
 The dataset is formatted according to the Brain Imaging Data Structure. See the
 `dataset_description.json` file for the specific version used.
 

 Generally, you can find data in the .tsv files and descriptions in the
 accompanying .json files.
 

 An important BIDS definition to consider is the ""Inheritance Principle"", which
 is described in the BIDS specification under the following link:
 

 https://bids-specification.rtfd.io/en/stable/02-common-principles.html#the-inheritance-principle
 

 The section states that:
 

 > Any metadata file (such as .json, .bvec or .tsv) may be defined at any directory level,
 > but no more than one applicable file may be defined at a given level [...]
 > The values from the top level are inherited by all lower levels unless
 > they are overridden by a file at the lower level.
 

 

 Details about the experiment
 ----------------------------
 For a detailed description of the task, see Gramann et al. (2021).
 What follows is a brief summary.
 

 Data were collected from 20 healthy adults (11 females) with a mean age of 30.25 years 
 (SD = 7.68, ranging from ages 20 to 46) who received 10€/h or course credit for compensation. 
 All participants reported normal or corrected to normal vision and no history of neurological disease. 
 Eighteen participants reported being right-handed (two left-handed). 
 

 To control for the effects of different reference frame proclivities on neural dynamics, 
 the online version of the spatial reference frame proclivity test (RFPT44, 45) 
 was administered prior to the experiment. 
 Participants had to consistently use an ego- or allocentric reference frame 
 in at least 80% of their responses. 
 Of the 20 participants, nine preferentially used an egocentric reference frame, 
 nine used an allocentric reference frame, and two used a mixed strategy. 
 One participant (egocentric reference frame) dropped out of the experiment 
 after the first block due to motion sickness and was removed from further data analyses. 
 The reported results are based on the remaining 19 participants. 
 The experimental procedures were approved by the local ethics committee 
 (Technische Universität Berlin, Germany) 
 and the research was performed in accordance with the ethics guidelines. 
 The study was conducted in accordance to the Declaration of Helsinki 
 and all participants signed a written informed consent. 
 

 Participants performed a spatial orientation task in a sparse virtual environment 
 (WorldViz Vizard, Santa Barbara, USA) consisting of an infinite floor granulated in green and black.
 The experiment was self-paced and participants advanced the experiment 
 by starting and ending each trial with a button press using the index finger of the dominant hand.
 A trial started with the onset of a red pole, which participants had to face and align with.
 Once the button was pressed the pole disappeared 
 and was immediately replaced by a red sphere floating at eye level. 
 The sphere automatically started to move around the participant 
 along a circular trajectory at a fixed distance (30 m) 
 with one of two different velocity profiles. 
 Participants were asked to rotate on the spot and to follow the sphere, 
 keeping it in the center of their visual field (outward rotation). 
 The sphere stopped unpredictably at varying eccentricity between 30° and 150° and turned blue, 
 which indicated that participants had to rotate back to the initial heading (backward rotation). 
 When participants had reproduced their estimated initial heading, 
 they confirmed their heading with a button press and the red pole reappeared for reorientation.
 

 The participants completed the experimental task twice, 
 using (i) a traditional desktop 2D setup (visual flow controlled through joystick movement; “joyR”), 
 and (ii) equipped with a MoBI setup 
 (visual flow controlled through active physical rotation with the whole body; “physR”). 
 The condition order was balanced across participants. 
 To ensure the comparability of both rotation conditions, 
 participants carried the full motion capture system at all times. 
 In the joyR condition participants stood in the dimly lit experimental hall in front of a standard TV monitor 
 (1.5 m viewing distance, HD resolution, 60 Hz refresh rate, 40´´ diagonal size) 
 and were instructed to move as little as possible. 
 They followed the sphere by tilting the joystick 
 and were thus only able to use visual flow information to complete the task. 
 In the physical rotation condition participants were situated in a 3D virtual reality environment 
 using a head mounted display (HTC Vive; 2 × 1080 × 1200 resolution, 90 Hz refresh rate, 110° field of view). 
 Participants’ movements were unconstrained, 
 i.e., in order to follow the sphere they physically rotated on the spot, 
 thus enabling them to use motor and kinesthetic information (i.e., vestibular input and proprioception) 
 in addition to the visual flow for completing the task. 
 If participants diverged from the center position as determined through motion capture of the head position, 
 the task automatically halted and participants were asked to regain center position, 
 indicated by a yellow floating sphere, before continuing with the task. 
 Each movement condition was preceded by recording a three-minute baseline, 
 during which the participants were instructed to stand still and to look straight ahead.
 

 Data Recordings: EEG. 
 EEG data was recorded from 157 active electrodes with a sampling rate of 1000 Hz 
 and band-pass filtered from 0.016 Hz to 500 Hz (BrainAmp Move System, Brain Products, Gilching, Germany). 
 Using an elastic cap with an equidistant design (EASYCAP, Herrsching, Germany), 
 129 electrodes were placed on the scalp, and 28 electrodes were placed around the neck 
 using a custom neckband (EASYCAP, Herrsching, Germany) in order to record neck muscle activity. 
 Data were referenced to an electrode located closest to the standard position FCz. 
 Impedances were kept below 10k? for standard locations on the scalp, and below 50k? for the neckband. 
 Electrode locations were digitized using an optical tracking system (Polaris Vicra, NDI, Waterloo, ON, Canada).
 

 Data Recordings: Motion Capture. 
 Two different motion capture data sources were used: 19 red active light-emitting diodes (LEDs) were captured 
 using 31 cameras of the Impulse X2 System (PhaseSpace Inc., San Leandro, CA, USA) with a sampling rate of 90 Hz. 
 They were placed on the feet (2 x 4 LEDs), around the hips (5 LEDs), on the shoulders (4 LEDs), 
 and on the HTC Vive (2 LEDs; to account for an offset in yaw angle between the PhaseSpace and the HTC Vive tracking). 
 Except for the two LEDs on the HTC Vive, they were subsequently grouped together 
 to form rigid body parts of feet, hip, and shoulders, enabling tracking with 
 six degrees of freedom (x, y, and z position and roll, yaw, and pitch orientation) per body part. 
 Head motion capture data (position and orientation) was acquired using the HTC Lighthouse tracking system 
 with 90Hz sampling rate, since it was also used for the positional tracking of the virtual reality view. 
 

 The original data was recorded in `.xdf` format using labstreaminglayer
 (https://github.com/sccn/labstreaminglayer). It is stored in the `/sourcedata`
 directory. To comply with the BIDS format, the .xdf format was converted to
 BrainVision format (see the `.eeg` file for binary eeg data, the `.vhdr` as a
 text header filer containing meta data, and the `.vmrk` as a text file storing
 the eeg markers).",1,0,0
ds004473,2023-02-07 22:05:05,Alexander Rockhill,1.0.2,sEEG Forced Two-Choice Task,2023-02-08 0:47:59,0,1,6767570000,0.007 TB,126,8,26,47,1.9.2,CC0,"Alexander P. Rockhill, Alessandra Mantovani, Brittany Stedelin, Admed M. Raslan, Nicole C. Swann",,,,,doi:10.18112/openneuro.ds004473.v1.0.2,,SlowFast,,"iEEG, MRI","Welcome to our dataset! Here we present stereoelectroencephalography data from a forced two-choice response task collected in the epilepsy monitoring unit at Oregon Health & Science University. The data was analyzed in collaboration with the University of Oregon. The accompanying paper the first reference below.
 

 References
 ----------
 

 Rockhill, A. P., Mantovani, A., Stedelin, B., Nerison, C. S., Raslan, A. M., & Swann, N. C. (2022). Stereo-EEG recordings extend known distributions of canonical movement-related oscillations. Journal of Neural Engineering. https://doi.org/10.1088/1741-2552/acae0a
 

 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Holdgraf, C., Appelhoff, S., Bickel, S., Bouchard, K., D'Ambrosio, S., David, O., … Hermes, D. (2019). iEEG-BIDS, extending the Brain Imaging Data Structure specification to human intracranial electrophysiology. Scientific Data, 6, 102. https://doi.org/10.1038/s41597-019-0105-7",1,1,0
ds004475,2023-02-08 2:21:05,Noelle Jacobsen,1.0.3,,2023-07-31 17:42:53,0,1,52115300000,0.052 TB,246,30,0,0,1.1.1,CC0,"Noelle A. Jacobsen, Daniel P. Ferris",Noelle A. Jacobsen and Daniel P. Ferris (2023). Mobile EEG split-belt walking study. OpenNeuro. [Dataset] doi:10.18112/openneuro.ds004475.v1.0.0,Noelle A. Jacobsen and Daniel P. Ferris (2023). Mobile EEG split-belt walking study. OpenNeuro. [Dataset] doi:10.18112/openneuro.ds004475.v1.0.0,NIH R01-NS104772 ===NEMAR-SEP=== T32-NS082128,"Jacobsen, N. A., & Ferris, D. P. (2023). Electrocortical activity correlated with locomotor adaptation during split-belt treadmill walking. The Journal of Physiology. https://doi.org/10.1113/JP284505",doi:10.18112/openneuro.ds004475.v1.0.3,IRB201701603,task,,"EEG, MRI",This mobile brain body imaging (MoBI) experiment investigates brain activity correlated to gait adaptation during split-belt treadmill walking. 30 participants completed an abrupt and gradual split-belt walking paradigm (2:1 belt speed ratio).,1,1,0
ds004477,2023-02-08 10:52:55,Rodrigo Ramele,1.0.2,PES - Pandemic Emergency Scenario,2023-02-08 16:28:07,0,1,23989900000,0.024 TB,68,9,0,0,1.7.0,CC0,"Tasos Papastylianou, Rodrigo Ramele, Luca Citi, Caterina Cinel, Riccardo Poli",This research was supported by the Defence Science and Technology Laboratory (Dstl) on behalf of the UK Ministry of Defence (MOD) via funding from US/UK DoD Bilateral Academic Research Initiative (BARI).,,UK Ministry of Defence (MOD) via funding from US/UK DoD BARI,https://www.essex.ac.uk/research-projects/adaptive-joint-cognitive-systems ===NEMAR-SEP=== https://www.defense.gov/News/Releases/Release/Article/1644263/dod-announces-bari-award-for-us-uk-collaboration-on-human-machine-teaming/,doi:10.18112/openneuro.ds004477.v1.0.2,"The study complied at all times with the Declaration of Helsinki ethical guidelines for research involving human subjects; formal ethical approval was granted by the Ministry of Defence Research Ethics Committee MoDREC - Application No: 983/MoDREC/19 first approved on 5th September 2019, with revisions (ver. 3) approved on the 3rd of June 2021.",PES,,EEG,"Experiment:
 

 PES is a complex and strategic decision-making ""Pandemic"" Experiment. In this experiment, users were shown a map that gives a description of the spread of a pandemic emergency situation in various locations within the map. Resources (in terms, medicines, personnels) are allocated to few cities in the beginning. The user must allocate more resources to new cities that are displayed on the map. The user must keep in mind that the resources are limited and handing over all resources could mean that new cities (if displayed) might not get any resources.
 

 In this experiment, 9 participants are paired with an artificial agent and they have to decide resource allocation on this scenario, providing their reported confidences for each decision. The experiment is divided in 64 sequences.
 

 Neurophysiological markers and behavioural information is obtained for each participant as they provide the number of allocated resources and their own subjective perception of the accuracy of each response for each trial. There is a span of 10 seconds where the Participant can press the mouse button (the Hold Response event), drag the mouse upwards while keeping the mouse-button pressed, thereby increasing the number of plus symbols that appear around the city icon, or downwards to decrease them, and finally release the mouse button when the decision is made (the Release Response event). Immediately after that, there is an additional span of 5 seconds where the participant reports the confidence in their decision by moving the mouse wheel. After that (the End-of-trial event) a black screen replaces the map, and the responses from the other players are shown for 2 seconds.
 Each participant sat comfortably at about 1 meter from an LCD monitor; each participant wore an EEG cap connected to a Biosemi ActiveTwo system. Wet electrodes were used and recordings were performed with 64 electrodes in the International 10-20 System. Eight additional external channels were also included, two measuring the electrocardiogram (ECG), while 4 measured the electrooculogram (EOG) signal. The EEG data was sampled at 2048 Hz.
 

 Ethical Statement:
 

 The study complied at all times with the Declaration of Helsinki ethical guidelines for research involving human subjects; formal ethical approval was granted by the Ministry of Defence Research Ethics Committee MoDREC – Application No: 983/MoDREC/19 first approved on 5th September 2019, with revisions (ver. 3) approved on the 3rd of June 2021.
 

 

 Acknowledgment:
 

 This research was supported by the Defence Science and Technology Laboratory (Dstl) on behalf of the UK Ministry of Defence (MOD) via funding from US/UK DoD Bilateral Academic Research Initiative (BARI).
 

 Code: https://github.com/BCI-NE/PES",1,1,0
ds004483,2023-02-08 23:22:49,Fosca Al Roumi,1.0.0,ABSeqMEG,2023-07-07 16:31:14,0,1,25146600000,0.025 TB,1384,19,0,0,1.6.0,CC0,"Samuel Planton*, Fosca Al Roumi*, Liping Wang, Stanislas Dehaene",,Please cite our preprint at https://www.biorxiv.org/content/10.1101/2022.10.15.512361v1 or the published version of this paper,"European Research Council (ERC, https://erc.europa.eu/) grant to S.D. (NeuroSyntax, ID: 695403).",,doi:10.18112/openneuro.ds004483.v1.0.0,,abseq,,MEG,"This dataset contains the MEG data from the article entitled Compression of binary sound sequences in human working memory https://www.biorxiv.org/content/10.1101/2022.10.15.512361v1 
 

 According to the language of thought hypothesis, regular sequences are compressed in human working memory using recursive loops akin to a mental program that predicts future items. We tested this theory by probing working memory for 16-item sequences made of two sounds. We recorded brain activity with functional MRI and magneto-encephalography (MEG) while participants listened to a hierarchy of sequences of variable complexity, whose minimal description required transition probabilities, chunking, or nested structures. Occasional deviant sounds probed the participants’ knowledge of the sequence. We predicted that task difficulty and brain activity would be proportional to minimal description length (MDL) in our formal language. Furthermore, activity should increase with MDL for learned sequences, and decrease with MDL for deviants. These predictions were upheld in both fMRI and MEG, indicating that sequence predictions are highly dependent on sequence structure and become weaker and delayed as complexity increases. The proposed language recruited bilateral superior temporal, precentral, anterior intraparietal and cerebellar cortices. These regions overlapped extensively with a localizer for mathematical calculation, and much less with spoken or written language processing. We propose that these areas collectively encode regular sequences as repetitions with variations and their recursive composition into nested structures.",1,1,0
ds004502,2023-02-16 21:52:37,José M. G. Peñalver,1.0.1,Anticipatory differences between Attention and Expectation,2023-02-16 23:40:24,0,1,63828200000,0.064 TB,319,48,18,28,1.2,CC0,"Jose M. G. Penalver, David Lopez-Garcia, Blanca Aguado-Lopez, Carlos Gonzalez-Garcia, Maria Ruz",,,,,doi:10.18112/openneuro.ds004502.v1.0.1,,attexp,,EEG,,1,1,0
ds004504,2023-02-17 8:35:28,??????? ??????????,1.0.7,"A dataset of 88 EEG recordings from: Alzheimer's disease, Frontotemporal dementia and Healthy subjects",2023-02-17 8:53:52,0,1,2829900000,2.6 GB,269,88,0,0,v1.2.1,CC0,"Andreas Miltiadous, Katerina D. Tzimourta, Theodora Afrantou, Panagiotis Ioannidis, Nikolaos Grigoriadis, Dimitrios G. Tsalikakis, Pantelis Angelidis, Markos G. Tsipouras, Evripidis Glavas, Nikolaos Giannakeas, Alexandros T. Tzallas","We aknowledge support of the 2nd Department of Neurology of AHEPA General University Hospital of Thessaloniki.
 

 We acknowledge support of this work from the project “Immersive Virtual, Augmented and Mixed Reality Center of Epirus” (MIS 5047221) which is implemented under the Action “Reinforcement of the Research and Innovation Infrastructure”, funded by the Operational Programme “Competitiveness, Entrepreneurship and Innovation” (NSRF 2014-2020) and co-financed by Greece and the European Union (European Regional Development Fund).","Please cite:
 Data descriptor: 10.3390/data8060095
 First study on this dataset: 10.1109/ACCESS.2023.3294618",,"Miltiadous, A., Tzimourta, K. D., Afrantou, T., Ioannidis, P., Grigoriadis, N., Tsalikakis, D. G., Angelidis, P., Tsipouras, M. G., Glavas, E., Giannakeas, N., & Tzallas, A. T. (2023). A Dataset of Scalp EEG Recordings of Alzheimer’s Disease, Frontotemporal Dementia and Healthy Subjects from Routine EEG. Data, 8(6), 95. doi: 10.3390/data8060095 ===NEMAR-SEP=== Miltiadous, A., Gionanidis, E., Tzimourta, K. D., Giannakeas, N., & Tzallas, A. T. (2023). DICE-net: A Novel Convolution-Transformer Architecture for Alzheimer Detection in EEG Signals. IEEE Access, 1–1. doi: 10.1109/ACCESS.2023.3294618",doi:10.18112/openneuro.ds004504.v1.0.7,"The study was conducted in accordance with the Declaration of Helsinki and approved by the Scientific and Ethics Committee of AHEPA University Hospital, Aristotle University of Thessaloniki, under protocol number 142/12-04-2023.",eyesclosed,,EEG,"This dataset contains the EEG resting state-closed eyes recordings from 88 subjects in total.
 

 Participants: 36 of them were diagnosed with Alzheimer's disease (AD group), 23 were diagnosed with Frontotemporal Dementia (FTD group) and 29 were healthy subjects (CN group).
 Cognitive and neuropsychological state was evaluated by the international Mini-Mental State Examination (MMSE). MMSE score ranges from 0 to 30, with lower MMSE indicating more severe cognitive decline.
 The duration of the disease was measured in months and the median value was 25 with IQR range (Q1-Q3) being 24 - 28.5 months.
 Concerning the AD groups, no dementia-related comorbidities have been reported. The average MMSE for the AD group was 17.75 (sd=4.5), for the FTD group was 22.17 (sd=8.22) and for the CN group was 30.
 The mean age of the AD group was 66.4 (sd=7.9), for the FTD group was 63.6 (sd=8.2), and for the CN group was 67.9 (sd=5.4).
 

 Recordings: Recordings were aquired from the 2nd Department of Neurology of AHEPA General Hispital of Thessaloniki by an experienced team of neurologists. For recording, a Nihon Kohden EEG 2100 clinical device was used,
 with 19 scalp electrodes (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, and O2) according to the 10-20 international system and 2 reference electrodes (A1 and A2) placed on the mastoids for impendance check, according to the manual of the device. Each recording was performed according to the clinical protocol with participants being in a sitting position having their eyes closed.
 Before the initialization of each recording, the skin impedance value was ensured to be below 5k?. The sampling rate was 500 Hz with 10uV/mm resolution.
 The recording montages were anterior-posterior bipolar and referential montage using Cz as the common reference. The referential montage was included in this dataset.
 The recordings were received under the range of the following parameters of the amplifier: Sensitivity: 10uV/mm, time constant: 0.3s, and high frequency filter at 70 Hz.
 Each recording lasted approximately 13.5 minutes for AD group (min=5.1, max=21.3), 12 minutes for FTD group (min=7.9, max=16.9) and 13.8 for CN group (min=12.5, max=16.5).
 In total, 485.5 minutes of AD, 276.5 minutes of FTD and 402 minutes of CN recordings were collected and are included in the dataset.
 

 Preprocessing: The EEG recordings were exported in .eeg format and are transformed to BIDS accepted .set format for the inclusion in the dataset.
 Automatic annotations of the Nihon Kohden EEG device marking artifacts (muscle activity, blinking, swallowing) have not been included for language compatibility purposes
 (If this is an issue, please use the preprocessed dataset in Folder: derivatives).
 The unprocessed EEG recordings are included in folders named: sub-0XX. Folders named sub-0XX in the subfolder derivatives contain the preprocessed and denoised EEG recordings.
 The preprocessing pipeline of the EEG signals is as follows. First, a Butterworth band-pass filter 0.5-45 Hz was applied and the signals were re-referenced to A1-A2.
 Then, the Artifact Subspace Reconstruction routine (ASR) which is an EEG artifact correction method included in the EEGLab Matlab software was applied to the signals,
 removing bad data periods which exceeded the max acceptable 0.5 second window standard deviation of 17, which is considered a conservative window.
 Next, the Independent Component Analysis (ICA) method (RunICA algorithm) was performed, transforming the 19 EEG signals to 19 ICA components.
 ICA components that were classified as “eye artifacts” or “jaw artifacts” by the automatic classification routine “ICLabel” in the EEGLAB platform were automatically rejected.
 It should be noted that, even though the recording was performed in a resting state, eyes-closed condition, eye artifacts of eye movement were still found at some EEG recordings.
 

 A complete analysis of this dataset can be found in the published Data Descriptor paper ""A Dataset of Scalp EEG Recordings of Alzheimer’s Disease, Frontotemporal Dementia and Healthy Subjects from Routine EEG"", https://doi.org/10.3390/data8060095",1,0,0
ds004370,2022-12-16 14:47:48,Epilab UMCU,1.0.2,PRIOS,2024-03-05 16:27:50,0,1,29584400000,27.6 GB,130,7,13,53,Brain Imaging Data Structure Specification v1.6.0,CC0,"van Blooijs D, Blok S, Huiskamp GJM, Leijten FSS",persons to acknowledge,"D. van Blooijs, S. Blok, G.J.M. Huiskamp, P. van Eijsden, H.G.E. Meijer, F.S.S. Leijten The effect of propofol on local effective brain networks (submitted).",EpilepsieNL #17-07 ===NEMAR-SEP=== EpilepsieNL #19-12 ===NEMAR-SEP=== NIH RO1MH122258,see HowToAcknowledge,doi:10.18112/openneuro.ds004370.v1.0.2,"This study was approved by the Medical Ethics Committee in the UMC Utrecht, the Netherlands","task-SPESclin, task-SPESprop",,iEEG,"# Dataset description
 This dataset consists of 6 patients age 13-53 years old where Cortico-Cortical Evoked Potentials (CCEPs) were recorded with Electro-CorticoGraphy (ECoG) during single pulse electrical stimulation (SPES) in the awake patient for clinical routine (SPES-clinical) and under general propofol-anesthesia (SPES-propofol). 
 

 For a detailed description see:
 - The effect of propofol on local effective brain networks (submitted). D. van Blooijs, S. Blok, G.J.M. Huiskamp, P. van Eijsden, H.G.E. Meijer, F.S.S. Leijten
 

 The study was approved by the Medical Ethical Committee from the UMC Utrecht, the Netherlands.
 

 ## Contact 
 - Dorien van Blooijs: D.vanBlooijs@umcutrecht.nl
 - Frans Leijten: F.S.S.leijten@umcutrecht.nl
 

 

 # Data organization
 This data is organized according to the Brain Imaging Data Structure specification. A community-driven specification for organizing neurophysiology data along with its metadata. For more information on this data specification, see https://bids-specification.readthedocs.io/en/stable/ 
 

 Each patient has their own folder (e.g., `sub-PRIOS01` to `sub-PRIOS09`) which contains the iEEG recordings data for that patient, as well as the metadata needed to understand the raw data and event timing.
 

 Data are logically grouped in the same BIDS session and stored across runs indicating the day and time point of recording during the monitoring period.
 We use the optional run key-value pair to specify the day and the start time of the recording (e.g. run-021315, day 2 after implantation, which is day 1 of the monitoring period, at 13:15).
 The task key-value pair in long-term iEEG recordings describes the patient's state during the recording of this file. The task label is “SPESclin“ when these files contain data collected during clinical single pulse electrical stimulation (SPES) and ""SPESprop"" when these files contain data collected during single pulse electrical stimulation (SPES) in the operating room. 
 

 Electrode positions were estimated by running Freesurfer on the individual subject MRI scan. All shared electrode positions were converted to MNI305 space using the Freesurfer surface based non-linear transformation. We note that this surface based transformation distorts the dimensions of the grids, but maintains the gyral anatomy.
 

 # License
 This dataset is made available under the Public Domain Dedication and License CC v1.0, whose full text can be found at 
 https://creativecommons.org/publicdomain/zero/1.0/. 
 We hope that all users will follow the ODC Attribution/Share-Alike Community Norms (http://www.opendatacommons.org/norms/odc-by-sa/); 
 in particular, while not legally required, we hope that all users of the data will acknowledge by citing the following in any publication.
 The effect of propofol on local effective brain networks (submitted). D. van Blooijs, S. Blok, G.J.M. Huiskamp, P. van Eijsden, H.G.E. Meijer, F.S.S. Leijten
 

 # Code
 Code to analyses these data is available at: https://github.com/UMCU-EpiLAB/umcuEpi_PRIOS
 

 # Acknowledgements 
 We thank all patients for participating in this study. 
 

 # Funding
 Research reported in this publication was supported by EpilepsieNL under Award Number NEF17-07 (DvB) and NEF 19-12 (DvB, SB) and the National Institute of Mental Health of the National Institutes of Health under Award Number R01MH122258 (DvB, the content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health).",1,0,0
ds004505,2023-02-17 17:44:35,Amanda Studnicki,1.0.4,Real World Table Tennis,2023-03-30 17:04:08,0,1,37123200000,0.037 TB,230,25,0,0,1.1.1,CC0,"Amanda Studnicki, Daniel P. Ferris","Thank-you to the Human Neuromechanics Lab and to all the undergraduate research assistants who helped with data collections: Christina Collings, Corey Orlando, Sebastian Huerta, and Juan De La Espriella.",,National Science Foundation Division of Behavioral and Cognitive Sciences (BCS-1835317),https://doi.org/10.3390/s22155867,doi:10.18112/openneuro.ds004505.v1.0.4,University of Florida Institutional Review Board (IRB201801727),TableTennis,,"EEG, MRI","Our dataset contains high-density, dual-layer electroencephalography (EEG), neck electromyography (EMG), inertial measurement unit (IMU) acceleration, T1 structural MR images, and video data from 25 participants playing real-world table tennis. Participants played 60 minutes of table tennis (in total) with a ball machine and a human player, with an additional 10 minutes of standing baseline. For 17 of the participants, we also include video data of all trials. The Adobe Premiere project files (linked to each video) have the timing of hit events marked.
 

 Data in the main subject folders have been processed. We include the ICA decomposition and dipole model in EEG.etc. The components retained in our analyses are shown in EEG.etc.KeepComponents. The raw data can be found in the sourcedata folder.  
 

 Please refer to our publication for more details.",1,1,0
ds004511,2023-02-22 14:36:22,Anshu Te,1.0.2,Deception\_data,2023-02-24 9:56:26,0,2,217195000000,0.217 TB,993,45,20,43,1.8.0,CC0,"Makowski, Dominique, Pham, Tam, Lau, Zen Juen",Special thanks to An-Shu Te for helping with formatting this dataset in BIDS,,,,doi:10.18112/openneuro.ds004511.v1.0.2,,"CC, GG, Rest",,EEG,"# Overview
 

 This dataset was collected in 2020 and comprises electroencephalography, physiological and behavioural data. The dataset includes both resting-state (eyes closed) and task-related neurophysiological signals acquired from 44 healthy individuals (ages: 21-40). The tasks administered to subjects include a spontaneous deception task (Gambling Game; GG) as well as a task assessing cognitive control (CC).
 

 

 # Task Description
 ## Spontaneous Deception Task (GG)
 Participants were informed that the GG task aimed to study a player's behaviour during a gambling game. They were given SGD 50 at the start of the game. They were to undergo 144 rounds of making a prediction about the outcome of a dice roll. They were to also place a bet ranging from 10 cents to 80 cents for each prediction; they win the bet if the prediction was true and lose it if it was false.
 

 Participants were also informed that they were the only ones who knew the outcome of the dice roll and were responsible for reporting if their predictions were true to the system, and were debriefed at the end regarding this cover story.
 

 ## Cognitive Control (CC)
 Participants performed 60 trials of a simple processing speed task, 80 trials of a simple response selection task, 160 trials of a response inhibition task, and 160 trials of a conflict resolution task. See details of the task https://github.com/neuropsychology/CognitiveControl. 
 

 

 # Data acquisition
 ## EEG data acquisition
 

 EEG signals were recorded using the TruScan 128 Research EEG system and TruScan Aquisition software (DeyMed Diagnostics s.r.o). Electrodes were placed on the EEG cap according to the standard 10-5 system of electrode placement (Oostenveld & Praamsrta, 2001) and impedance was kept below 20 kOhm for each subject. The ground electrode was placed on the zygomatic bone and two electrodes were fixed on the mastoids to be used as references. During recording, the sampling rate was 3000Hz. Note that channels 124 and 125 were placed above and below the eyes respectively for vertical EOG signals.
 

 ### Note
 sub-S200203 does not have any EEG acquisition file pertaining to the Gambling Game task due to technical errors during the recording.
 

 

 ## Physiological data acquisition
 

 Participants' physiological signals, that is their electrocardiogram (*ECG*), respiration signals (*RSP*), electrodermal activity (*EDA*) and electromyography (*EMG*), were obtained at a sampling frequency of 4000Hz. All physiological signals were recorded via the BioPac MP160 system (BioPac Systems Inc., USA) and the AcqKnowledge 5.0 software.
 

 ECG was collected using three ECG electrodes placed according to a modified Lead II configuration, and RSP was acquired using a respiration belt tightened over participants' upper abdomen. EDA, a measure of skin conductance, was acquired using electrodes placed on the middle and index fingers of subjects' non-dominant hands and EMG was obtained by measuring the electrical activity of the corrugator muscles.
 

 ### Note
 With regards to the Cognitive Control task, physiological data was collected over 2 sessions for sub-S200303 as a result of technical errors during the recording.",1,1,0
ds004515,2023-03-01 21:24:04,James F Cavanagh,1.0.0,EEG: Alcohol imagery reinforcement learning task with light and heavy drinker participants,2023-06-09 21:11:02,0,1,10177200000,0.01 TB,1076,54,18,55,1.1.1,CC0,"Garima Singh, James F Cavanagh",,,,pending,doi:10.18112/openneuro.ds004515.v1.0.0,,ProbabilisticSelection,,EEG,"Affective state reinforcement learning task in N=54 Community participants. High and low drinkers. Data collected from 2019-2021 in the CRCL at UNM. The paper [Singh, G., Campbell, E., Hogeveen, J; Witkiewitz,K., Claus, E.D., & Cavanagh, J.F. Alcohol Imagery Boosts The Reward Positivity in Heavy Drinkers] Under review at the moment. Your best bet for understanding this task would be to read that paper first. - James F Cavanagh 08/02/2022",1,1,0
ds004519,2023-03-03 23:49:01,Edward Ester,1.0.1,EsterAndNouri2023,2023-03-04 1:33:53,0,1,13486800000,0.013 TB,286,40,0,0,1.8.0,CC0,"Edward Ester, Asal Nouri",,,,https://www.biorxiv.org/content/10.1101/2022.07.05.498906v4.abstract,doi:10.18112/openneuro.ds004519.v1.0.1,,ProAntiCue,8.1.0,EEG,"Preprocessed data files from ""Internal selective attention is delayed by competition between endogenous and exogenous factors"". A preprint describing the work can be found at https://www.biorxiv.org/content/10.1101/2022.07.05.498906v4.abstract, and analysis scripts can be found at https://osf.io/wat6d/. This study was conceptualized and analyzed before our lab made the switch to BIDS archival. If you want to use the analysis scripts linked above to analyze the BIDS data, you'll have to modify them to load the BIDS .set files rather than the .mat files we analyzed in our lab (the .set and .mat files, however, are identical). You will also need to modify the analysis scripts to load in the \_behavSummary.mat files for alignment with the EEG data. If you have questions or run into problems, please e-mail the corresponding author of the study (eester@unr.edu)""",1,1,0
ds004520,2023-03-04 23:57:19,Edward Ester,1.0.1,EsterPytel2023,2023-03-06 17:31:31,0,1,11175900000,0.011 TB,235,33,0,0,1.8.0,CC0,"Edward Ester, Paige Pytel",,,,https://www.biorxiv.org/content/10.1101/2022.08.12.503778v3.abstract,doi:10.18112/openneuro.ds004520.v1.0.1,,Retrocue,8.1.0,EEG,"Preprocessed data from Experiment 2 of Ester & Pytel ""Changes in behavioral priority influence the accessibility of working memory content"". Analytic scripts for this project can be found on OSF: https://osf.io/gtd5f/. Note that to analyze the BIDS data, you'll need to modify the analysis scripts to read in the BIDS .set files rather than the expected .mat files. See the OSF wiki for more information",1,1,0
ds004521,2023-03-05 1:39:56,Edward Ester,1.0.1,Changes in behavioral priority influence the accessibility of working memory content - Experiment 2,2023-03-07 18:35:41,0,1,11470000000,0.011 TB,242,34,0,0,1.8.0,CC0,"Edward Ester, Paige Pytel",,,,https://www.biorxiv.org/content/10.1101/2022.08.12.503778v3.abstract,doi:10.18112/openneuro.ds004521.v1.0.1,,Postcues,8.1.0,EEG,"Preprocessed data from Experiment 1 of Ester & Pytel ""Changes in behavioral priority influence the accessibility of working memory content"". Analytic scripts for this project can be found on OSF: https://osf.io/gtd5f/. Note that to analyze the BIDS data, you'll need to modify the analysis scripts to read in the BIDS .set files rather than the expected .mat files. See the OSF wiki for more information",1,1,0
ds004532,2023-03-15 18:50:32,James F Cavanagh,1.2.0,EEG: Probabilistic Selection Task (PST) + PST with Cabergoline Challenge,2023-03-16 16:17:50,0,2,23367800000,0.023 TB,1009,110,18,30,1.1.1,CC0,"James F Cavanagh, Michael J Frank",,,,PMID: 25367437,doi:10.18112/openneuro.ds004532.v1.2.0,,PST,,EEG,"Probabilistic Selection Task. Unpublished!  Same sample as this published study: 10.1038/ncomms6394.  
 

 Study 1: 80 healthy participants + 5 placebo session from a pilot of the drug study. Total n=85.  But Subj 173 might have bad EEG. 
 

 Study 2: 30 healthy participants (3 dropout) in a double-blind drug study. Total n=27.  Drug was Cabergoline 1.25 mg. 
 

 If you look in the code folder at the .xls sheet, you'll see that subjects had different initial IDs.  
 

 Study 1 subjects had subject IDs 101-180 plus the 5 placebo runs from an early test of ultra-low-dose cabergoline: these pilot runs were subject # 301/401 | 305/405.  Study 2 subjects had IDs 306/406 | 335/435.  Why the odd ranges for the drug study?  Glad you asked. The dual numbers were for session: 300s were first session, 400s were second session. The last two digits were subject ID. (here with the benefit of BIDS formatting we have simply put them in as session 1 and session 2 with unique sub-#, which is BETTER).  
 

 For example. Joe Smith would have been 305 on visit 1, then 405 on visit 2.  Jane Henderson would have been 306 on visit 1, then 406 on visit 2.  Whatever visit got cab or placebo is indicated in the .xls sheet as well as on the Sess1\_Drug and Sess2\_Drug columns in the main .tsv file. 
 

 Task included in Matlab programming language.  Data collected circa 2012-2013 in Laboratory for Neural Computation & Cognition at Brown. Check the .xls sheet under code folder for more meta data.  A few old analysis scripts are included. - James F Cavanagh 02/15/2021
 

 UPDATES: 
 1) Uploaded a .json sidecar developed by EEGLab for NEMAR indexing: task-PST\_events.json
 2) Since this was updated, I had to erase each subject's *\_events.json files. 
 3) Note that the Reward and Penalty feedback labels ('FB: 0' and 'FB: +1') are incorrect here.  The actual feedback was 'Correct!' or 'Incorrect.'  I'm just going to leave those as-is in the files since it doesn't change too much. Run the task (under /stimuli) to see what the feedbacks look like.
 4) there was a bug in the original task description that indicated this as 'Simon Conflict'. This is not that task. This is a Probabilistic Selection Task. These should have been changed to PST, but if you see SimonConflict just realize that was an original mis-label.",1,1,0
ds004551,2023-04-07 2:24:47,Kaz Sakakura,1.0.6,testMIproject,2023-04-07 11:11:05,0,3,74017400000,68.9 GB,504,114,1,41,1.7.0,CC0,"Kazuki Sakakura, Naoto Kuroda, Masaki Sonoda, Takumi Mitsuhashi, Ethan Firestone, Aimee F. Luat, Neena I. Marupudi, Sandeep Sood, Eishi Asano",,N/A,N/A,,doi:10.18112/openneuro.ds004551.v1.0.6,IRB of Wayne State University,sleep,,iEEG,"This dataset was curated for publication as part of the manuscript in Sakakura et al. (in preparation). 
 It contains iEEGs collected from 114 individuals during slow wave sleep. 
 The available Matlab code can be found at https://github.com/kaz1126/MI_HFO.
 The iEEG coordinate system employed in this dataset is MNI305.",1,0,0
ds004554,2023-04-21 15:35:10,Binbin XU,1.0.4,Forced Picture Naming Task,2023-04-21 16:15:57,0,1,9432070000,0.009 TB,101,16,0,0,1.8.0,CC0,"V. Volpert, B. Xu, A. Tchechmedjiev, S. Harispe, A. Aksenov, Q. Mesnildrey and A. Beuter",,,,"Characterization of spatiotemporal dynamics in EEG data during picture naming with optical flow patterns. {Volpert, V.; Xu, B.; Tchechmedjiev, A.; Harispe, S.; Aksenov, A.; Mesnildrey, Q. & Beuter, A.}. {Mathematical Biosciences and Engineering, 2023, 20, 11429-11463}. https://doi.org/10.3934/mbe.2023507",doi:10.18112/openneuro.ds004554.v1.0.4,EEG data collection was approved by the ethical committee CER Grenoble Alpes-Avis-2020-09-01-3,picturenaming,8.1.0,EEG,"This is the preprocessed dataset used for study ""Characterization of spatiotemporal dynamics in EEG data during picture naming with optical flow patterns"". 
 

 The Picture Naming Task study included sixteen native French-speaking men, ranging in age from 18 to 70 years old. The participants met the inclusion criteria, which required normal or corrected-to-normal vision and hearing, as well as right-handedness, as determined by a handedness questionnaire [Oldfield1971assessment]. Exclusion criteria were in place to ensure that participants had no history of neurological or psychiatric disorders, drug addiction, or head trauma. In total 20 subjects were included in the study. The four first subjects' data was excluded due to hardware failure. 
 

 Participants were required to name the pictures shown on a screen. Each event (random pictures) has three phases: [-2s, 0s] is the baseline (pre-visual-stimulation); at time 0 picture is shown on screen; then [0s, 1.5s] post-stimulation phase; [1.5s, 3s], naming phase. Pictures used in the task were selected from the Snodgrass & Vanderwart black-and-white line drawing corpus [Snodgrass1980standardized]. ""./code/experiment\_schema.pdf"" showed the task design.
 

 Data pre-processing pipeline is illustrated in ""./code/preprocess\_pipeline.pdf"". In total, 270 trials each for the 16 subjects.",1,1,0
ds004561,2023-05-03 22:19:35,John Veillette,1.0.0,Illusion of Agency over Electrically-Actuated Movements,2023-05-04 0:31:19,0,1,104872000000,0.105 TB,212,23,19,24,1.6.0,CC0,"John Veillette, Pedro Lopes, Howard Nusbaum",,,,,doi:10.18112/openneuro.ds004561.v1.0.0,,agencyRT,,EEG,"References
 ----------
 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G., Phillips, C., Delorme, A., Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Scientific Data, 6, 103. https://doi.org/10.1038/s41597-019-0104-8",1,1,0
ds004563,2023-05-10 10:26:34,Sophie Smit,1.0.1,Vicarious touch: overlapping neural patterns between seeing and feeling touch,2023-07-05 0:22:01,0,3,108333000000,0.108 TB,377,40,17,40,v1.8.0,CC0,"Sophie Smit, Denise Moerel, Regine Zopf, Anina N Rich",,"Smit, S., Moerel, D., Zopf, R., & Rich, A. N. (2023). Vicarious touch: overlapping neural patterns between seeing and feeling touch. bioRxiv (2023). https://doi.org/10.1101/2022.06.21.497107",,,doi:10.18112/openneuro.ds004563.v1.0.1,,Touch decoding,,EEG,"Data collection took place at Macquarie University in Sydney Australia. The study was approved by the Macquarie University Ethics Committee.
 

 We used time-resolved multivariate pattern analysis on whole-brain EEG data from people with and without vicarious touch experiences to test whether seen touch evokes overlapping neural representations with the first-hand experience of touch. Participants felt touch to the fingers (tactile trials) or watched carefully matched videos of touch to another person’s fingers (visual trials). 
 

 There were 12 runs in total, divided into four blocks of 36 trials (with alternating sets of nine tactile and nine visual trials) resulting in a total of 1728 trials (864 tactile and 864 visual). There were an additional 240 target trials (20 per run), which were excluded from analysis.
 

 Between trials there was an inter-trial-interval of 800ms. Each run lasted approximately 7-8 minutes with short breaks between blocks and runs.
 

 Whole brain 64-channel EEG data were recorded using an Active Two Biosemi system (Biosemi, Inc.) at 2048Hz and 10-20 standard caps. Stimuli were presented using MATLAB (MathWorks) and Psychtoolbox (Brainard and Vision). The experiment presentation script, all analysis code, and stimuli are made available (see code and stimuli folder). The data is made available both in raw form (see each participant's file) and after processing (see derivatives).",1,1,0
ds004574,2023-05-24 16:38:44,Nandakumar Narayanan,1.0.0,Cross-modal Oddball Task.,2023-05-24 17:04:50,0,1,14470000000,0.014 TB,1174,146,0,0,v1.2.1,CC0,"Arun Singh arun.singh@usd.edu, Rachel Cole rachel-cole@uiowa.edu, Arturo Espinoza arturo-espinoza@uiowa.edu, Jan R Wessel jan-wessel@uiowa.edu, Jim Cavanagh jcavanagh@unm.edu, Nandakumar Narayanan nandakumar-narayanan@uiowa.edu",,,"This dataset was supported by NIH P20NS123151 and R01NS100849 to NSN, ===NEMAR-SEP=== and NRSA F32 AG069445-01 to RC. All research protocols were approved by the University of Iowa ===NEMAR-SEP=== Human Subjects Review Board (IRB# 201707828).",doi: https://doi.org/10.1101/2022.07.26.22278079,doi:10.18112/openneuro.ds004574.v1.0.0,,Oddball,,EEG,"This experiment includes 146 subjects: 98 individuals with Parkinsons disease, 
 and 48 controls. The data were collected from 2017-2021. Subjects completed this oddball task (along with multiple other cognitive tasks) 
 while EEG was recorded with a 64-channel BrainVision cap. This task includes a primary GO cue,
 (white arrow) that required a directional response. That response could be correct or incorrect. The primary cue 
 

 was preceeded by a visual pre-cue and an auditory pre-cue, which occurred at the same time (500ms before arrow cue). 
 Each trial had either standard for both pre-cues, oddball visual pre-cue, or oddball auditory pre-cue. 
 Our analysis focused only on trials with both pre-cues standard or oddball auditory pre-cue.",1,1,0
ds004577,2023-05-24 23:19:29,Thalia Harmony,1.0.1,Dataset containing resting EEG for a sample of 103 normal infants in the first year of life,2023-05-25 16:05:35,0,4,684370000,0.001 TB,914,103,0,0,1.6.0,CC0,"Thalía Harmony (Neurodevelopment Research Unit; Instituto de Neurobiología; Universidad Nacional Autónoma de México), Gloria Otero-Ojeda (Facultad de Medicina; Universidad Autónoma del Estado de México), Eduardo Aubert (Centro de Neurociencias de Cuba), Thalía Fernández (Neurodevelopment Research Unit; Instituto de Neurobiología; Universidad Nacional Autónoma de México), Lourdes Cubero-Rego (Neurodevelopment Research Unit; Instituto de Neurobiología; Universidad Nacional Autónoma de México)","The authors acknowledge Teresa Álvarez-Vázquez, Héctor Belmont-Tamayo, and Paulina Álvarez-García for their collaboration",,Dirección General Del Personal Académico IT200223,"Otero GA, Harmony T, Pliego-Rivero FB, Ricardo-Garcell J, Bosch-Bayard J, Porcayo-Mercado R, Fernández-Bouzas A, Díaz-Comas L, Galán L, Vieyra-Reyes P, Fernández T. QEEG norms for the first year of life. Early Hum Dev, 87: 691-703, 2011.DOI: 10.1016/j.earlhumdev.2011.05.010 ===NEMAR-SEP=== Bosch-Bayard Jorge, Valdés-Sosa Pedro A., Fernandez T., Otero G., Pliego Rivero B., Ricardo-Garcell J., González-Frankenberger B., Galán-García L., Fernandez-Bouzas A., Aubert-Vazquez E., Lage-Castellanos A., Rodríguez-Valdés R., Harmony, T. 3D Statistical Parametric Mapping of quiet sleep EEG in the first year of life. Neuroimage, 59: 3297–3308, 2012. DOI: 10.1016/j.neuroimage.2011.11.001 ===NEMAR-SEP=== Bosch-Bayard J, Biscay RJ, Fernandez T, Otero GA, Ricardo-Garcell J, Aubert-Vazquez E, Evans AC, Harmony T. EEG effective connectivity during the first year of life mirrors brain synaptogenesis, myelination, and early right hemisphere predominance. NeuroImage 252 (2022) 119035. https://doi.org/10.1016/j.neuroimage.2022.119035",doi:10.18112/openneuro.ds004577.v1.0.1,Ethical permission was obtained from the Ethics Committee of the Instituto de Neurobiología of the Universidad Nacional Autónoma de México which also complied with the Ethical Principles for Medical Research Involving Human Subjects outlined in the Helsinki Declaration.,EEG,,EEG,"May 25th 2023
 Neurodevelopment Research Unit, Instituto de Neurobiología, Universidad Nacional Autónoma de México
 

 This is a dataset containing resting EEG for a sample of 103 normal infants (41 female and 62 male) in the first year of life.
 

 81 subjects with 1 EEG recording
 18 subjects with 2 EEG recordings
 3 subjects with 3 EEG recording
 1 subject with 4 EEG recordings
 

 130 EEG recordings in total distributed in 4 sessions",1,1,0
ds004579,2023-05-25 15:19:58,Nandakumar Narayanan,1.0.0,Interval Timing Task,2023-05-25 16:41:59,0,1,25896700000,0.026 TB,979,139,0,0,v1.2.1,CC0,"Arun Singh arun.singh@usd.edu, Rachel Cole rachel-cole@uiowa.edu, Arturo Espinoza arturo-espinoza@uiowa.edu, Jan R Wessel jan-wessel@uiowa.edu, Jim Cavanagh jcavanagh@unm.edu, Nandakumar Narayanan nandakumar-narayanan@uiowa.edu",,,,doi: https://doi.org/10.1101/2022.07.26.22278079,doi:10.18112/openneuro.ds004579.v1.0.0,,IntervalTiming,,EEG,"This experiment includes 139 subjects: 94 individuals with Parkinsons disease, 
 and 45 controls. Subjects completed this IntervalTiming task (along with multiple other cognitive tasks) 
 while EEG was recorded with a 64-channel BrainVision cap. This task presented black instructional 
 text on the center of a white screen that read ""Short interval"" on 3-second interval trials 
 and ""Long interval"" on 7-second interval trials. The researchers never communicated the actual 
 interval durations to the patient. The instructions were displayed for 1 second, and the appearance 
 of an image of a solid box in the center of the computer screen indicated the start of the interval. 
 The cue was displayed on the screen for the entire trial, which lasted 6 s for 3-s intervals and 
 14 s for 7-s intervals. The researchers instructed participants to press the keyboard spacebar 
 when they judged the target interval to have elapsed. Participants were directed not to count, and 
 a distractor vowel appeared at random intervals in the screen center.",1,1,0
ds004580,2023-05-25 15:52:41,Nandakumar Narayanan,1.0.0,Simon-conflict Task.,2023-05-25 16:55:56,0,1,17008400000,0.017 TB,1182,147,0,0,v1.2.1,CC0,"Arun Singh arun.singh@usd.edu, Rachel Cole rachel-cole@uiowa.edu, Arturo Espinoza arturo-espinoza@uiowa.edu, Jan R Wessel jan-wessel@uiowa.edu, Jim Cavanagh jcavanagh@unm.edu, Nandakumar Narayanan nandakumar-narayanan@uiowa.edu",,,"This dataset was supported by NIH P20NS123151 and R01NS100849 to NSN, ===NEMAR-SEP=== and NRSA F32 AG069445-01 to RC. All research protocols were approved by the University of Iowa ===NEMAR-SEP=== Human Subjects Review Board (IRB# 201707828).",doi: https://doi.org/10.1101/2022.07.26.22278079,doi:10.18112/openneuro.ds004580.v1.0.0,,Simon,,EEG,"This experiment includes 146 subjects: 98 individuals with Parkinsons disease, 
 and 48 controls. Subjects completed this Simon task (along with multiple other cognitive tasks) 
 while EEG was recorded with a 64-channel BrainVision cap. This task included a stimulus presented
 to the left or right side of the screen. The researchers instructed participants to press a left key when the 
 was yellow or red and a right key when it was cyan or blue. The stimulus was either spatially congruent 
 with the screen side matching the response hand or incongruent with the screen side contralateral 
 to the response hand. The researchers analyzed data from congruent and incongruent trials separately.",1,1,0
ds004584,2023-05-31 13:45:50,Nandakumar Narayanan,1.0.0,Rest eyes open,2023-06-07 21:22:39,0,1,3078220000,0.003 TB,1049,149,0,0,v1.2.1,CC0,"Arun Singh arun.singh@usd.edu, Rachel Cole rachel-cole@uiowa.edu, Arturo Espinoza arturo-espinoza@uiowa.edu, Jim Cavanagh jcavanagh@unm.edu, Nandakumar Narayanan nandakumar-narayanan@uiowa.edu",,,,doi: https://doi.org/10.1101/2022.07.26.22278079,doi:10.18112/openneuro.ds004584.v1.0.0,,Rest,,EEG,"This experiment includes 149 subjects: 100 individuals with Parkinsons disease, 
 and 49 controls. EEG was recorded with a 64-channel BrainVision cap. Resting-state 
 EEG was collected from patients sitting in a quiet room with their eyes open for 
 two minutes.",1,1,0
ds004588,2023-06-01 11:30:58,Kostas Georgiadis,1.2.0,Neuma,2023-06-01 11:51:40,0,1,560004000,0.001 TB,216,42,0,0,1.8.0,CC0,"Kostas Georgiadis, Fotis P. Kalaganis, Kyriakos Riskos, Eleytheria Matta, Vangelis P. Oikonomou, Yfantidou Ioanna, Dimitris Chantziaras, Kyriakos Pantouvakis, Spiros Nikolopoulos, Nikos A. Laskaris, Ioannis Kompatsiaris",,,,,doi:10.18112/openneuro.ds004588.v1.2.0,,unnamed,8.1.0,EEG,"A novel multimodal Neuromarketing dataset that encompasses the data from 42 individuals who participated in an advertising brochure-browsing scenario is introduced here. In more detail, participants were exposed to a series of supermarket brochures (containing various products) and instructed to select the products they intended to buy. The data collected for each individual executing this protocol included: (i) encephalographic (EEG) recordings, (ii) eye tracking (ET) recordings, (iii) questionnaire responses (demographic, profiling and product related questions), and (iv) computer mouse data. 
 

 The preprocessed version of this dataset can be found here: https://figshare.com/articles/dataset/NeuMa\_PreProcessed\_A\_multimodal\_Neuromarketing\_dataset/22117124",1,1,0
ds004595,2023-06-09 20:54:43,James F Cavanagh,1.0.0,EEG: RL Task (3-Armed Bandit) with alcohol cues in hazardous drinkers and ctls,2023-07-28 15:58:30,0,1,8390890000,0.008 TB,569,53,18,55,1.1.1,CC0,"Ethan Campbell, James F Cavanagh",,,,pending,doi:10.18112/openneuro.ds004595.v1.0.0,,ThreeArmedBandit,,EEG,"RL task (3-armed bandit) with alcohol vs. beverage cues in N=53 Community participants.  Data collected from 2019-2021 in the CRCL at UNM.  The paper [Campbell, E., Singh, G., Claus, E.D., Witkiewitz,K., Costa, V.D., Hogeveen, J; & Cavanagh, J.F. Electrophysiological markers of aberrant cue-specific exploration in hazardous drinkers] Should be coming out in print soonish. Your best bet for understanding this task would be to read that paper first.  For more info on triggers and outputs, see BEH\_EXPLAIN.m file in code folder. - James F Cavanagh 03/06/2023",1,1,0
ds004598,2023-06-12 9:30:19,Monica van den Berg,1.0.0,LFP during linear track in 6-month old TgF344-AD rats,2023-07-06 12:13:11,0,3,10673000000,9.9 GB,63,9,0,0,1.2.0,CC0,"Moradi Faraz, van den Berg Monica, Mirjebreili Morteza, Kosten Lauren, Verhoye Marleen, Amiri Mahmood, Keliris A. Georgios",,N/A,N/A,,doi:10.18112/openneuro.ds004598.v1.0.0,Etical commitee of University of Antwerp,Lineartrack,,EEG,,1,0,0
ds004602,2023-06-12 22:02:31,Peter Clayson,1.0.1,Registerd Replication Report of ERN/Pe Psychometrics,2023-07-21 19:05:20,0,1,79364500000,0.079 TB,3828,182,0,0,v1.8.0,CC0,"Peter E Clayson, Michael J Larson",,"Please cite primary manuscript for the dataset: https://psyarxiv.com/465wq
 

 Peter E Clayson and Michael J Larson (2023). Registered Replication Report of ERN/Pe Psychometrics. OpenNeuro. [Dataset] doi:10.18112/openneuro.ds004602.v1.0.0",,https://osf.io/8cbua/,doi:10.18112/openneuro.ds004602.v1.0.1,,mixed,8.1.0,EEG,"This dataset supports a registered replication report that is described at https://osf.io/8cbua/. Scripts used for data processing are posted there.
 

 Abstract
 Intact cognitive control is critical for goal-directed behavior and is widely studied in healthy and clinical populations using the error-related negativity (ERN). A common assumption in such studies is that ERNs recorded during different experimental paradigms reflect the same construct or functionally equivalent processes and that ERN is functionally distinct from other error-monitoring event-related potentials (ERPs; error positivity [Pe]), other neurophysiological indices of cognitive control (N2), and even other indices unrelated to cognitive control (visual N1). The present registered report represents a replication-plus-extension study of the psychometric validity of cognitive control ERPs (Riesel et al., 2013, Biological Psychology) and evaluated the convergent and divergent validity of ERN, Pe, N2, and visual N1 recorded during three paradigms (flanker, Stroop, Go/no-go). Data from 182 participants were collected from two study sites, and ERP psychometric reliability and validity were evaluated. Findings supported convergent and divergent validity of ERN, Pe, and delta-Pe (error minus correct)—these ERPs correlated more with themselves across tasks than with other ERPs measured during the same task. Convergent validity of delta-ERN was not replicated, despite high internal consistency. ERN was strongly correlated with N2 at levels similar or higher than those in support of convergent validity for other ERPs, and the present study failed to provide evidence of divergent validity for ERN and Pe from N2 or the theoretically unrelated N1. Present findings underscore the importance of considering the psychometric validity of ERPs as it provides a foundation for interpreting and comparing ERPs across different tasks and studies.",1,1,0
ds004603,2023-06-13 0:55:13,Benjamin G. Lowe,1.1.0,Visual Attribute-Specific Contextual Trajectory Paradigm,2023-06-13 3:10:20,0,1,29391600000,0.029 TB,338,37,17,30,1.11.0,CC0,"Benjamin Lowe (ben.lowe@mq.edu.au), Jonathan Robinson (jonathan.robinson@monash.edu), Naohide Yamamoto (naohide.yamamoto@qut.edu.au), Hinze Hogendoorn (hinze.hogendoorn@qut.edu.au), Patrick Johnston (dr.pat.johnston@icloud.com)",,,,,doi:10.18112/openneuro.ds004603.v1.1.0,,CTP,,EEG,"These data were recorded from 37 subjects using the following exclusion
 criteria: Normal, or correct to normal, vision; no history of neurological disorder; and less than 35 years of age.
 

 Subjects completed a novel, visual contextual trajectory paradigm (CTP) wherein
 the onset of a bound stimulus violated an established trajectory in terms of
 its brightness, size, or orientation. No attribute was violated during control
 trials. Full method details can be read within the following published paper: https://doi.org/10.1016/j.cortex.2023.08.004
 

 Analysis code is available at: https://github.com/benjaminglowe/attribute-specific-prediction-error-analysis-code
 

 Please email ben.lowe@mq.edu.au if you have any further questions.",1,1,0
ds004621,2023-06-26 12:25:41,Patrycja Dzianok,1.0.1,The Nencki-Symfonia EEG/ERP dataset,2023-10-12 13:01:56,0,1,83086900000,0.083 TB,848,42,20,34,1.0.2,CC0,"Dzianok Patrycja, Antonova Ingrida, Wojciechowski Jakub, Dreszer Joanna, Kublik Ewa",,Please cite this paper https://pubmed.ncbi.nlm.nih.gov/35254424/,Polish National Science Center (NCN) grant no. 2016/20/W/NZ4/00354,"Dzianok P, Antonova I, Wojciechowski J, Dreszer J, Kublik E. Supporting data for 'The Nencki-Symfonia EEG/ERP dataset: Multiple cognitive tasks and resting-state data collected in a sample of healthy adults'. GigaScience Database, 2022. http://doi.org/10.5524/100990 ===NEMAR-SEP=== Dzianok P, Antonova I, Wojciechowski J, Dreszer J, Kublik E. The Nencki-Symfonia electroencephalography/event-related potential dataset: Multiple cognitive tasks and resting-state data collected in a sample of healthy adults. Gigascience. 2022 Mar 7;11:giac015. doi: 10.1093/gigascience/giac015",doi:10.18112/openneuro.ds004621.v1.0.1,"This study was approved by the Research Ethics Committee, Faculty of Humanities, Nicolaus Copernicus University in Torun, Poland (No. 6/2018)","Extended Multi-Source Interference Task (MSIT+), Oddball, Resting state protocol (REST), Simple reaction time task (SRT)",,EEG,"The Nencki-Symfonia EEG/ERP dataset (dataset DOI: doi.org/10.5524/100990)
 

 Description: mixed cognitive tasks [(i) an extended multi-source interference task, MSIT+; (ii) a 3-stimuli oddball task; (iii) a control, simple reaction task, SRT; and (iv) a resting-state protocol]
 

 Please cite the following references if you use these data:
 1. Dzianok P, Antonova I, Wojciechowski J, Dreszer J, Kublik E. The Nencki-Symfonia electroencephalography/event-related potential dataset: Multiple cognitive tasks and resting-state data collected in a sample of healthy adults. Gigascience. 2022 Mar 7;11:giac015. doi: 10.1093/gigascience/giac015.
 2. Dzianok P, Antonova I, Wojciechowski J, Dreszer J, Kublik E. Supporting data for ""The Nencki-Symfonia EEG/ERP dataset: Multiple cognitive tasks and resting-state data collected in a sample of healthy adults"" GigaScience Database, 2022. http://doi.org/10.5524/100990
 

 Release history:
 

 26/01/2022: Initial release (GigaDB)
 

 15/06/2023: Added to OpenNeuro; updated README and dataset\_description.json; minor updated to .json files related with BIDS errors/warnings. Updated events files (ms changed to s).
 

 12/10/2023: public release on OpenNeuro after deleting some additional, not needed system information from raw logfiles",1,0,0
ds004696,2023-08-15 20:54:55,A Gabriela Ojeda Valencia,1.0.1,HAPwave_bids,2023-08-16 13:32:23,0,1,15213000000,14.2 GB,5163,8,13,63,1.12.0,CC0,"Ojeda Valencia, G., Gregg, N., Huang, H., Lundstrom, B., Brinkmann, B., Pal Attia1, T., Van Gompel, J., Bernstein,M., In, M., Huston, J., Worrell1, G., Miller, K., Hermes, D.","Special thanks to the Mayo Clinic, particularly to the Multimodal Neuroimaging Laboratory (MNL), the Bioelectronics Neurophysiology and Engineering Laboratory (BNEL), the DERIVE Office and the Mayo Clinic Center for Biomedical Discovery","Please cite this paper: Ojeda Valencia G, Gregg N, Huang H, Lundstrom B, Brinkmann B, Pal Attia T, Van Gompel J, Bernstein M, In MH, Huston J, Worrell G, Miller KJ, and Hermes D. 2023. Signatures of electrical stimulation driven network interactions in the human limbic system. Journal of Neuroscience (in press).",NIH RO1MH122258 ===NEMAR-SEP=== The DERIVE Office and the Mayo Clinic Center for Biomedical Discovery,,doi:10.18112/openneuro.ds004696.v1.0.1,,Cortico-Cortical Evoked Potentials -CCEPs,,iEEG,"# Information
 This dataset contains intracranial EEG (iEEG) recordings from 8 patients during single pulse electrical stimulation used in the publication of:
 Ojeda Valencia G, Gregg N, Huang H, Lundstrom B, Brinkmann B, Pal Attia T, Van Gompel J, Bernstein M, In MH, Huston J, Worrell G, Miller KJ, and Hermes D. 2023. Signatures of electrical stimulation driven network interactions in the human limbic system. Journal of Neuroscience (in press).
 

 # License
 This dataset is made available under the Public Domain Dedication and License CC v1.0, whose full text can be found at 
 https://creativecommons.org/publicdomain/zero/1.0/. 
 We hope that all users will follow the ODC Attribution/Share-Alike Community Norms (http://www.opendatacommons.org/norms/odc-by-sa/); 
 in particular, while not legally required, we hope that all users of the data will acknowledge by citing the following in any publication:
 

 Ojeda Valencia G, Gregg N, Huang H, Lundstrom B, Brinkmann B, Pal Attia T, Van Gompel J, Bernstein M, In MH, Huston J, Worrell G, Miller KJ, and Hermes D. 2023. Signatures of electrical stimulation driven network interactions in the human limbic system. Journal of Neuroscience. DOI: https://doi.org/10.1523/JNEUROSCI.2201-22.2023 
 

 # Task Description
 Patients were resting in the hospital bed, while single pulse stimulation was performed. The stimulation had a duration of 200 microseconds, was biphasic and had an amplitude of 6mA. For subject 7 stimulation amplitude was sometimes reduced to 4mA to minimize interictal responses.
 

 # Code
 Code to analyses these data is available at: https://github.com/MultimodalNeuroimagingLab/HAPwave
 

 # Dataset
 This data is organized according to the Brain Imaging Data Structure specification (BIDS version 1.12.0). A community- driven specification for organizing neurophysiology data along with its metadata. For more information on this data specification, see https://bids-specification.readthedocs.io/en/stable/
 Each subject has their own folder (e.g., ‘sub-01’) containing intracranial EEG (iEEG) recordings from 8 patients during single pulse electrical stimulation, as well as the metadata needed to understand the raw data and event timing.
 

 # Acknowledgements
 This project was funded by the National Institute Of Mental Health of the National Institutes of Health Brain Initiative under Award Number R01 MH122258, “CRCNS: Processing speed in the human connectome across the lifespan"". The overall goal of this project is to develop a large database of single pulse stimulation data and develop tools to advance our understanding of the human connectome across the lifespan. The data was collected by Dora Hermes, Nick Gregg, Brian Lundstrom, Cindy Nelson, Gabriela Ojeda Valencia, Gregg Worrell and Kai J. Miller. The BIDS formatting was performed by Dora Hermes and Gabriela Ojeda Valencia.
 

 # Contact
 Please contact Dora Hermes (hermes.dora@mayo.edu) or Gabriela Ojeda Valencia (OjedaValencia.Alma@mayo.edu) for questions.",1,0,0
ds004626,2023-07-06 13:36:52,Lukasz Okruszek,1.0.2,Can we dissociate hypervigilance to social threats from altered perceptual decision-making processes in lonely individuals? An exploration with Drift Diffusion Modelling and EEG event-related potentials.,2023-08-07 12:06:38,0,1,21336300000,0.021 TB,266,52,19,35,1.8.0,CC0,"Szymon Maka, Marta Chrustowicz, Lukasz Okruszek",,,,,doi:10.18112/openneuro.ds004626.v1.0.2,,DotProbe,8.1.0,EEG,"Dataset is related to publication: Maka, S., Chrustowicz, M., & Okruszek, L. (2023). Can we dissociate hypervigilance to social threats from altered perceptual decision-making processes in lonely individuals? An exploration with Drift Diffusion Modeling and event-related potentials. Psychophysiology, e14406. https://doi. org/10.1111/psyp.14406",1,1,0
ds004635,2023-07-11 21:15:36,Gaffrey Lab,3.0.0,Gaffrey Lab Infant Microstates Reliability,2023-07-11 22:03:22,0,1,27971400000,26.1 GB,292,48,0,0,1.8.0,CC0,"Armen Bagdasarov, Michael S. Gaffrey",,,,https://github.com/gaffreylab/EEG-Microstate-Analysis-Tutorial/tree/main,doi:10.18112/openneuro.ds004635.v3.0.0,,resting,8.1.0,EEG,"Participants were 48, 5-10-month-old infants (27 male). All research was approved by the Duke University Health System Institutional Review Board and carried out in accordance with the Declaration of Helsinki. Caregivers provided informed consent, and compensation was provided for their participation. Infants sat on their caregiver’s lap and watched up to 15 minutes of relaxing videos with sound (i.e., 10, 90-second videos separated by breaks during which caregivers could play with their infant). Before each video started, an attention grabber (i.e., three-second video of a noisy rattle) directed the infant’s attention to the screen. Videos were presented with E-Prime software (Psychological Software Tools, Pittsburgh, PA). Caregivers were instructed to silently sit still during videos. If infants shifted their attention away from the screen, caregivers were permitted to re-direct their attention only by pointing to the screen. EEG was recorded at 1000 Hertz (Hz) and referenced to the vertex (channel Cz) using a 128-channel HydroCel Geodesic Sensor Net (Electrical Geodesics, Eugene, OR). Impedances were maintained below 50 kilohms throughout the EEG session. For more information, visit: https://github.com/gaffreylab/EEG-Microstate-Analysis-Tutorial/wiki",1,0,0
ds004642,2023-07-17 13:57:44,Vasileios S. Dimakopoulos,1.0.1,Intraoperative recordings of medianus stimulation with low and high impedance ECoG,2023-07-31 8:40:13,0,1,1292810000,0.001 TB,55,10,19,56,1.4.0,CC0,"Vasileios Dimakopoulos, Marian Neidert, Johannes Sarnthein",We acknowledge a grant awarded by the Swiss National Science Foundation (funded by the SNSF 204651 to JS) and a scholarship awarded by the Alexander S. Onassis Public Benefit Foundation (to VD). The funders had no role in the design or analysis of the study,Please cite this paper: https://doi.org/10.1016/j.clinph.2023.07.002,"Swiss National Science Foundation, SNSF 204651 ===NEMAR-SEP=== Alexander S. Onassis Public Benefit Foundation Scholarship","Vasileios Dimakopoulos, Marian C. Neidert, Johannes Sarnthein, Low impedance electrodes improve detection of high frequency oscillations in the intracranial EEG, Clinical Neurophysiology, 2023",doi:10.18112/openneuro.ds004642.v1.0.1,Kantonale Ethikkommision PB-2017-00094,lozHFO,,iEEG,"## Intraoperative recordings of medianus stimulation with low and high impedance ECoG
 This dataset of medianus SEP was first analyzed in publication [1]. There we investigated whether the low impedance ECoG electrode (LoZ) improves fast ripple detection over a standard electrode with high impedance contacts (HiZ).
 There are 10 patients (median age 40 y, range 19-56 y, 6 female) who underwent brain tumor resections in the perirolandic region at our institution. The data includes the continuous raw data from the ECoG contacts of both electrodes. We recorded medianus SEP intraoperatively (stimulation rate = 4.7 Hz) from two 4-contacts ECoG strips simultaneously (LoZ: a contacts, HiZ: b contacts) that had different median impedance (LoZ: 3.4 k?, HiZ: 6.9 k?).
 

 ### Repository structure
 

 ### Main directory (LoZ HFO)
 

 Contains metadata files in the BIDS standard about the participants and the study. Folders are explained below.
 

 ### Subfolders
 

 -  LoZ HFO/sub-/ Contains folders for each subject, named sub- and session information.
 -  LoZ HFO/sub-/ses-01/ieeg/ Contains the raw ieeg data in .edf format for each subject. Each *ieeg.edf file contains continuous iEEG data from one stimulation rate recorded at the hand area ? from both the electrodes simultaneously . Details about the channels are given in the corresponding .tsv file.
 

 ### Note from the paper
 

 ""The offline data processing used the continuous ECoG that was recorded in parallel to the SEP recordings. Data analysis was performed with custom scripts in Matlab. To detect the SEP stimulation artefact, we first filtered the ECoG (high pass cutoff = 200 Hz) and performed local peak detection (minimum peak prominence between peaks = 30 ms, minimum peak width = 4 ms, samples = 0.2 ms). We used the times of the detected stimulus artifact as triggers to define sweeps with post-stimulus recording sweep length 50 ms. We classified sweeps with amplitude ±100 µV as artefact-ridden and excluded them from further analysis.
 

 We averaged 100 sweeps and filtered the averaged trace (bandpass [30 300] Hz, IIR filter, response roll-off -12 db per octave, forward and reverse filtering to avoid phase distortion). We visually inspected the data and selected one optimal channel with high N20 amplitude (positive or negative) for further analysis. From the averaged N20 trace, we determined the N20 peak latency. To obtain the N20 peak amplitude and the SNR, we inspected the latency of the N20 peak. If the N20 latency was >20 ms, we selected a signal window [20 25] ms. If the N20 latency was <= 20 ms, we selected a signal window [17 22] ms. In the same way, we filtered the averaged trace in the [250 500] Hz band to obtain the evoked FR and in the [500 1000] Hz band to obtain the evoked HFO. We doubled the largest deflection in the signal window of the N20 frequency band to define the N20 signal amplitude. In the FR and HFO bands we used the peak-to-peak amplitude.""
 

 ### BIDS Conversion
 

 bids-starter-kid and custom Matlab scripts were used to convert the dataset into BIDS format.
 

 ### References
 [1] Vasileios Dimakopoulos, Marian C. Neidert, Johannes Sarnthein, Low impedance electrodes improve detection of high frequency oscillations in the intracranial EEG, Clinical Neurophysiology, 2023, ISSN 1388-2457, https://doi.org/10.1016/j.clinph.2023.07.002
 

 If you have any inquiries or questions, contact:
 

 * Vasileios Dimakopoulos (vasileios.dimakopoulos@usz.ch)
 * Johannes Sarnthein (johannes.sarnthein@usz.ch)",1,1,0
ds004657,2023-08-05 2:11:45,Kevin King,1.0.3,Driving with Autonomous Aids,2023-08-05 3:11:25,0,6,46237300000,43.1 GB,838,24,0,0,1.8.0,CC0,"Jason Metcalfe, Amar Marathe, Tony Johnson, Stephen Gordon, Jon Touryan, Kevin King",,,,,doi:10.18112/openneuro.ds004657.v1.0.3,,Drive,8.1.0,EEG,"TX20 dataset
 

 Vehicle survivability is critically important in today’s military. Survivability is critically impacted by the performance of human operators – especially as it degrades with various factors. Significant DoD investments have focused on developing and integrating autonomous technologies to mitigate the effects of human error. However, simply implementing autonomy without having a clear plan for integrating with human operators can lead to relatively poor performance and thus low user acceptance. Human trust in automation (TiA) is a well-documented determinant of acceptance and use, but more important than achieving a certain level of trust is to find an appropriate match between the capabilities of the technology and the operator's trust. Finding means to calibrate TiA to elicit the desired use of the autonomy is an important goal, but requires reliable quantitative indicators that can be continuously monitored. Considerable research on interpersonal trust has revealed measurable patterns of physiological change that correlate significantly with changing levels of subjective trust and trust-based decision making. This research was aimed at facilitating the eventual real-time management of TiA by developing initial psychophysiology-based metrics for monitoring and predicting continuous changes in trust and/or trust-related behaviors.
 

 Complete a semi-automated driving task involving lane maintenance, following distance from a lead vehicle, and collision avoidance (with oncoming traffic and frequently appearing pedestrians). Under certain conditions, an automated driving assistant was available and could be engaged and disengaged at the discretion of the driver. The automated assistant was capable of managing limited aspects of the driving task (maintainance of following distance alone or maintaining following distance and lane position), but was not capable of collision avoidance. Separate driver responses (button presses) were required to successfully avoid collisions with pedestrians.
 

 This research was conducted to develop and validate methods for monitoring and predicting varying degrees of trust in automation (TiA) using both physiological and behavioral metrics characterizing real-time human-automation interactions. The overarching goal of this research was to develop and validate methods for measuring and drawing inferences about TiA, either directly or indirectly through correlated constructs. In particular, we examined operator trust in vehicle automation as it is reflected in changes observed in subjective reports as well as behavioral and physiological state variables during the execution of a shared human-autonomy driving task. The stated aims underlying this goal included:
 Aim #1: To develop and experimentally validate metrics (dependent variables) that index changes in TiA. Rather than focusing on single-modality metrics, we will record and explore the patterns of correlation and co-variance among a variety of psychophysiological and behavioral variables and focus particularly on metrics that predict decisions around sharing vehicle control with the autonomy in each condition. State measures will be derived from EEG, EOG (electrooculography), ECG, EDA, and gaze position tracking as well as the subject vehicle control behaviors.
 Aim #2: To develop an understanding of factors (independent variables and covariates) that influence the subject’s TiA. Whereas the Aim #1 targets the identification of metrics, or groups of metrics, that reliably predict trust-based decision-making, here we seek to gain insight as to which factors influence the likelihood and directionality of those same trust-based decisions. Such factors will include real-time tracking of variables such as task load, collision risk, and recent performance history or trending changes in success rate.
 

 Sessions/Conditions
 SCPB: PractB
 SCMM: Manual driving
 SCFB: Full Bad autonomy
 SCFG: Full Good autonomy
 SCSB: Speed Bad autonomy
 SCSG: Speed Good autonomy.",1,0,1
ds004660,2023-08-05 16:29:48,Kevin King,1.0.2,TNO,2023-08-05 16:56:32,0,1,7782410000,7.2 GB,299,21,0,0,1.8.0,CC0,"Tony Johnson, Stephen Gordon, Jon Touryan, Kevin King",,,,,doi:10.18112/openneuro.ds004660.v1.0.2,,P300,8.1.0,EEG,TNO dataset,1,0,1
ds004661,2023-08-05 16:52:47,Kevin King,1.0.1,ANDI,2023-08-05 16:56:43,0,1,1505580000,1.4 GB,90,17,0,0,1.8.0,CC0,"Tony Johnson, Stephen Gordon, Jon Touryan, Kevin King",,,,,doi:10.18112/openneuro.ds004661.v1.0.1,,nback,8.1.0,EEG,ANDI dataset,1,0,1
ds004703,2023-08-16 1:23:55,Anna Mai,1.1.0,PassiveListen,2023-08-18 13:22:54,0,2,13278800000,0.013 TB,325,10,21,55,1.6.0,CC0,"Anna Mai, Stephanie Ries, Sharona Ben-Haim, Jerry Shih, Timothy Gentner",,,,,doi:10.18112/openneuro.ds004703.v1.1.0,,PassiveListen,,iEEG,"CONTACT
 

 For questions about this data set, please contact Anna Mai (anna.mai@mpi.nl; ORCiD 0000-0002-8343-9216). 
 

 PERMISSIONS
 

 These data may not be used for commericial purposes, including but not limited to use in any kind of training set for commercial machine learning applications.
 

 These data may not be used in any way that either in part or in whole disambiguates participant identity, including but not limited to attempts at 3D facial reconstruction.
 

 RECORDING SETUP
 

 These data were collected from June 2018 to August 2019.
 

 For all patients, a scalp electrode was used for referencing and ground. These were 13mm, 2.5M single lead subdermal electrodes made by Rochester Electro-Medical with serial number S81025-A-24RM.
 

 Depth electrodes were manufactured by Ad-Tech and are Spencer Probe depth electrodes. Each electrode has 10 leads evenly spaced 3-7mm apart. 
 

 With the exception of patients SD012 and SD022, all implants are depth electrodes. Patients SD012 and SD022 had grid and strip electrodes implanted in addition to several depth electrodes.
 

 Any channel names beginning with ``C'' were not used and should be dropped from analyses.
 

 TASK
 

 Participants passively listened to 30-45s passages of conversational speech and verbally answered a 2AC content question after each passage. 6 blocks with 7 passages per block.
 

 MISSING DATA
 

 Anatomical scans for particpant SD012 are not available due to excessive movement artifacts.",1,1,0
ds004706,2023-08-16 20:56:12,Joseph Rudoler,1.0.0,Spatial memory and non-invasive closed-loop stimulus timing,2023-08-23 16:36:12,0,9,1426140000000,1.4 TB,2711,34,0,0,1.6.0,CC0,"Joseph H. Rudoler, Matthew R. Dougherty, Brandon S. Katerman, James P. Bruska, Woohyeuk Chang, David J. Halpern, Nicholas B. Diamond, Michael J. Kahana",,,,,doi:10.18112/openneuro.ds004706.v1.0.0,,"NiclsCourierClosedLoop, NiclsCourierReadOnly",,EEG,"This dataset contains behavioral events and electrophysiological recordings from an experiment run in the Computational Memory Lab at the University of Pennsylvania from 2021-2022 with funding from U.S. Army Medical Research and Development Command (USAMRDC) through the Medical Technology Enterprise Consortium (MTEC) project MTEC-20-06-MOM-013, ""Restoring memory with task-independent semi-chronic closed-loop direct brain stimulation and non-invasive closed-loop stimulus timing optimization"". This experiment constitutes the non-invasive portion of the project, which targeted memory improvement through classifier-based stimulus presentation. 
 

 The experiment is a hybrid spatial-navigation and free recall paradigm in which subjects play the role of a courier delivering items to stores across a virtual town, and are subsequently asked to recall their deliveries. There are two phases - ""read-only"" and ""closed-loop"". In read-only sessions, there is no classifier-based timing manipulation and participants simply perform the task in order to generate training data for the models used in subsequent closed-loop sessions. After collecting sufficient training data, classifier models predict recall in closed-loop sessions and the stimulus presentation is timed to coincide with predicted good or bad memory encoding. 
 

 Two publications are based on this experiment:
 [""Neural correlates of memory in an immersive spatiotemporal context""](https://www.biorxiv.org/content/10.1101/2022.11.30.518606) studies the navigation and memory dynamics in read-only sessions, and ""Optimizing learning via real-time neural decoding"" (link pending) explores the results of the closed-loop manipulation.
 

 Note: memory dynamics in closed-loop sessions are potentially influenced by the closed-loop timing manipulation, and so may be biased in a way that precludes them from analyses of general mnemonic function. The read-only sessions, however, were not subject to this manipulation and therefore can be used for studying spatial and episodic memory (as in the first paper mentioned above).",1,1,0
ds004718,2023-08-27 3:17:03,Zhengwu Ma,1.0.6,Le Petit Prince Hong Kong: Naturalistic fMRI and EEG dataset from older Cantonese speakers,2023-10-30 4:20:59,0,1,39971800000,37.2 GB,762,52,65,77,1.12.0,CC0,"Mohammad Momenian, Zhengwu Ma, Shuyi Wu, Chengcheng Wang, Jixing Li",,,,,doi:10.18112/openneuro.ds004718.v1.0.6,,"lppHK, rest",,"MRI, EEG",,1,0,0
ds004738,2023-09-03 9:15:15,Bahne Bahners,1.0.1,sfb\_eeg\_phantom (B04/C01),2023-09-03 10:41:07,0,2,6587100000,0.007 TB,99,4,0,0,1.2.0,CC0,"Bahne H. Bahners, Roxanne Lofredi, Tilmann Sander, Alfons Schnitzler, Andrea A. Kuhn, Esther Florin",We thank Georg Bahners for his help with building the phantom and Pia Hartmann and Luisa Spallek for their assistance during recordings. EF gratefully acknowledges support by the Volkswagen Foundation (Lichtenberg program 89387). BHB gratefully acknowledges support by the Prof. Dr. Klaus Thiemann Foundation (Parkinson Fellowship 2022). RL gratefully acknowledges support by the Berlin Institute of Health (Clincian Scientist Programm 2021 2024).,"Please acknowledge the authors and cite the following reference: Bahne H Bahners, Roxanne Lofredi, Tilmann Sander, Alfons Schnitzler, Andrea A Kuhn, Esther Florin. Deep brain stimulation device-specific artefacts in MEG recordings. doi: tba","Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation), Project ID 424778381, TRR 295.",,doi:10.18112/openneuro.ds004738.v1.0.1,,"dip13, dip13mov",,MEG,,1,1,0
ds004745,2023-09-08 14:10:18,Velu Prabhakar Kumaravel,1.0.1,8-Channel SSVEP EEG Dataset with Artifacts Trials,2023-09-08 14:13:02,0,1,253839000,0 TB,30,6,0,0,1.8.0,CC0,"Velu Prabhakar Kumaravel, Victor Kartsch, Simone Benatti, Giorgio Vallortigara, Elisabetta Farella, Marco Buiatti",,,NeuroSoNew ERC PoC Grant 842243,10.1109/EMBC46164.2021.9629771,doi:10.18112/openneuro.ds004745.v1.0.1,,unnamed,8.1.0,EEG,"Dataset consists of 6 participants who performed SSVEP tasks. We designed stimulations at 3 different frequencies (2 Hz, 4 Hz, 8 Hz). Each participant attended to 3 trials for each frequency in which they remained static as much as possible to avoid artifacts. They attended to 3 trials for each frequency in which they made voluntary head/neck and eye movements. Please refer to Kumaravel et al., (IEEE EMBC 2021) for further details.",1,1,0
ds004752,2023-09-13 13:22:42,Vasileios S. Dimakopoulos,1.0.1,"Dataset of intracranial EEG, scalp EEG and beamforming sources from epilepsy patients performing a verbal working memory task",2023-09-13 16:26:52,0,8,10984400000,0.011 TB,904,15,18,56,1.4.0,CC0,"Vasileios Dimakopoulos, Lennart Stieglitz, Lukas Imbach, Johannes Sarnthein",We thank the physicians and the staff at Schweizerische Epilepsie-Klinik for their assistance and the patients for their participation. We acknowledge the grant awarded by the Swiss National Science Foundation (funded by SNSF 204651 to JS) and a scholarship by Alexander S Onassis Foundation (to VD). The funders had no role in the design or analysis of the study.,Please cite this paper: https://doi.org/10.7554/eLife.78677,"Swiss National Science Foundation, SNSF 204651 ===NEMAR-SEP=== Alexander S Onassis Foundation PhD Scholarship","Vasileios Dimakopoulos, Pierre Mégevand, Lennart H Stieglitz, Lukas Imbach, Johannes Sarnthein (2022) Information flows from hippocampus to auditory cortex during replay of verbal working memory items eLife 11:e78677. https://doi.org/10.7554/eLife.78677",doi:10.18112/openneuro.ds004752.v1.0.1,"The participants provided written informed consent for the study, which was approved by the institutional ethics review board (PB 2016–02055).",verbalWM,,"iEEG, EEG","### Dataset of intracranial EEG, scalp EEG and beamforming sources from human epilepsy patients performing a verbal working memory task
 

 #### Description
 We present an electrophysiological dataset recorded from fifteen subjects during a verbal working memory task. Subjects were epilepsy patients undergoing intracranial monitoring for localization of epileptic seizures. Subjects performed a modified Sternberg task in which the encoding of memory items, maintenance, and recall were temporally separated. The dataset includes simultaneously recorded scalp EEG with the 10-20 system, intracranial EEG (iEEG) recorded with depth electrodes, waveforms, and the MNI coordinates and anatomical labels of all intracranial electrodes. The dataset includes also reconstructed virtual sensor data that were created by performing LCMV beamforming on the EEG at specific brain regions including, temporal superior lobe, lateral prefrontal cortex, occipital cortex, posterior parietal cortex, and Broca. Subject characteristics and information on sessions (set size, match/mismatch, correct/incorrect, response, response time for each trial) are also provided. This dataset enables the investigation of working memory by providing simultaneous scalp EEG and iEEG recordings, which can be used for connectivity analysis, alongside reconstructed beamforming EEG sources that can enable further cognitive analysis such as replay of memory items.
 

 ### Repository structure
 

 ### Main directory (verbal WM)
 

 Contains metadata files in the BIDS standard about the participants and the study. Folders are explained below.
 

 ### Subfolders
 

 -  verbalWM/sub-/: Contains folders for each subject, named sub- and session information.
 -  verbalWM/sub-/ses-/ieeg/: Contains the raw iEEG data in .edf format for each subject. Each subject performed more than 1 working memory session (ses-0x) each of which includes ~50 trials. Each *ieeg.edf file contains continuous iEEG data during the working memory task. Details about the channels are given in the corresponding .tsv file. We also provide the information on the trial start and end in the events.tsv files by specifying the start and end sample of each trial.
 -  verbalWM/sub-/ses-/eeg/: Contains the raw EEG data in .edf format for each subject. Each subject performed more than 1 working memory session (ses-0x) each of which includes ~50 trials. Each *eeg.edf file contains continuous EEG data during the working memory task. Details about the channels are given in the corresponding .tsv file. We also provide the information on the trial start and end in the events.tsv files by specifying the start and end sample of each trial.
 - verbalWM/derivatives/sub-/: Contains the LCMV beamforming sources during encoding and maintenance. The beamforming sources are in the form of virtual EEG sensors each of which corresponds to a specific brain region. The naming convention used for the virtual sensors is the following: DLPFC; dorsolateral pre-frontal cortex, OFC; orbitofrontal cortex, PPC; posterior parietal cortex, AC; auditory cortex, V1; primary visual cortex
 

 ### BIDS Conversion
 

 bids-starter-kid and custom Matlab scripts were used to convert the dataset into BIDS format.
 

 ### References
 [1] Dimakopoulos V, Megevand P, Stieglitz LH, Imbach L, Sarnthein J. Information flows from hippocampus to auditory cortex during replay of verbal working memory items. Elife 2022;11. 10.7554/eLife.78677
 

 [2] Boran E, Fedele T, Klaver P, Hilfiker P, Stieglitz L, Grunwald T, et al. Persistent hippocampal neural firing and hippocampal-cortical coupling predict verbal working memory load. Science Advances 2019;5(3):eaav3687. 10.1126/sciadv.aav3687
 

 [3] Boran E, Fedele T, Steiner A, Hilfiker P, Stieglitz L, Grunwald T, et al. Dataset of human medial temporal lobe neurons, scalp and intracranial EEG during a verbal working memory task. Scientific Data 2020;7(1):30. 10.1038/s41597-020-0364-3",1,1,0
ds004770,2023-09-22 16:13:23,riyo ueda,1.0.0,iEEG on children during gameplay,2023-09-22 19:54:52,0,2,9325610000,8.7 GB,93,10,9,20,1.7.0,CC0,"Riyo Ueda, Kazuki Sakakura, Takumi Mitsuhashi, Masaki Sonoda, Ethan Firestone, Naoto Kuroda, Yu Kitazawa, Hiroshi Uda, Aimee F. Luat , Elizabeth L. Johnson, Noa Ofen, Eishi Asano",,N/A,N/A,,doi:10.18112/openneuro.ds004770.v1.0.0,IRB of Wayne State University,game,,iEEG,"Dataset of intracranial EEG from human epilepsy patients performing a visuospatial working memory task
 

 Description:
 

 We present an electrophysiological dataset recorded from ten subjects during a visuospatial working memory task. Subjects were epilepsy patients undergoing intracranial monitoring for localization of epileptic seizures. Subjects completed 60 trials (five sessions) of Memory Matrix - a visuospatial working memory game on the Lumosity platform (https://www.lumosity.com/; Lumos Labs, Inc, San Francisco, CA) - during interictal iEEG recording.
 

 Repository structure:
 

 Main directory (iEEG from children during gameplay)
 Contains iEEG files of each participant in the study. Folders are explained below.
 

 Subfolders:
 

 1. sub-/: Contains folders for each subject, named sub- and session information.
 2. sub-/ses-: Contains folders for base and task. 
 3. sub-/ses-/ieeg/: Contains the raw iEEG data in .edf format for each subject. Each subject performed 60 working memory trials (ses-task). Each *ieeg.edf file contains continuous iEEG data during the working memory task. Details about the channels are given in the corresponding .tsv file. We also provide the information on the timing of the stimulus onset and finger tapping on ieeg/edf file by specifying the start and end sample of each trial. (101 is for task display, 401 is for finger tapping to the successful grid, and 501 is for finger tapping to the failed grid). Each subject also had baseline periods (ses-base). To establish baseline, we selected 60 non-overlapping 2,000-ms time windows during periods of spontaneous, resting, eye-open wakefulness immediately preceding the game sessions.",1,0,0
ds004771,2023-09-25 3:25:26,Iris Kuo,1.0.0,EEG/ERP data from a Python Reading Task,2023-09-25 3:48:57,0,1,1462180000,0.001 TB,327,61,18,33,1.8.0,CC0,"Chu-Hsuan Kuo, Chantel S. Prat",Thank you to Mark Pettet for assistance with data analysis and the conversion of data to BIDS format.,,"Office of Naval Research, Cognitive Science of Learning program (N00014-20-1-2393)",See manuscript,doi:10.18112/openneuro.ds004771.v1.0.0,All experimental procedures were approved by the University of Washington Institutional Review Board,PY,8.1.0,EEG,"EEG data for the Python reading task (acceptability judgments) described in [Kuo, C-H. and Prat, C.S. Programmers show distinct, language-like brain responses to violations in form and meaning when reading code], pending submission to Nature Communications.
 

 This study recruited 62 total subjects. 1 subject did not complete the EEG session and was removed from all analyses and is not included in this dataset. The remaining 61 individuals' EEG data are included. The participants info file contains information regarding which individuals were included in the final analyses (per artifact rejection criteria detailed in the article).
 

 The stimuli for this study was administered in Presentation; as such, the files are in the formats compatible with this program.
 

 The provided code was used for processing the EEG data. All statistics were run in Jamovi, an R-based open source software; feel free to reach out for the original files if you are interested.",1,1,0
ds004785,2023-09-29 20:10:02,Scott Boebinger,1.0.1,EEG data for paper titled - Precise cortical contributions to feedback sensorimotor control during reactive balance,2023-10-02 13:59:41,0,1,368224000,0 TB,74,17,0,0,1.8.0,CC0,"Scott Boebinger, Aiden Payne, Giovanni Martino, Kennedy Kerr, Jasmine Mirdamadi, J. Lucas McKay, Michael Borich, Lena Ting",,,,,doi:10.18112/openneuro.ds004785.v1.0.1,,unnamed,8.1.0,EEG,"Electroencephalography data for paper titled ""Precise cortical contributions to feedback sensorimotor control during reactive balance""",1,0,0
ds004789,2023-10-10 20:42:35,Haydn Herrema,3.0.1,Delayed Free Recall of Word Lists,2023-10-10 22:55:36,0,12,618809000000,576.3 GB,7425,280,18,65,1.7.0,CC0,"Haydn G. Herrema, Michael J. Kahana",,,,,doi:10.18112/openneuro.ds004789.v3.0.1,,FR1,,iEEG,"### Delayed Free Recall of Word Lists
 

 #### Description
 This dataset contains behavioral events and intracranial electrophysiological recordings from a delayed free recall task. The experiment consists of participants studying a list of words, presented visually one at a time, completing simple arithmetic problems that function as a distractor, and then freely recalling the words from the just-presented list in any order. The data was collected at clinical sites across the country as part of a collaboration with the Computational Memory Lab at the University of Pennsylvania.
 

 #### To Note
 * The iEEG recordings are labeled either ""monopolar"" or ""bipolar."" The monopolar recordings are referenced (typically a mastoid reference), but should always be re-referenced before analysis. The bipolar recordings are referenced according to a paired scheme indicated by the accompanying bipolar channels tables.
 * Each subject has a unique montage of electrode locations. MNI and Talairach coordinates are provided when available, along with brain region annotations.
 * Recordings were made on multiple different systems, so we have done the scaling to provide all voltage values in V.
 

 #### Contact
 For questions or inquiries, please contact sas-kahana-sysadmin@sas.upenn.edu.",1,0,0
ds004802,2023-10-17 8:13:37,Joe Bathelt,1.0.0,Pilot data for Loneliness in the Brain: Distinguishing Between Hypersensitivity and Hyperalertness,2023-10-19 10:58:01,0,1,10810000000,0.011 TB,139,38,0,0,1.8.0,CC0,"Joe Bathelt, Marte Otten",,,,https://osf.io/c2svz/?view\_only=4ee744ac88c74f41a4d955824a69284b,doi:10.18112/openneuro.ds004802.v1.0.0,,RovingOddball,3.0.0,EEG,,1,0,0
ds004816,2023-10-24 5:14:16,Tijl Grootswagers,1.0.1,EEG-attention-rsvp-exp1,2023-10-26 0:01:55,0,1,9731820000,0.01 TB,84,20,19,41,1.0.2,CC0,"Grootswagers, Tijl, Robinson, Amanda, Shatek, Sofia, Carlson, Thomas",,"Grootswagers T., Robinson A.K., Shatek S.M., Carlson T.A. (2021). The neural dynamics underlying prioritisation of task-relevant information. Neurons, Behaviour, Data Analysis, and Theory, 5(1) https://doi.org/10.51628/001c.21174",,https://doi.org/10.51628/001c.21174,doi:10.18112/openneuro.ds004816.v1.0.1,,rsvp,,EEG,"EEG data for Grootswagers et al 2021 experiment 1 (small letters on big objects)
 

 Grootswagers T., Robinson A.K., Shatek S.M., Carlson T.A. (2021). The neural dynamics underlying prioritisation of task-relevant information. Neurons, Behaviour, Data Analysis, and Theory, 5(1) https://doi.org/10.51628/001c.19129
 

 See also https://osf.io/7zhwp/ and https://openneuro.org/datasets/ds004817",1,0,0
ds004817,2023-10-24 5:15:21,Tijl Grootswagers,1.0.1,EEG-attention-rsvp-exp2,2023-10-26 0:01:25,0,1,10808500000,0.011 TB,83,20,19,36,1.0.2,CC0,"Grootswagers, Tijl, Robinson, Amanda, Shatek, Sofia, Carlson, Thomas",,"Grootswagers T., Robinson A.K., Shatek S.M., Carlson T.A. (2021). The neural dynamics underlying prioritisation of task-relevant information. Neurons, Behaviour, Data Analysis, and Theory, 5(1) https://doi.org/10.51628/001c.21174",,https://doi.org/10.51628/001c.21174,doi:10.18112/openneuro.ds004817.v1.0.1,,rsvp,,EEG,"EEG data for Grootswagers et al 2021 experiment 2 (small objects on big letters)
 

 Grootswagers T., Robinson A.K., Shatek S.M., Carlson T.A. (2021). The neural dynamics underlying prioritisation of task-relevant information. Neurons, Behaviour, Data Analysis, and Theory, 5(1) https://doi.org/10.51628/001c.19129
 

 See also https://osf.io/7zhwp/ and https://openneuro.org/datasets/ds004816",1,0,0
ds004819,2023-10-25 17:12:15,Angelique Paulk,1.0.0,"Flexible, Scalable, High Channel Count Stereo-Electrode for Recording in the Human Brain",2023-10-26 0:06:46,0,1,722128000,0.001 TB,30,1,0,14,1.7.0,CC0,"Keundong Lee, Angelique C. Paulk, Yun Goo Ro, Daniel R. Cleary, Karen J. Tonsfeldt, Yoav Kfir, John Pezaris, Youngbin Tchoe, Jihwan Lee, Andrew M. Bourhis, Ritwik Vatsyayan, Joel R. Martin, Samantha M. Russman, Jimmy C. Yang, Amy Baohan, R. Mark Richardson, Ziv M. Williams, Shelley I. Fried, Hoi Sang U, Ahmed M. Raslan, Sharona Ben-Haim, Eric Halgren, Sydney S. Cash, Shadi. A. Dayeh","We are grateful for the technical support from the nano3 cleanroom facilities at UCSD’s Qualcomm Institute where the depth electrode fabrication was conducted. This work was performed, in part, at the San Diego Nanotechnology Infrastructure (SDNI) of UCSD, a member of the National Nanotechnology Coordinated Infrastructure, which is supported by the NSF (grant ECCS1542148). We thank Yangling Chou, Aaron Tripp, Fausto Minidio, Daniel J. Soper, and Alexandra O’Donnell for their help in collecting the data.","This project contains the data for the publication Lee et al, ""Flexible, Scalable, High Channel Count Stereo-Electrode for Recording in the Human Brain"". It contains the raw and preprocessed (epoched) intracranial EEG (iEEG) data files for multiple species to test novel high resolution micro-stereo-electrodes for recording neural activity in the brain. The data set involves the use of direct electrical stimulation to examine effects of stimulation in the brain. Contact: Shadi Dayeh (sdayeh@eng.ucsd.edu) or Angelique C. Paulk (apaulk@mgh.harvard.edu). If you use this data as a part of any publications, please use the following citation: Flexible, Scalable, High Channel Count Stereo-Electrode for Recording in the Human Brain. Keundong Lee, Angelique C. Paulk, Yun Goo Ro, Daniel R. Cleary, Karen J. Tonsfeldt, Yoav Kfir, John Pezaris, Youngbin Tchoe, Jihwan Lee, Andrew M. Bourhis, Ritwik Vatsyayan, Joel R. Martin, Samantha M. Russman, Jimmy C. Yang, Amy Baohan, R. Mark Richardson, Ziv M. Williams, Shelley I. Fried, Hoi Sang U, Ahmed M. Raslan, Sharona Ben-Haim, Eric Halgren, Sydney S. Cash, Shadi. A. Dayeh. bioRxiv 2022.11.08.515705; doi: https://doi.org/10.1101/2022.11.08.515705.",NIH UG3NS123723-01 ===NEMAR-SEP=== NIH R01NS123655-01 ===NEMAR-SEP=== NIH NBIB DP2-EB029757 ===NEMAR-SEP=== NIH MH120886-01 ===NEMAR-SEP=== NSF 1728497 ===NEMAR-SEP=== NSF 1351980 ===NEMAR-SEP=== NSF DGE-1650112 ===NEMAR-SEP=== NIH K24-NS088568 ===NEMAR-SEP=== NIH R01-NS062092 ===NEMAR-SEP=== Tiny Blue Dot Foundation ===NEMAR-SEP=== NIH NS047101 ===NEMAR-SEP=== NIH K99 NS119291 ===NEMAR-SEP=== MGH - ECOR,"Flexible, Scalable, High Channel Count Stereo-Electrode for Recording in the Human Brain. Keundong Lee, Angelique C. Paulk, Yun Goo Ro, Daniel R. Cleary, Karen J. Tonsfeldt, Yoav Kfir, John Pezaris, Youngbin Tchoe, Jihwan Lee, Andrew M. Bourhis, Ritwik Vatsyayan, Joel R. Martin, Samantha M. Russman, Jimmy C. Yang, Amy Baohan, R. Mark Richardson, Ziv M. Williams, Shelley I. Fried, Hoi Sang U, Ahmed M. Raslan, Sharona Ben-Haim, Eric Halgren, Sydney S. Cash, Shadi. A. Dayeh. bioRxiv 2022.11.08.515705; doi:https://doi.org/10.1101/2022.11.08.515705.",doi:10.18112/openneuro.ds004819.v1.0.0,"All patients voluntarily participated after fully informed consent as monitored by the Partners Institutional Review Board covering Massachusetts General Hospital (MGH). Participants were informed that participation in the stimulation tests would not alter their clinical treatment in any way, and that they could withdraw at any time without jeopardizing their clinical care. All rodent and pig experiments were approved by the UCSD Institutional Animal Care and Use Committee (IACUC, protocol # S19030). For NHPs, all efforts were made to minimize discomfort, and the Institutional Animal Care and Use Committee at the Massachusetts General Hospital monitored care and approved all procedures.",,,iEEG,"This project contains the data for the publication Lee et al, ""Flexible, Scalable, High Channel Count Stereo-Electrode for Recording in the Human Brain"". It contains the raw and preprocessed (epoched) intracranial EEG (iEEG) data files for multiple species to test novel high resolution micro-stereo-electrodes for recording neural activity in the brain. The data set involves the use of direct electrical stimulation to examine effects of stimulation in the brain. 
 

 Data are in the iEEG-BIDS format with binary files and channel maps included in the related derivatives folder.",1,0,0
ds004837,2023-11-01 15:54:14,Fran López-Caballero,1.0.0,Magnetoencephalographic (MEG) Pitch and Duration Mismatch Negativity (MMN) in First-Episode Psychosis,2023-11-02 20:12:01,0,1,113857000000,106 GB,243,60,0,0,1.1.1,CC0,"Fran López-Caballero, Mark Curtis, Brian Coffman, Dean Salisbury",,European Journal of Neuroscience. 2023;1-18. doi:10.1111/ejn.16107,R01 MH108568 and MH113533 NIH/United States,,doi:10.18112/openneuro.ds004837.v1.0.0,All procedures were approved by the University of Pittsburgh IRB and in accordance with the WMA Declaration of Helsinki Ethical Principles for Medical Research Involving Human Subjects.,"attnmod, p_five",,"MEG, MRI","Project Title: Pittsburgh Early Psychosis Program (PEPP): Mismatch Negativity (MMN) in First-Episode Psychosis
 

 Expected experimentation period:
 Start date: 12/01/2017
 End date: 09/15/2021
 

 Project Description:
 Oddball paradigm with standards, pitch deviants and duration deviants. Extended description at: https://doi.org/10.1111/ejn.16107
 

 Participant categories:
 Healthy controls (HC), First-episode Schizophrenia (FESZ), First-episode Affective Psychosis (FEAFF)
 Further information about clinical, neuropsychological, demographic and medication data can be found in derivatives/participants.csv
 

 Events:
 1: standard
 2: pitch deviant
 3: duration deviant
 

 Funding: 
 National Institutes of Health (R01 MH108568 and MH113533)",1,0,0
ds004840,2023-11-12 15:48:23,Jose Gabriel,1.0.1,"Dataset of electrophysiological signals (EEG, ECG, EMG) during Music therapy with adult burn patients in the Intensive Care Unit.",2023-11-12 22:23:16,0,2,628619000,0.585 GB,107,9,18,65,1.8.0,CC0,"Jose Cordoba-Silva, Rafael Maya, Mario Valderrama, Luis Felipe Giraldo, William Betancourt-Zapata, Andrés Salgado-Vascob, Juliana Marín-Sánchez, Viviana Gómez-Ortega, Mark Ettenberger",,,,,doi:10.18112/openneuro.ds004840.v1.0.1,The study was approved by the ethics committee of the FSFB (CCEI-11234-2019/CCEI-11971-2020) and is registered at Clinicaltrials.gov (NCT04571255) - All participants signed informed consent,"task-musicTherapy, task-postMusicTherapy, task-preMusicTherapy",,EEG,"# Dataset of electrophysiological signals (EEG, ECG, EMG) during Music therapy with adult burn patients in the Intensive Care Unit - README
 

 ## Table of Contents
 

 - [1. Experimental Design](#1-experimental-design)
  - [1.1 Study Overview](#11-study-overview)
  - [1.2 Electrophysiological Measurements](#12-electrophysiological-measurements)
 - [2. Pain Perception and Anxiety-Depression Levels](#2-pain-perception-and-anxiety-depression-levels)
 - [3. Code GIT repository](#3-code-GIT-repository)
 

 ## 1. Experimental Design
 

 ### 1.1 Study Overview
 

 This dataset forms part of an ongoing single-site Randomized Clinical Trial (RCT) involving adult burn patients admitted to the Intensive Care Unit (ICU). The key details of the study include:
 

 - **Participants**: The study encompasses 82 adult burn patients admitted to the ICU.
 - **Randomization**: Participants were randomly assigned to either an intervention group or a control group in a 1:1 ratio. The intervention group received standard care in addition to a maximum of six Music therapy sessions provided by a certified music therapist over a 2-week period.
 - **Electrophysiological Measures**: Electrophysiological measures were taken from a subset of 9 participants in the intervention group (11%).
 - **Ethics and Registration**: The study was approved by the ethics committee of the Fundación Santa Fe de Bogotá (FSFB) with approval IDs CCEI-11234-2019 and CCEI-11971-2020. It is registered on Clinicaltrials.gov under the identifier NCT04571255.
 - **Informed Consent**: All participants provided informed consent to participate in the study.
 

 ### 1.2 Electrophysiological Measurements
 

 - **Participants**: The electrophysiological measurements were conducted with nine adult burn patients hospitalized in the ICU of the University Hospital Fundación Santa Fe de Bogotá (FSFB).
 - **Inclusion Criteria**: Inclusion criteria involved individuals of legal adult age with an expected hospitalization period of more than 7 days. Patients with known psychiatric disorders, cognitive disabilities, sedation, or mechanical ventilation were excluded. Patients with burns in regions above the neck were also excluded.
 - **Measurement Sessions**: Electrophysiological measurements were performed with each patient during two Music-Assisted Relaxation (MAR) sessions on two different days.
 - **Recording Phases**: Each recording session included three phases:
  - Pre-Intervention (PRE): The resting state was measured as a baseline with the patient's eyes closed or fixed at a point.
  - MAR MTI: The specialist performed the MAR MTI.
  - Post-Intervention (POST): Measurements were taken during the patient's reincorporation after MAR.
 - **Equipment**: Recordings were made with the Micromed LTM64 equipment with a sampling frequency of at least 256Hz. The Micromed LTM64 is of clinical quality and has approval from the Colombian National Institute for Drug and Food Safety (approval ID: 20090486-2015).
 - **Electrode Setup**:
  - EEG: The electrode montage followed the international System 10-20. Due to time limitations, the number of electrodes was reduced to eight: FP1, FP2, T3, T4, C3, C4, O1, and O2. The reference electrode was set to Cz, and the ground electrode was placed on the mastoids.
  - ECG: ECG was acquired by a bipolar assembly of lead II with two electrodes located bilaterally in the upper part of the thorax or both arms, depending on each patient's possibilities or limitations.
  - EMG: For EMG, a bipolar electrode configuration was positioned on the left eyebrow to assess the motor activity of the corrugator supercilii muscle. The electrodes were placed with a 20 mm distance between them, following the natural alignment of the muscle fibers.
 

 ## 2. Pain Perception and Anxiety-Depression Levels
 

 - **Measures**: To correlate pain, anxiety, and depression levels with electrophysiological signals, two complementary measures were obtained.
 - **Pain Assessment**: A Visual Analog Scale (VAS) was administered before the PRE and after the POST. The VAS ranged from 0 (indicating no pain) to 10 (representing the maximum pain possible).
 - **Anxiety and Depression**: The Colombian version of the Hospital Anxiety and Depression Scale (HADS) was used. HADS consists of two sub-scales for anxiety and depression, each containing seven items. Items are rated on a four-point Likert scale (0 to 3), with higher scores indicating increased anxiety or depression levels (maximum score of 21 for each subscale). HADS was administered after obtaining informed consent, both as a baseline and after the last music therapy sessions.
 

 ## 3. Code GIT repository
 - All the analysis and graphics in this project were conducted using Python 3.7. We utilized custom scripts in combination with various libraries, such as NumPy, Pandas, SciPy, MNE, Biopsy, neurokit2, and Visbrain. After generating the graphics, we enhanced them in PowerPoint by adding titles, symbols, and any necessary supplementary information.
 - Code is Open by MIT license at: https://github.com/jgcordoba/BurnICU_MusicTherapy_Signals.git
 

 

 For more detailed information about the study, please refer to the associated article titled ""Article not submitted"", DOI: '-Insert Link-'
 

 ---
 

 **Last Update**: 23/10/2023
 **Author**: Jose Gabriel Cordoba Silva",1,0,0
ds004841,2023-11-13 17:40:25,Kevin King,1.0.1,TX14,2023-11-13 17:59:29,0,2,7846930000,7.3 GB,1034,20,0,0,1.8.0,CC0,"Gabriella Larkin, James A. Davis, Victor Paul, Marcel Cannon, Chris Manteuffel, Ben Brewster, Tony Johnson, Mike Dunkel, Stephen Gordon, Kevin King",,,,,doi:10.18112/openneuro.ds004841.v1.0.1,,DriveOnMission,8.1.0,EEG,"TX14 dataset: Perform a local situational awareness task while maintaining supervisory control of a semi-autonomous vehicle. 
 

 This Army’s transition to a leaner, more agile and rapidly-deployable force requires the advent of autonomous technologies and systems, and more reliance on computers and machines. This move from traditional warfare to FCS represents a shift in the human role, as well. Technological advancement has made it so that the role of the user has been transformed from active controller to system monitor and manager, intervening only in the case of a problem. As such, the soldier’s dependency on robotics technologies, tele-operations, indirect driving and autonomy is expected to increase significantly. Additionally, although semi-autonomous driving technologies have proven beneficial in aggregate measures of local area awareness (i.e., target/threat detection) and vehicle control, it is important to understand the situational trade-offs between local area awareness and vehicle control, as situational trade-offs provide the basis for developing dynamic task allocation within Crewstations.",1,0,1
ds004842,2023-11-13 18:06:58,Kevin King,1.0.0,TX15,2023-11-13 18:17:55,0,2,5589050000,5.2 GB,719,14,0,0,1.8.0,CC0,"Gabriella Larkin, James A. Davis, Victor Paul, Marcel Cannon, Chris Manteuffel, Ben Brewster, Tony Johnson, Mike Dunkel, Stephen Gordon, Kevin King",,,,,doi:10.18112/openneuro.ds004842.v1.0.0,,DriveOnMission,8.1.0,EEG,TX15 dataset,1,0,1
ds004843,2023-11-13 18:52:00,Kevin King,1.0.0,T16,2023-11-13 19:01:55,0,1,8229210000,7.7 GB,649,14,0,0,1.8.0,CC0,"Tony Johnson, Stephen Gordon, Jon Touryan, Kevin King",,,,,doi:10.18112/openneuro.ds004843.v1.0.0,,VisualSituationalAwareness,8.1.0,EEG,TX16 dataset,1,0,1
ds004844,2023-11-13 19:09:08,Kevin King,1.0.0,T22,2023-11-13 19:24:20,0,4,23976100000,22.3 GB,481,17,0,0,1.8.0,CC0,"Jason S. Metcalfe, Victor Paul, Benamin Haynes, Corey Atwater, Amar Marathe, Gregory Gremillion, Kim Drnec, William Nothwang, Justin R. Estepp, Margaret Bowers, Jamie Lukos, Tony Johnson, Mike Dunkel, Stephen Gordon, Jon Touryan, Kevin King",,,,,doi:10.18112/openneuro.ds004844.v1.0.0,,Drive,8.1.0,EEG,"TX22 dataset: Predicting and influencing trust-based decisions about control authority hand-off and take-over during simulated, semi-automated driving in a leader-follower paradigm.Vehicle survivability is critically important in todays military. Significant DoD investments have focused on developing and integrating autonomous vehicle technologies to mitigate the effects of human error and thus enhance surviability and mission effectiveness. In a previous experiment (SANDR designation: ARL_TX20), we explored how a human operators acceptance and use of advanced technology is influenced by their trust and related factors, like subjective workload and automation reliability. Nevertheless, more critical than measuring and achieving a certain level of trust is the need for a capability to resolve observed (or predicted) discrepancies between trust and trustworthiness that will undermine effective joint system performance. Using the same paradigm as we developed for our previous experiment (ARL_TX20), here we explore our ability to (a) make accurate real-time predictions of instances where intervention is necessary and (b) use those predictions to provide feedback to the driver that is intended to support active ""trust management"" by influencing the trust-based decisions of the driver.",1,0,1
ds004849,2023-11-14 15:26:32,Kevin King,1.0.0,STRONG,2023-11-14 15:37:02,0,1,83056600,0.077 GB,10,1,0,0,1.8.0,CC0,"Tony Johnson, Stephen Gordon, Jon Touryan, Kevin King",,,,,doi:10.18112/openneuro.ds004849.v1.0.0,,nback,8.1.0,EEG,"STRONG dataset
 

 This is a placeholder dataset.",1,0,1
ds004850,2023-11-14 15:27:15,Kevin King,1.0.0,ODE,2023-11-14 15:37:08,0,1,83056600,0.077 GB,10,1,0,0,1.8.0,CC0,"Tony Johnson, Stephen Gordon, Jon Touryan, Kevin King",,,,,doi:10.18112/openneuro.ds004850.v1.0.0,,nback,8.1.0,EEG,"ODE dataset
 

 This is a placeholder dataset.",1,0,1
ds004851,2023-11-14 15:33:49,Kevin King,1.0.0,HID,2023-11-14 15:37:12,0,1,83056600,0.077 GB,10,1,0,0,1.8.0,CC0,"Tony Johnson, Stephen Gordon, Jon Touryan, Kevin King",,,,,doi:10.18112/openneuro.ds004851.v1.0.0,,nback,8.1.0,EEG,"HID dataset
 

 This is a placeholder dataset.",1,0,1
ds004852,2023-11-14 15:34:20,Kevin King,1.0.0,InsurgentCivilian,2023-11-14 15:37:16,0,1,83056700,0.077 GB,10,1,0,0,1.8.0,CC0,"Tony Johnson, Stephen Gordon, Jon Touryan, Kevin King",,,,,doi:10.18112/openneuro.ds004852.v1.0.0,,nback,8.1.0,EEG,"InsurgentCivilian dataset
 

 This is a placeholder dataset.",1,0,1
ds004853,2023-11-14 15:34:46,Kevin King,1.0.0,TX17,2023-11-14 15:37:20,0,1,83056600,0.077 GB,10,1,0,0,1.8.0,CC0,"Tony Johnson, Stephen Gordon, Jon Touryan, Kevin King",,,,,doi:10.18112/openneuro.ds004853.v1.0.0,,nback,8.1.0,EEG,"TX17 dataset
 

 This is a placeholder dataset.",1,0,1
ds004854,2023-11-14 15:35:17,Kevin King,1.0.0,TX18,2023-11-14 15:37:24,0,1,83056600,0.077 GB,10,1,0,0,1.8.0,CC0,"Tony Johnson, Stephen Gordon, Jon Touryan, Kevin King",,,,,doi:10.18112/openneuro.ds004854.v1.0.0,,nback,8.1.0,EEG,"TX18 dataset
 

 This is a placeholder dataset.",1,0,1
ds004855,2023-11-14 15:35:42,Kevin King,1.0.0,FT,2023-11-14 15:37:29,0,1,83056600,0.077 GB,10,1,0,0,1.8.0,CC0,"Tony Johnson, Stephen Gordon, Jon Touryan, Kevin King",,,,,doi:10.18112/openneuro.ds004855.v1.0.0,,nback,8.1.0,EEG,"FT dataset
 

 This is a placeholder dataset.",1,0,1
ds004859,2023-11-22 4:07:20,Kaz Sakakura,1.0.0,iEEG on children during Stroop task,2023-11-23 4:12:37,0,2,2435510000,2.3 GB,40,7,10,17,1.7.0,CC0,"Kazuki Sakakura, Eishi Asano",,N/A,N/A,,doi:10.18112/openneuro.ds004859.v1.0.0,IRB of Wayne State University,stroop,,iEEG,,1,0,0
ds004860,2023-11-23 14:54:51,Flora Schwartz,1.0.0,Investigating the cognitive conflict triggered by moral judgment of accidental harm : an event-related potentials study,2023-11-23 17:52:39,0,1,4065630000,3.8 GB,99,31,19,45,1.8.0,CC0,"Flora Schwartz, Radouane El-Yagoubi, Julie Cayron, Pierre-Vincent Paubel, Bastien Tremoliere",,,,,doi:10.18112/openneuro.ds004860.v1.0.0,,HarmN400,,EEG,,1,0,0
ds004012,2022-02-03 9:47:39,MUHAMMAD HAKIMI BIN MOHD. RASHID,1.0.0,BRAR_NQ,2023-12-21 17:05:19,0,1,84108400000,78.3 GB,949,30,0,0,1.6.0,CC0,"Nur Syairah Ab Rani, Nurfaizatul Aisyah Ab Aziz, Mohammed Farouq Reza, Muzaimi Mustapha",This work was supported by the Universiti Sains Malaysia (USM).,"Cite this data, the study in the references section and consider including the following message: 'This data was obtained from the OpenNeuro database. Its accession number is '",The study has been funded by the USM Research Grant for Fundamental Neuroscience–Neurobehaviour (BrainReward and Anti-Reward) (1002/CNEURO/910114) and USM Research University Grant (1001.PPSP.812189),"Mustapha, M., Rani, N. S. A., Reza, M. F., Wan Daud, W. N., & Ghani, M. A. A. (2016). Neurotechnological Advances in Exploring Melodic Recitation of the Noble Quran: Uncovering the Neural Circuitry in the Human Brain. 229-235. doi:10.1007/978-981-287-778-9_15. (Springer) ===NEMAR-SEP=== http://usmrewardantirewardteam.com/neuroquran-research-group",doi:10.18112/openneuro.ds004012.v1.0.0,,"rest01, stim02, stim01, stim03, stim06, stim04, stim05, stim07, rest02, stim08",,MEG,"References
 ----------
 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., Moreau, J., Oostenveld, R., Schoffelen, J., Tadel, F., Wexler, J., Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5, 180110. https://doi.org/10.1038/sdata.2018.110",1,0,0
ds004784,2023-09-29 18:42:55,Annalisa Salazar,1.0.4,"Phantom EEG Dataset with Motion, Muscle, and Eye Artifacts and Example Scripts",2023-12-20 15:06:58,0,1,1084180000,1 GB,51,1,0,0,1.8.0,CC0,"Ryan J. Downey, Daniel P. Ferris",,"Downey, R.J.; Ferris, D.P. iCanClean Removes Motion, Muscle, Eye, and Line-Noise Artifacts from Phantom EEG. Sensors 2023, 23, 8214. https://doi.org/10.3390/s23198214",,https://doi.org/10.3390/s23198214,doi:10.18112/openneuro.ds004784.v1.0.4,,mixed,8.1.0,EEG,"This phantom experiment contains data collected from a an 
 electrically conductive head phantom. Six conditions were 
 tested: brain-only [no artifacts], or brain with eye, 
 jaw muscle, neck muscle, or motion artifacts present, 
 or brain with all artifacts simultaneously present. 
 Also contained is a copy of the iCanClean plugin for EEGLAB 
 and a set of other helpful scripts that enable parameter sweep 
 testing and validation with ground truth knowledge of the brain 
 signals of interest. Please see derivatives folder and 
 read the How To document within. 
 A copy of iCanClean plugin is in derivatives->Scripts->plugins 
 Please see reference for methodological details 
 https://doi.org/10.3390/s23198214 
 

 - Ryan Downey (December 20, 2023)",1,0,0
ds004883,2023-12-14 11:26:49,Peter Clayson,1.0.0,Registerd Report of ERN During Three Versions of a Flanker Task,2023-12-21 12:36:16,0,1,131859000000,122.8 GB,3618,172,0,0,v1.8.0,CC0,"Peter E. Clayson, Michael J. Larson",Please cite this repository and the associated preprint.,,,https://osf.io/qt2zh/ ===NEMAR-SEP=== https://osf.io/preprints/psyarxiv/a3s42,doi:10.18112/openneuro.ds004883.v1.0.0,,mixed,8.1.0,EEG,"This study is described at https://osf.io/qt2zh/. Scripts used for data processing are posted there.
 

 Here is the script from the manuscript that describes these data.
 

 Error-related negativity is a widely used measure of error monitoring, and many projects are independently moving ERN recorded during a flanker task towards standardization, optimization, and eventual clinical application. However, each project uses a different version of the flanker task and tacitly assumes ERN is functionally equivalent across each version. The routine neglect of a rigorous test of this assumption undermines efforts to integrate ERN findings across tasks, optimize and standardize ERN assessment, and widely apply ERN in clinical trials. The purpose of this registered report was to determine whether ERN shows similar experimental effects (correct vs. error trials) and data quality (intraindividual variability) during three commonly-used versions of a flanker task. ERN was recorded from 172 participants during three versions of a flanker task across two study sites. ERN scores showed numerical differences between tasks, raising questions about the comparability of ERN findings across studies and tasks. Although ERN scores from all three versions of the flanker task yielded high data quality and internal consistency, one version did outperform the other two in terms of the size of experimental effects and the data quality. Exploratory analyses of the error positivity (Pe) provided tentative support for the other two versions of the task over the paradigm that appeared optimal for ERN. The present study provides a roadmap for how to statistically compare psychometric characteristics of ERP scores across paradigms and gives preliminary recommendations for flanker tasks to use for ERN- and Pe-focused studies.",1,0,0
ds004944,2024-01-30 10:41:43,Filippo Costa,1.1.0,Dataset of BCI2000-compatible intraoperative ECoG with neuromorphic encoding,2024-01-30 13:49:34,0,2,473062000,0.441 GB,379,22,1,67,1.4.0,CC0,"Filippo Costa, Niklaus Krayenbühl, Georgia Ramantani, Ece Boran, Kristina König, Johannes Sarnthein",We thank V. Dimakopoulos for help in reformating the data to BIDS. We acknowledge a grant awarded by the Swiss National Science Foundation (funded by the SNSF 204651 to JS and GI with GR and NK as project partners). The funders had no role in the design or analysis of the study.,"If using the raw data, please cite: Ece Boran, Georgia Ramantani, Niklaus Krayenbühl, Maxine Schreiber, Kristina König, Tommaso Fedele, Johannes Sarnthein **High-density ECoG improves the detection of high frequency oscillations that predict seizure outcome**, _Clinical Neurophysiology_ [https://doi.org/10.1016/j.clinph.2019.07.008]
 

 If using the derivatives, please cite: Filippo Costa, Eline Schaft, Geertjan Huiskamp, Erik Aarnoutse, Maryse van’t Klooster, Niklaus Krayenbühl, Georgia Ramantani, Maeike Zijlmans, Giacomo Indiveri, Johannes Sarnthein, **Robust compression and detection of epileptiform patterns in ECoG using a real-time spiking neural network hardware framework**, _Nature Communications_ [https://doi.org/10.1038/s41467-024-47495-y]","Swiss National Science Foundation, SNSF 204651","Ece Boran, Georgia Ramantani, Niklaus Krayenbühl, Maxine Schreiber, Kristina König, Tommaso Fedele, Johannes Sarnthein **High-density ECoG improves the detection of high frequency oscillations that predict seizure outcome**, _Clinical Neurophysiology_ [https://doi.org/10.1016/j.clinph.2019.07.008] ===NEMAR-SEP=== Filippo Costa, Eline Schaft, Geertjan Huiskamp, Erik Aarnoutse, Maryse van’t Klooster, Niklaus Krayenbühl, Georgia Ramantani, Maeike Zijlmans, Giacomo Indiveri, Johannes Sarnthein, **Robust compression and detection of epileptiform patterns in ECoG using a real-time spiking neural network hardware framework**, _Nature Communications_ [https://doi.org/10.1038/s41467-024-47495-y]",doi:10.18112/openneuro.ds004944.v1.1.0,The collection of patient data and their analysis was approved and performed in accordance with the guidelines ===NEMAR-SEP=== and regulations of the local research ethics committee (Kantonale Ethikkommission Zürich 2018–02171). ===NEMAR-SEP=== All patients and/or their parents provided informed consent to reuse their clinical data for research purposes.,acute,,iEEG,"## Overview
 

 This dataset comprises recordings of intraoperative Electrocorticography (ECoG) from 22 patients undergoing resective epilepsy surgery.
 For each patient, the dataset is organized into pre-resection recording (referred to as SITUATION1A) and post-resection recording (referred to as SITUATION2A).
 

 We provide raw ECoG recordings for each patient and a derivative folder that contains all the main processing stages 
 performed with our neuromorphic processing pipeline: [https://doi.org/10.1038/s41467-024-47495-y](https://doi.org/10.1038/s41467-024-47495-y).
 

 The pipeline preprocesses ECoG recordings in real-time and performs Asynchronous Delta Modulator (ADM) encoding with a custom BCI2000 module.
 The ADM-encoded data are processed by a hardware Spiking Neural Network (SNN). The SNN-encoded data are then used to detect epileptiform patterns in the ECoG.
 The code to perform preprocessing and ADM encoding in BCI2000, together with the code to detect epileptiform patterns from SNN-encoded data, are provided at [https://github.com/CostaFilippo/BCI2000_DYNAP-SE.git](https://github.com/CostaFilippo/BCI2000_DYNAP-SE.git). 
 

 In a previous publication, this dataset has been analyzed with a offline software algorithm: [https://doi.org/10.1016/j.clinph.2019.07.008](https://doi.org/10.1016/j.clinph.2019.07.008). 
 

 The annotations of the epileptiform patterns detected with the offline approach are provided at: sub-\*/ses-SITUATION\*/sub-\*_ses-SITUATION\*_task-acute_events.tsv.
 

 The annotations of the epileptiform patterns detected with the online neuromorphic processing are provided at derivative/sub-\*/ses-SITUATION\*/sub-\*_ses-SITUATION*_task-EV.csv.
  
 ## Dataset Structure
 

 The derivative folder is structured as follows:
 

  - sub-*
  - ses-SITUATION1A
  - *task-BCI.dat
  - *task-ADM.csv
  - *task-SNN.csv
  - *task-EV.csv
  - ses-SITUATION2A
  - *task-BCI.dat
  - *task-ADM.csv
  - *task-SNN.csv
  - *task-EV.csv
 

 ### File Descriptions
 

 - derivative
 

  - **task-BCI.dat:* BCI2000-compatible file containing the ECoG recording.
  - **task-ADM.csv:* ADM encoding of the ECoG recording.
  - **task-SNN.csv:* SNN encoding of the ECoG recording.
  - **task-EV.cvs:* Annotations of the detected epileptiform patterns in the ECoG recording.
 

 ## Data Formats
 

 Details of the neuromorphic processing pipeline can be found at [https://doi.org/10.1038/s41467-024-47495-y](https://doi.org/10.1038/s41467-024-47495-y).
 

 #### task-BCI.dat
 

 The BCI2000-compatible file contains the raw ECoG recording.
 

 It can be streamed in real-time using the 'FilePlayback' BCI2000 module.
 

 #### task-ADM.csv
 

 The ADM file is formatted as follows:
 

  - _pulseType_: -1 for DN pulse, +1 for UP pulse.
  - _pulseTime_: Time at which the pulse occurred.
  - _channel_: Channel in which the pulse occurred.
  - _band_: 0 for EEG band, 1 for HFO band
 

 #### task-SNN.csv
 

 The SNN file is formatted as follows:
 

  - _time_: Time at which the SNN neuron activated.
  - _neuronId_: Number id of the SNN neuron (DYNAP-SE numbering from 0 to 1024). 
  - _neuronCounter_: Number id of the SNN neuron (sequential numbering from 0 to 40).
  - _moduleName_: Population (ACC_4_0; ACC_0_4), band (EEG; HFO) and module number (ch 0-7) of the neuron.
  - ACC_4_0 = ACC UP
  - ACC_0_4 = ACC DN
  - _moduleId_: Module number (0-7).
  - _channelId_: Channel for which the SNN neuron activated.
 

 #### task-EV.csv
 

 The EV file contains annotations of the detected epileptiform patterns with the following format:
 

  - _time_: time of the detected event.
  - _channelId_: channel id of the detected event.
  - _location_: channel name of the detected event.
 

 ## Contact Information
 

 For inquiries or additional information, please contact Filippo.Costa@usz.ch or Johannes.Sarnthein@usz.ch
 

 ## Acknowledgements
 

 We thank V. Dimakopoulos for help in reformatting the data to BIDS. We acknowledge a grant awarded by the Swiss National Science Foundation (funded by the SNSF 204651 to JS and GI with GR and NK as project partners). The funder had no role in the design or analysis of the study.",1,0,0
ds005034,2024-03-13 12:26:33,Yuri Pavlov,1.0.1,The effect of theta tACS on working memory,2024-03-20 18:56:20,0,2,65885300000,61.4 GB,406,25,18,39,1.8.0,CC0,"Yuri G. Pavlov, Dauren Kasanov",,https://doi.org/10.1101/2024.03.20.585954,,,doi:10.18112/openneuro.ds005034.v1.0.1,,mixed,8.1.0,EEG,"Following either a 20-minute verum or sham stimulation applied to Fpz-CPz at 1 mA and 6 Hz, the participants performed WM tasks, while EEG was recorded. The task required participants to either mentally manipulate memory items or retain them in memory as they were originally presented. In addition, before the working memory task, resting state EEG with eyes closed was recorded for 3 minutes and with eyes open for 1.5 minutes.
 

 Behavioral performance data are available on OSF (https://osf.io/v2qwc/)",1,0,0
ds004625,2023-07-02 1:56:00,Chang Liu,1.0.2,Mind in Motion Young Adults Walking Over Uneven Terrain,2024-02-03 4:43:05,0,1,67068900000,62.5 GB,3435,32,20,35,v1.0.0,CC0,"Chang Liu, Ryan J. Downey, Jacob S. Salminen, Sofia Arvelo Rojas, Erika M. Pliner, Natalie Richer, Jungyun Hwang, Yenisel Cruz-Almeida, Todd M. Manini, Chris J. Hass, Rachael D. Seidler, David J. Clark, Daniel P. Ferris",Thank you to all undergraduate students who worked on Mind in Motion Project in Human Neuromechanics Lab,,This study was supported by the National Institute of Health (U01AG061389).,,doi:10.18112/openneuro.ds004625.v1.0.2,The study was conducted in accordance with the Declaration of Helsinki and approved by the Institutional Review Board of the University of Florida (IRB 201802227).,mixed,8.1.0,EEG,"Our dataset contains high-density, dual-layer electroencephalography (EEG), neck electromyography (EMG), inertial measurement unit (IMU) acceleration, ground reaction forces, head model constructed from T1 structural MR images from 32 participants walking over uneven terrain and at different speeds. Participants completed two trials for each condition for three minutes and a seated rest trial for three minutes. Digitized electrode locations (txt) are included in each subject folder.
 

 Please refer to our publication for more detail. 
 

 This study was supported by the National Institute of Health (U01AG061389).",1,0,0
ds004951,2024-02-06 14:30:18,Marleen Haupt,1.0.0,Braille letters - EEG,2024-02-12 14:43:20,0,2,23627400000,22 GB,200,11,29,61,1.7.0,CC0,"Marleen Haupt, Monika Graumann, Santani Teng, Carina Kaltenbach, Radoslaw M. Cichy",,,CI241/1-1 ===NEMAR-SEP=== CI241/3-1 ===NEMAR-SEP=== CI241/7-1 ===NEMAR-SEP=== ERC-StG-2018-803370,,doi:10.18112/openneuro.ds004951.v1.0.0,,letters,,EEG,"This dataset contains the raw EEG data accompanying the paper ""The transformation of sensory to perceptual braille letter representations in the visually deprived brain"". Please cite the above paper if you use this data.
 

 The dataset includes:
 

 Brainvision files (.eeg, .vhdr, .vmrk) for all participants.
 

 Please note, for some participants the EEG decording had to be stopped and restarted within a session. In this case, the different files are indicated as separate runs. In addition, some participants completed a second session.
 

 The events files contain the onsets, durations, trial types and values for all trials in the corresponding run. Stimuli are Braille letters (B,C,D,L,M,N,V,Z) presented on Braille cells under the left and right index fingers of participants. Triggers S1-8 are letters presented to the left hand, triggers S9-16 are letters presented to the right hand. 
 

 Other triggers:
 

 starttrigger  = S100;
 trialonset  = S101;
 stimulusonset = S222;
 catchtrial  = S200;
 pedalpress_correct  = S253;
 pedalpress_incorrect = S254;
 endtrigger  = S255;
 

 

 For a full description of the paradigm and the employed procedures please see the paper.  
 

 References for MNE BIDS conversion
 ----------
 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G., Phillips, C., Delorme, A., Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Scientific Data, 6, 103. https://doi.org/10.1038/s41597-019-0104-8",1,0,0
ds004572,2023-05-20 9:38:46,Yeganeh Farahzadi,1.2.1,OTKA PLB-HYP Study1,2023-05-25 10:59:59,0,1,46776300000,43.6 GB,3153,52,0,0,1.6.0,CC0,"Zoltan Kekecs, Yeganeh Farahzadi, Kyra Girán, Vanda Vizkievicz, Anna Lutoskin",,,"Hungarian National Research, Development and Innovation Office (NKFIH, Grant No.: FK 132248)",,doi:10.18112/openneuro.ds004572.v1.2.1,,"baseline1, baseline2, experience1, experience2, experience3, experience4, induction1, induction2, induction3, induction4",,EEG,"52 participants (39 females) took part in this study and their brain electrophysiological activity were being recorded using 64-channel EasyCap from Brain Products. After mounting the EEG electrode cap, the study protocol started with 5 minutes of closed-eyes rest (Pre-hypnosis Baseline), followed by four experimental conditions (Experimental Blocks), and ended with another 5 minutes of closed-eyes rest (Post-hypnosis Baseline). Throughout the four Experimental Blocks, participants were exposed to either conventional or unconventional (placebo) hypnotic inductions described either as hypnosis or as simple relaxation technique in a 2 x 2 balanced placebo design. In other words, each participant underwent four trials, in which they were exposed to a conventional hypnosis induction presented as “hypnosis”; a conventional hypnosis induction presented as “control”; an unconventional hypnosis induction presented as “hypnosis”; and an unconventional hypnosis induction presented as “control” in a randomized order.
 

 See this preprint for more information about our data collection methods: https://www.researchsquare.com/article/rs-3095794/v1",1,0,0
ds004952,2024-02-07 5:08:55,Haiyan Wu,1.2.0,Novel Reading,2024-02-08 18:15:58,0,2,222394000000,207.1 GB,1780,10,0,0,1.7.0,CC0,"Xinyu Mou, Cuilin He, Liwei Tan, Junjie Yu, Huadong Liang, Jianyu Zhang, Tian Yan, Yu-Fang Yang, Ting Xu, Qing Wang, Miao Cao, Zijiao Chen, Chuan-Peng Hu, Xindi Wang, Quanying Liu, Haiyan Wu",,,,"Mou, X., He, C., Tan, L., Yu, J., Liang, H., Zhang, J., Yan, T., Yang, Y.-F., Xu, T., Wang, Q., Cao, M., Chen, Z., Hu, C.-P., Wang, X., Liu, Q., & Wu, H. (2024). ChineseEEG: A Chinese Linguistic Corpora EEG Dataset for Semantic Alignment and Neural Decoding. bioRxiv. Cold Spring Harbor Laboratory. https://doi.org/10.1101/2024.02.08.579481",doi:10.18112/openneuro.ds004952.v1.2.0,,reading,,EEG,"# ChineseEEG: A Chinese Linguistic Corpora EEG Dataset for Semantic Alignment and Neural Decoding
 

 ## Introduction
 

 ""ChineseEEG"" (Chinese Linguistic Corpora EEG Dataset) contains high-density EEG data and simultaneous eye-tracking data recorded from 10 participants, each silently reading Chinese text for about 11 hours. This dataset further comprises pre-processed EEG sensor-level data generated under different parameter settings, offering researchers a diverse range of selections. Additionally, we provide embeddings of the Chinese text materials encoded from BERT-base-chinese model, which is a pre-trained NLP specifically used for Chinese, aiding researchers in exploring the alignment between text embeddings from NLP models and brain information representations.
 

 ## Participant Overview
 

 In total, data from 10 participants were used (18-24 years old, averaged 20.68 years old, and 5 males). No participants reported neurological or psychiatric history. All participants are right-handed and have normal or corrected-to-normal vision. 
 

 ## Experiment Materials
 

 The experimental materials consist of two novels in Chinese, both in the genre of children's literature. The first is **The Little Prince** and the second is **Garnett Dream**. 
 For **The Little Prince**, the preface was used as material for the practice reading phase. The main body of the novel was then used for seven sessions in the formal reading phase. The first six sessions each included 4 chapters of the novel, while the seventh session included the last two chapters.
 For **Garnett Dream**, the first 18 chapters were used for 18 sessions in the formal reading stage, with each session including a complete chapter. 
 

 To properly present the text on the screen during the experiments, the content of each session was segmented into a series of units, with each unit containing no more than 10 Chinese characters. These segmented contents were saved in Excel (.xlsx) format for subsequent usage. During the experiment, three adjacent units from each session's content will be displayed on the screen in three separate lines, with the middle line highlighted for the participant to read. 
 In summary, a total of 115,233 characters (24,324 in **The Little Prince** and 90,909 in **Garnett Dream**), of which 2985 characters were unique, were used as experimental stimuli in ChineseEEG dataset. 
 

 The original and segmented novels are saved in the `derivatives/novels` folder. The `segmented_novel` folder in `novels` folder contains two types of Excel files: one type of file has names ending with ""display,"" while the other type does not contain this suffix. The former stores units that have been segmented; the latter includes units that have been reassembled according to the experimental presentation format. These files ending with ""display"" will be used to support the execution of relevant code, in order to achieve effective stimulus presentation in the experiment. 
 

 The code for generating these two types of files, as well as the code for experimental presentation, can be found in the GitHub repository: https://github.com/ncclabsustech/Chinese_reading_task_eeg_processing.
 

 ## Experiment Procedures
 

 Participants were tasked with reading a novel and were required to keep their heads still and keep their gaze on the highlighted (red) Chinese characters moving across the screen, reading at a pace set by the program. They were required to read an entire novel in multiple runs within a single session. Each run is divided into two phases: the eye-tracker calibration phase and the reading phase.
 

 The eye-tracker calibration phase is at the beginning of each run, requiring participants to keep their gaze at a fixation point, which sequentially appeared at the four corners and the center of the screen.
 

 In the reading phase, the screen initially displayed the serial number of the current chapter. Subsequently, the text appeared with three lines per page, ensuring each line contained no more than ten Chinese characters (excluding punctuation). On each page, the middle line was highlighted as the focal point, while the upper and lower lines were displayed with reduced intensity as the background. Each character in the middle line was sequentially highlighted with red color for 0.35 s, and participants were required to read the novel content following the highlighted cues.
 

 For detailed information about the experiment settings and procedures, please refer to our paper at https://doi.org/10.1101/2024.02.08.579481.
 

 ## Markers
 

 To precisely co-register EEG segments with individual characters during the experiment, we marked the EEG data with triggers. 
 

 - EYES: Eyetracker starts to record
 - EYEE: Eyetracker stops recording
 - CALS: Eyetracker calibration starts
 - CALE: Eyetracker calibration stops
 - BEGN: EGI starts to record
 - STOP: EGI stops recording
 - CHxx:Beginning of specific chapter (Numbers correspond with chapters) 
 - ROWS: Beginning of a row
 - ROWE: End of a row
 - PRES:Beginning of the preface
 - PREE:End of the preface
 

 ## Data Record
 

 The raw EEG data has a sampling rate of 1 kHz, while the filtered data and pre-processed data has a sampling rate of 256 Hz.
 

 ### Data Structure
 

 The dataset is organized following the EEG-BIDS specification using the MNE-BIDS package. The dataset contains some regular BIDS files, 10 participants’ data folders, and a derivatives folder. The stand-alone files offer an overview of the dataset: i) dataset_description.json is a JSON file depicting the information of the dataset, such as the name, dataset type and authors; ii) participants.tsv contains participants’ information, such as age, sex, and handedness; iii) participants.json describes the column attributes in participants.tsv; iv) README.md contains a detailed introduction of the dataset.
 Each participant’s folder contains two folders named ses-LittlePrince and ses-GarnettDream, which store the data of this
 participant reading two novels, respectively. Each of the two folders contains a folder eeg and one file sub-xx_scans.tsv. The tsv
 file contains information about the scanning time of each file. The eeg folder contains the source raw EEG data of several runs,
 channels, and marker events files. Each run includes an eeg.json file, which encompasses detailed information for that run,
 such as the sampling rate and the number of channels. Events are stored in events.tsv with onset and event ID. The EEG data
 is converted from raw metafile format (.mff file) to BrainVision format (.vhdr, .vmrk and .eeg files) since EEG-BIDS is not
 officially compatible with .mff format.
 The derivatives folder contains six folders: eyetracking_data, filtered_0.5_80, filtered_0.5_30, preproc, novels, and text_embeddings. The eyetracking_data folder contains all the eye-tracking data. Each eye-tracking data is formatted in a
 .zip file with eye moving trajectories and other parameters like sampling rate saved in different files. The filtered_0.5_80
 folder and filtered_0.5_30 folder contain data that has been processed up to the pre-processing step of 0.5-80 Hz and 0.5-30
 Hz band-pass filtering respectively. This data is suitable for researchers who have specific requirements and want to perform
 customized processing on subsequent pre-processing steps like ICA and re-referencing. The preproc folder contains minimally
 pre-processed EEG data that is processed using the whole pre-processing pipeline. It includes four additional types of files
 compared to the participants’ raw data folders in the root directory: i) bad_channels.json contains bad channels marked during
 bad channel rejection phase. ii) ica_components.npy stores the values of all independent components in the ICA phase. iii)
 ica_components.json includes the independent components excluded in ICA (the ICA random seed is fixed, allowing for
 reproducible results). iv) ica_components_topography.png is a picture of the topographic maps of all independent components,
 where the excluded components are labeled in grey. The novels folder contains the original and segmented text stimuli materials. The original novels are saved in .txt format and the segmented novels corresponding to each experimental run are saved in Excel (.xlsx) files. The text_embeddings folder contains embeddings of the two novels. The embeddings corresponding to each experimental run are stored in NumPy (.npy) files  
 

 For an overview of the structure, please refer to our paper at https://doi.org/10.1101/2024.02.08.579481.
 

 ### Pre-processing
 

 For the pre-processed data in the derivatives folder, we only did minimal pre-processing to retain most useful information. The pre-processing steps include data segmentation, downsampling, filtering, bad channel interpolation, ICA, averaging. 
 

 During the data segmentation phase, we only retained data from the formal reading phase of the experiment. Based on the
 event markers during the data collection phase, we segmented the data, removing sections irrelevant to the formal experiment
 such as calibration and preface reading. To minimize the impact of subsequent filtering steps on the beginning and end of the
 signal, an additional 10 seconds of data was retained before the start of the formal reading phase. Subsequently, the signal was
 downsampled to 256 Hz.
 Following this, a 50 Hz notch filter was applied to remove the powerline noise from the signal. Next, we performed
 band-pass overlap-add FIR filter on the signal to eliminate the low-frequency direct current components and high-frequency
 noise. Here, two versions of filtered data were offered. The first one has a filter band of 0.5-80 Hz and the second one has
 a filter band of 0.5-30 Hz. Researchers can choose the appropriate version based on their specific needs. After filtering, we
 performed an interpolation of bad channels. 
 Independent Component Analysis (ICA) was then applied to the data, utilizing the infomax algorithm. The number of independent components was set to 20, ensuring that they contain the majority of information while not being so numerous to increase the burden of manual processing. We excluded obvious noise components such as Electrooculography (EOG) and Electrocardiogram (ECG). Finally, the data was re-referenced using the average method. 
 

 The detailed information of the pre-processing can be found in our paper at https://doi.org/10.1101/2024.02.08.579481.
 

 ### Text Embeddings
 

 The dataset provides embeddings of two novels calculated using a pre-trained language model BERT-base-Chinese. During the experimental procedure, each displayed line of text contains n Chinese characters. The BERT-base-Chinese model processes these n Chinese characters, yielding an embedding of size (n, 768), where n represents the number of Chinese characters, and 768 the dimensionality of the embedding. To ensure displayed lines of varying length to have embeddings of the same shape, the first dimension of the embeddings is averaged to standardize the embedding size to (1, 768) for each instance. 
 

 ### Missing Data
 

 Due to technical reasons, there are some raw data lost:
 

 - EEG:
 

  - Sub-09 ses-LittlePrince run 1-3
 

  - Sub-14 ses-GranettDream run 9
 

  - Sub-15 ses-GranettDream run 12
 

  - Sub-07 ses-GranettDream run 18 (substituted by run 19)
  Notice: Sub-07 ses-GranettDream run 19 read chapter 19 of GranettDream instead of chapter 18.
 

 

 - Eyetracking data:
  - Sub-08 ses-LittlePrince run 1-2
 

 

 

 

 Due to bad quality of data or other reasons, there are some pre-processed data lost:
 

 - In the 0.5-30 Hz filtered version:
 

  - Sub-15 ses-LittlePrince run 1-7
 

  - Sub-15 ses-GranettDream run 1, 2, 3, 7, 10, 16
 

 

 - In the 0.5-80 Hz filtered version:
 

  - Sub-13 ses-LittlePrince run 4, 5
 

  - Sub-15 ses-LittlePrince run 1-7
 

  - Sub-13 ses-GranettDream run 14
 

  - Sub-15 ses-GranettDream run 1, 2, 3, 7, 10, 16
 

 

 ## Usage Note
 

 If you want to know more about the dataset, including the detailed parameter settings in our pre-processing steps or how to align the text with EEG segments, please refer to our paper at https://doi.org/10.1101/2024.02.08.579481. You can find the relevant code for the text presentation, data processing and other functions at the GitHub repository: https://github.com/ncclabsustech/Chinese_reading_task_eeg_processing.
 

 References
 ----------
 

 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G., Phillips, C., Delorme, A., Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Scientific Data, 6, 103. https://doi.org/10.1038/s41597-019-0104-8",1,0,0
ds004942,2024-01-23 2:33:47,Paul Kieffaber,1.0.0,SpatialMemory,2024-02-23 13:49:25,0,1,26899900000,25.1 GB,315,62,0,0,1.8.0,CC0,"Paul Kieffaber, Makenna McGill",,,,None,doi:10.18112/openneuro.ds004942.v1.0.0,,SpatialMemory,8.1.0,EEG,"Visuo-spatial working memory (VSWM) for sequences is thought to be crucial for daily behaviors. Decades of research indicate that oscillations in the gamma and theta bands play important functional roles in the support of visuo-spatial working memory, but the vast majority of that research emphasizes measures of neural activity during memory retention. The primary aims of the present study were (1) to determine whether oscillatory dynamics in the Theta and Gamma ranges would reflect item-level sequence encoding during a computerized spatial span task, (2) to determine whether item-level sequence recall is also related to these neural oscillations, and (3) to determine the nature of potential changes to these processes in healthy cognitive aging. Results indicate that VSWM sequence encoding is related to later (~700 ms) gamma band oscillatory dynamics and may be preserved in healthy older adults; high gamma power over midline frontal and posterior sites increased monotonically as items were added to the spatial sequence in both age groups. Item-level oscillatory dynamics during the recall of VSWM sequences were related only to theta-gamma phase amplitude coupling (PAC), which increased monotonically with serial position in both age groups. Results suggest that, despite a general decrease in frontal theta power during VSWM sequence recall in older adults, gamma band dynamics during encoding and theta-gamma PAC during retrieval play unique roles in VSWM and that the processes they reflect may be spared in healthy aging.",1,0,0
ds004993,2024-02-25 23:36:29,Liberty Hamilton,1.1.2,"WIRED ICM Sample Dataset - Workshop on Intracranial Recordings in Humans, Epilepsy, DBS",2024-02-26 1:22:25,0,2,319927000,0.298 GB,30,3,14,19,1.7.0,CC0,"Liberty S. Hamilton, Maansi Desai, Alyssa Field","The authors would like to acknowledge members of the Hamilton Lab, our clinical collaborators at Dell Children's Medical Center, and the patients who participated in this research.",Please acknowledge and include the original link to this repository and cite the authors if you use this data for any other purpose.,"National Institutes of Health - National Institute on Deafness and Other Communication Disorders (R01 DC018579, to LSH)",https://slhs.utexas.edu/research/hamilton-lab,doi:10.18112/openneuro.ds004993.v1.1.2,All procedures were approved by the University of Texas at Austin Institutional Review Board.,"movietrailers, timit5, timit4",,"iEEG, MRI","WIRED ICM TUTORIAL DATA
 ------------------------
 

 *Contributors:* Liberty S. Hamilton, PhD, Maansi Desai, PhD, Alyssa Field, MEd
 

 *Email:* liberty.hamilton@austin.utexas.edu
 

 This is a sample BIDS dataset for the WIRED ICM course in Paris, France in March 2024.
 

 This contains intracranial recordings collected by the Hamilton Lab at the University of Texas at Austin. These recordings include examples of evoked data during natural listening tasks along with some examples of seizure-related activity and vagus nerve stimulator (VNS) artifact for illustrative purposes. All procedures were approved by the University of Texas at Austin Institutional Review Board. 
 

 *Funding:* Support was provided by the National Institutes of Health National Institute on Deafness and Other Communication Disorders (R01 DC018579, to LSH).
 

 Tasks:
 -------
 

 1. `movietrailers` - this task involves patients listening to movie clips from various Pixar, Disney, Dreamworks, and other movies. We have published previously using these stimuli in EEG (Desai et al. 2021).
 2. `timit4` and `timit5` - these tasks involve patients listening to subsets of the TIMIT acoustic phonetic corpus (Garofolo et al 1993). The events provided in the dataset mark the onset and offset of each sentence. In `timit4`, each sentence is unique, while in `timit5`, 10 sentences are repeated 10 times. This is the same stimulus set used in Mesgarani et al. 2014, Hamilton et al. 2018, Hamilton et al. 2021, and Desai et. al 2021.
 

 Notes:
 -------
 

 * The movie trailer data for subject W1 was acquired at the start of a generalized tonic clonic seizure, and the research session was terminated. Large, synchronized spikes can be observed on multiple channels on the right parietal grid throughout the iEEG data.
 * The TIMIT data for subject W2 is an example of fairly clean sentence evoked data. 
 * The TIMIT data for subject W3 is a good example of on-and-off VNS artifact. The VNS has a strong artifact at ~20 Hz. Some patients with epilepsy may have these implanted devices to help control their seizures, so you should know how to spot artifact-related activity. Despite these artifacts, the evoked responses to sentences are quite strong.
 * The acquisition number (B3, B8, etc) has to do with the order in which this task was run relative to other tasks in an iEEG session, and can be ignored here.
 

 References
 ----------
 

 * Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 * Desai, M., Holder, J., Villarreal, C., Clark, N., Hoang, B., & Hamilton, L. S. (2021). Generalizable EEG encoding models with naturalistic audiovisual stimuli. Journal of Neuroscience, 41(43), 8946-8962.
 * Garofolo, J. S., Lamel, L. F., Fisher, W. M., Fiscus, J. G., & Pallett, D. S. (1993). DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1. NASA STI/Recon technical report n, 93, 27403.
 * Hamilton, L. S., Edwards, E., & Chang, E. F. (2018). A spatial map of onset and sustained responses to speech in the human superior temporal gyrus. Current Biology, 28(12), 1860-1871.
 * Hamilton, L. S., Oganian, Y., Hall, J., & Chang, E. F. (2021). Parallel and distributed encoding of speech across human auditory cortex. Cell, 184(18), 4626-4639.
 * Holdgraf, C., Appelhoff, S., Bickel, S., Bouchard, K., D'Ambrosio, S., David, O., … Hermes, D. (2019). iEEG-BIDS, extending the Brain Imaging Data Structure specification to human intracranial electrophysiology. Scientific Data, 6, 102. https://doi.org/10.1038/s41597-019-0105-7
 * Mesgarani, N., Cheung, C., Johnson, K., & Chang, E. F. (2014). Phonetic feature encoding in human superior temporal gyrus. Science, 343(6174), 1006-1010.",1,0,0
ds004995,2024-02-28 4:25:48,Denise Moerel,1.0.2,The Time-Course of Food Representation in the Human Brain,2024-02-28 5:59:22,0,1,29637600000,27.6 GB,83,20,0,0,1.0.2,CC0,"Denise Moerel, James Psihoyos, Thomas A. Carlson",We would like to acknowledge the University of Sydney HPC service for providing High Performance Computing resources.,"Moerel, D., Psihoyos, J., & Carlson, T. A. (2023). The Time-Course of Food Representation in the Human Brain. bioRxiv, 2023-06. https://doi.org/10.1101/2021.05.24.445376",ARC DP160101300 (TAC) ===NEMAR-SEP=== ARC DP200101787 (TAC),https://doi.org/10.17605/OSF.IO/PWC4K ===NEMAR-SEP=== https://doi.org/10.1101/2023.06.06.543985,doi:10.18112/openneuro.ds004995.v1.0.2,,,,EEG,"The main folder contains the raw EEG data in standard bids format. See references.
 

 Code and figures: https://doi.org/10.17605/OSF.IO/PWC4K
 Manuscript: https://doi.org/10.1101/2023.06.06.543985
 

 References:
 

 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., Moreau, J., Oostenveld, R., Schoffelen, J., Tadel, F., Wexler, J., Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5, 180110. https://doi.org/10.1038/sdata.2018.110",1,0,0
ds004998,2024-03-04 17:45:27,Fayed Rassoulou,1.1.0,Exploring the electrophysiology of Parkinson's disease - magnetoencephalography combined with deep brain recordings from the subthalamic nucleus.,2024-03-06 18:07:06,0,1,165157000000,153.8 GB,552,19,0,0,1.6.0,CC0,"Fayed Rassoulou, Alexandra Steina, Christian J. Hartmann, Jan Vesper, Markus Butz, Alfons Schnitzler, Jan Hirschmann",,,The project was funded by ERA-NET Neuron for 3 years.,,doi:10.18112/openneuro.ds004998.v1.1.0,,"HoldL, MoveL, noise, HoldR, MoveR, Rest",,MEG,"This dataset contains data from externalized DBS patients undergoing simultaneous MEG - STN LFP recordings with (MedOn) and without (MedOn) dopaminergic medication. It has two movement conditions: 1) 5 min of rest followed by static forearm extension (hold) and 2) 5 min of rest followed by self-paced fist-clenching (move). The movement parts contain pauses. Some patients were recorded in resting-state only (rest). The project aimed to understand the neurophysiology of basal ganglia-cortex loops and its modulation by movement and medication.
 

 Code for quickly start is available here:
 https://github.com/Fayed-Rsl/RHM_preprocessing
 

 References
 ----------
 

 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., Moreau, J., Oostenveld, R., Schoffelen, J., Tadel, F., Wexler, J., Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5, 180110. https://doi.org/10.1038/sdata.2018.110",1,0,0
ds005007,2024-03-05 23:44:50,Kitazawa Yu,1.0.0,Auditory naming task with questions that begin or end with a wh-interrogative,2024-03-07 19:23:34,0,2,8896470000,8.3 GB,172,40,6,54,1.7.0,CC0,"Yu Kitazawa, Eishi Asano",,N/A,N/A,,doi:10.18112/openneuro.ds005007.v1.0.0,IRB of Wayne State University,namingtask,,iEEG,,1,0,0
ds005021,2024-03-09 1:46:15,Anthony Harris,1.2.1,Tilt Illusion by Phase,2024-03-11 0:31:33,0,1,51007300000,47.5 GB,148,36,0,0,1.2.1,CC0,"Jessica G. Williams, William J. Harrison, Henry A. Beale, Jason B. Mattingley, Anthony M. Harris",,"Please cite Williams, Harrison, Beale, Mattingley, & Harris (2024). Effects of alpha oscillation power and phase on discrimination performance in a visual tilt illusion. Current Biology",Australian Research Council (DE190100136) ===NEMAR-SEP=== Australian Research Council (DE220101019) ===NEMAR-SEP=== National Health and Medical Research Council (NHMRC) Australia Investigator Grant (GNT2010141),,doi:10.18112/openneuro.ds005021.v1.2.1,All experimental procedures were approved by the Human Research Ethics Committee at The University of Queensland (2021/HE002284).,tilt-illusion,,EEG,"# Overview
 

 This is the ""Tilt Illusion"" dataset.
 

 In brief, it contains EEG data for 36 subjects responding to the percieved orientation 
 of a central target grating, that is titrated to appear vertical on average, and is 
 surrounded by an anular grating of +-30 degrees. We then looked at the prestimulus 
 EEG correlates of an increased or decreased tilt illusion.
 

 # Citing this dataset
 

 Please cite as follows:
 

 > Williams, J.G., Harrison, W.J., Beale, H.A., Mattingley, J.B., & Harris, A.M. (2024). Effects of alpha oscillation power and phase on discrimination performance in a visual tilt illusion. Current Biology.
 

 For more information, see the `dataset_description.json` file.
 

 # License
 

 The `tilt illusion` dataset is made available under the CC BY 4.0 license.
 

 Copyright (c) 2024, Jessica Williams, William Harrison, Henry Beale, Jason Mattingley, & Anthony Harris
 

 A human readable information can be found at:
 

 https://creativecommons.org/licenses/by/4.0/deed.en
 

 # Format
 

 The dataset is formatted according to the Brain Imaging Data Structure (BIDS).
 See the `dataset_description.json` file for the specific version used.
 

 Generally, you can find metadata in the `.tsv` files and documentation thereof in the accompanying `.json` files.
 

 An important BIDS definition to consider is the ""Inheritance Principle"", which
 is described in the BIDS specification under the following link:
 

 https://bids-specification.readthedocs.io/en/latest/common-principles.html#the-inheritance-principle
 

 In brief, the Inheritance Pinciple states that any metadata file (such as `.json`, `.tsv`)
 may be defined at any directory level, but no more than one applicable file may be defined at a given level [...],
 and the values from the top level are inherited by all lower levels --
 unless they are overridden by a file at the lower level.
 

 # Details about the experiment
 

 For a detailed description of the task, see Williams et al. (2024)
 What follows is a brief summary.
 

 Participants were seated in front of a computer screen placed on a desk.
 On each trial they were presented with a central target grating, surrounded by 
 an annular grating of +-30 degrees. This induced a 'tilt illusion' whereby the 
 percieved angle of the central grating was biased away from the angle of the 
 surround. We first titrated the angle of the central grating to each participant's
 percieved vertical angle, separately for each surround. Percieved vertical was 
 defined as the angle at which the participant reported the grating as tilted 
 leftward and rightward equally often. Participants responded with their right 
 hand by pressing the left and right arrow keys on a standard USB keyboard. Stimuli
 were presented very briefly (8.3ms) at 60% contrast, and were clearly visible.
 Between trials, a mask made from the combination of several gratings was presented
 to prevent the buildup of tilt aftereffects across trials.
 

 Throughout the experiment, EEG data was recorded using a Biosemi Active 2 system
 with 64 scalp electrods and 6 EOG electrodes (left and right HEOG, VEOG on left
 eye, and left and right mastoids - in positions EXG 3-8). 
 

 For more information, you can also consult the events.tsv and events.json files.
 

 The original data was recorded in `.bdf` format using Actiview. It is stored in 
 the `/sourcedata` directory. To comply with the BIDS format, the .bdf format was 
 converted to EEGLab format, constituting a '.set' file and a 'fdt' file for each 
 dataset. 
 

 Participant 1's data was corrupted by large artefacts that could not be corrected.
 Participants 8, 16, and 28 had no EEG data recorded, as their pre-task titration
 failed to converge. As such, the data for these 4 participants are not included 
 in this dataset.",1,0,0
ds005028,2024-03-11 23:10:26,Bill Speier,1.0.0,Comparing P300 Flashing paradigms in online typing with language models,2024-03-24 1:05:36,0,2,442568000,0.412 GB,108,11,0,0,1.8.0,CC0,"Nand Chandravadia, Shrita Pendekanti, Dustin Roberts, Robert Tran, Saarang Panchavati, Corey Arnold, Nader Pouratian, William Speier",This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under grant no. DGE-2034835.,"If you reference this dataset in your publications, please acknowledge its authors.",National Science Foundation Graduate Research Fellowship Program under grant no. DGE-2034835,"Chandravadia, Pendekanti, Roberts, Tran, Panchavati, Arnold, Pouratian, Speier. Comparing P300 flashing paradigms in online typing with language models. medRxiv 2022.06.24.22276882.",doi:10.18112/openneuro.ds005028.v1.0.0,UCLA IRB #11-002062,,8.2.0,EEG,"This dataset was created using BCI2000. The goal of this study was to explore the online typing performance of the P300 speller using language models and various flashing paradigms. For more information see Chandravadia et al. (https://www.medrxiv.org/content/10.1101/2022.06.24.22276882v1).
 

 If you reference this dataset in your publications, please acknowledge its authors.
 

 This dataset is made available under CC0.
 

 Note: subject 5 was not included in the analysis because the testing stage did not include all three flashing paradigms.",1,0,0
ds005048,2024-03-19 14:24:38,Mojtaba Lahijanian,1.0.0,40Hz Auditory Entrainment,2024-03-25 18:16:23,0,1,373200000,0.348 GB,181,35,0,0,1.8.0,CC0,"Mojtaba Lahijanian, Hamid Aghajan, Zahra Vahabi",The authors wish to thank Ziaeian Hospital in Tehran for providing staff time and equipment for data collection in this study. We are grateful to the patients and their families who participated in this study.,,"This work was partially funded by the Cognitive Sciences & Technologies Council of Iran and by the grant G970736 from Sharif University of Technology, which covered the cost of data collection. The funders had no role in the study conceptualization and design, data collection and analysis, decision to publish, or preparation of the manuscript.",,doi:10.18112/openneuro.ds005048.v1.0.0,This study was approved by the Review Board of Tehran University of Medical Sciences (Approval ID: IR.TUMS.MEDICINE.REC.1398.524) and all participants provided informed consent before participating and were free to withdraw at any time.,40HzAuditoryEntrainment,8.1.0,EEG,"Introduction 
 This experiment was designed to entrain the brain oscillations through synthetic auditory stimulation conducted on a group of elderly suffering from dementia. Recently, gamma entrainment has been proposed and shown effective in improving several symptoms of Alzheimer's Diseases (AD). The aim of this study is to investigate the effect of entrainment on brain oscillations using EEG signal recording during the auditory brain stimulation. This study was approved by the Review Board of Tehran University of Medical Sciences (Approval ID: IR.TUMS.MEDICINE.REC.1398.524). All methods were performed in accordance with the relevant guidelines and regulations, and all participants provided informed consent before participating and were free to withdraw at any time. To accommodate participants who preferred a shorter duration of data gathering, we designed both short and long sessions for entrainment. This approach aimed to minimize inconvenience for the participants who were less inclined to engage in lengthy procedures. 
 Entrainment session and auditory stimulation 
 Each session involved the presentation of a multi-trial auditory stimulus while simultaneously recording EEG data from the participant. To deliver the auditory stimulus, two speakers were placed in front of the participant 50cm apart from each other and directly pointed at the participant’s ears at a distance of 50cm. The sound intensity was around -40dB within a fixed range for all participants. To ascertain adequate hearing ability of the participants and to ensure individual comfort, each participant was asked before commencing the task if the sound was at a comfortable level, and adjustments were made to the volume. The auditory stimulus was a 5kHz carrier tone amplitude modulated with a 40Hz rectangular wave (40Hz On and Off cycles). Since a 40Hz tone cannot be easily heard, the 5KHz carrier frequency was used to render the 40Hz pulse train audible. In order to minimize the effect of the carrier sound, the duty cycle of the modulating 40Hz waveform was set to 4% (1ms of the 25ms cycle was On). The auditory stimulant was generated in MATLAB and played as a .wav file. This file consisted of multiple trials, with each trial lasting 40sec and interleaved by 20sec of rest (silence). The short session included six trials, while the long session comprised ten trials of the stimulus.
 EEG recording and preprocessing  
 All EEG data were recorded using 19 monopolar channels based on the standard 10/20 system. For the short session, the reference electrodes were placed on the earlobes, while for the long session, referencing was done to the FCz channel. Notably, referencing to the average was implemented during preprocessing, ensuring data integrity and minimizing potential interference. The sampling rate was set to 250Hz, and the impedance of the electrodes was kept under 20k?. During the experiment, participants were seated comfortably with open eyes in a quiet room, and they were instructed to relax their body to avoid muscle artifacts and to move their head as little as possible.  
 Data from all the participants were preprocessed identically following Makoto's preprocessing pipeline: Highpass filtering above 1Hz; removal of the line noise; rejecting potential bad channels; interpolating rejected channels; re-referencing data to the average; artifact subspace reconstruction (ASR); re-referencing data to the average again; estimating the brain source activity using independent component analysis (ICA); dipole fitting; rejecting bad dipoles (sources) for further cleaning the data. These preprocessing steps were performed using EEGLab toolbox in MATLAB.",1,0,0
ds005059,2024-04-03 21:09:33,Haydn Herrema,1.0.2,Paired Associates Learning: Memory for Word Pairs in Cued Recall,2024-04-03 22:24:23,0,7,179632000000,167.3 GB,2003,72,18,62,1.7.0,CC0,"Haydn G. Herrema, Michael J. Kahana",,,,,doi:10.18112/openneuro.ds005059.v1.0.2,,PAL1,,iEEG,"### Paired Associates Learning of Word Pairs
 

 #### Description
 This dataset contains behavioral events and intracranial electrophysiological recordings from a paired associates memory task. The experiment consists of participants studying pairs of visually presented words, solving simple arithmetic problems that function as a distractor, and then completing a cued recall task. The data was collected at clinical sites across the country as part of a collaboration with the Computational Memory Lab at the University of Pennsylvania.
 

 Each session contains 25 lists of the structure: encoding, distractor, cued recall. During encoding, 6 pairs of words are presented one pair at a time. Each pair remains on screen for 4000 ms and is followed by a 1000 ms interstimulus interval. During the cued recall, one randomly chosen word from each pair is shown, and the participant is asked to vocally recall the other word from the pair. Participants have 5000 ms for each recall, and then the next cue (i.e., a word from another pair) is shown. All 6 pairs of words are tested on each list.
 

 #### To Note:
 - The iEEG recordings are labeled either ""monopolar"" or ""bipolar."" The monopolar recordings are referenced (typically a mastoid reference), but should always be re-referenced before analysis. The bipolar recordings are referenced according to a paired scheme indicated by the accompanying bipolar channels tables.
 - Each subject has a unique montage of electrode locations. MNI and Talairach coordinates are provided when available, along with brain region annotations.
 - Recordings were made on multiple different systems, so we have done the scaling to provide all voltage values in V.",1,0,0
ds004902,2023-12-20 0:14:33,Chuqin Xiang,1.0.5,A Resting-state EEG Dataset for Sleep Deprivation,2023-12-20 10:28:10,0,2,8863460000,8.3 GB,1085,71,0,0,1.8.0,CC0,"Chuqin Xiang, Xinrui Fan, Duo Bai, Ke Lv, Xu Lei",This study is supported by National Key Research and Development Program of China (2021YFC2501500).,,,,doi:10.18112/openneuro.ds004902.v1.0.5,,"eyesclosed, eyesopen",8.1.0,EEG,"# General information
  
 The dataset provides resting-state EEG data (eyes open,partially eyes closed) from 71 participants who underwent two experiments involving normal sleep (NS---session1) and sleep deprivation(SD---session2) .The dataset also provides information on participants' sleepiness and mood states.
 (Please note here Session 1 (NS) and Session 2 (SD) is not the time order, the time order is counterbalanced across participants and is listed in metadata.)
 

 

 # Dataset 
  
 ## Presentation
  
 The data collection was initiated in March 2019 and was terminated in December 2020. The detailed description of the dataset is currently under working by Chuqin Xiang,Xinrui Fan,Duo Bai,Ke Lv and Xu Lei, and will submit to Scientific Data for publication.
 

  
  
 #### EEG acquisition
  
 * EEG system (Brain Products GmbH, Steing- rabenstr, Germany, 61 electrodes) 
 * Sampling frequency: 500Hz
 * Impedances were kept below 5k
 

  
 ## Contact 
  * If you have any questions or comments, please contact:
  * Xu Lei: xlei@swu.edu.cn  
 

 ## Article
 Xiang, C., Fan, X., Bai, D. et al. A resting-state EEG dataset for sleep deprivation. Sci Data 11, 427 (2024). https://doi.org/10.1038/s41597-024-03268-2",1,0,0
ds004215,2022-07-15 20:31:48,Arshitha Basavaraj,1.0.3,The NIMH Intramural Healthy Volunteer Dataset,2022-08-05 19:55:55,1,1,376520000000,350.7 GB,12897,157,18,89,1.6.0,CC0,"Allison C. Nugent, Adam G Thomas, Margaret Mahoney, Alison Gibbons, Jarrod Smith, Antoinette Charles, Jacob S Shaw, Jeffrey D Stout, Anna M Namyst, Arshitha Basavaraj, Eric Earl, Travis Riddle, Joseph Snow, Shruti Japee, Adriana Pavletic, Stephen Sinclair, Vinai Roopchansingh, Peter A Bandettini, Joyce Chung","We thank the NIMH Office of the Clinical Director, the outpatient behavioral health clinic and NMR center for providing support for the data collection. This work utilized the computational resources of the NIH HPC Biowulf cluster http://hpc.nih.gov. We thank Sil van der Woerd for graciously allowing us to use his film as a behavioral task. In addition, we thank the subjects who generously contributed their data to this project.","Nugent, A. C., Thomas, A. G., Mahoney, M., Gibbons, A., Smith, J. T., Charles, A. J., Shaw, J. S., Stout, J. D., Namyst, A. M., Basavaraj, A., Earl, E., Riddle, T., Snow, J., Japee, S., Pavletic, A. J., Sinclair, S., Roopchansingh, V., Bandettini, P. A., & Chung, J. (2022). The NIMH intramural healthy volunteer dataset: A comprehensive MEG, MRI, and behavioral resource. Scientific Data, 9, Article 518. https://doi.org/10.1038/s41597-022-01623-9",ZICMH002889 ===NEMAR-SEP=== ZICMH002960 ===NEMAR-SEP=== ZIAMH002783 ===NEMAR-SEP=== ZIDMH00291,https://nimhresearchvolunteer.ctss.nih.gov ===NEMAR-SEP=== https://github.com/nih-megcore/hv_protocol ===NEMAR-SEP=== doi:10.1016/j.psychres.2020.112822 ===NEMAR-SEP=== doi:10.1101/2021.04.28.21256253 ===NEMAR-SEP=== doi:10.1371/journal.pone.0184661 ===NEMAR-SEP=== doi:10.1038/s41592-018-0235-4,doi:10.18112/openneuro.ds004215.v1.0.3,NIH Institutional Review Board (Recruitment and Characterization of Healthy Research Volunteer for NIMH Intramural Studies NCT033046),"Resting state with eyes open., Resting state with reverse blip., airpuff, artifact, gonogo, haririhammer, movie, EmptyRoom, oddball, rest, sternberg",,"MEG, MRI","# The National Institute of Mental Health (NIMH) Intramural Healthy Volunteer Dataset
 

 A comprehensive dataset characterizing healthy research volunteers in terms of clinical assessments, mood-related psychometrics, cognitive function neuropsychological tests, structural and functional magnetic resonance imaging (MRI), along with diffusion tensor imaging (DTI), and a comprehensive magnetoencephalography battery (MEG).
 

 In addition, blood samples are currently banked for future genetic analysis. All data collected in this protocol are broadly shared in the OpenNeuro repository, in the Brain Imaging Data Structure (BIDS) format. In addition, task paradigms and basic pre-processing scripts are shared on GitHub. This dataset is unprecedented in its depth of characterization of a healthy population and will allow a wide array of investigations into normal cognition and mood regulation.
 

 This dataset is licensed under the [Creative Commons Zero (CC0) v1.0 License](https://creativecommons.org/publicdomain/zero/1.0/).
 

 ## Participant Eligibility
 

 To be eligible for the study, participants need to be medically healthy adults over 18 years of age with the ability to read, speak and understand English. All participants provided electronic informed consent for online pre-screening, and written informed consent for all other procedures. Participants with a history of mental illness or suicidal or self-injury thoughts or behavior are excluded. Additional exclusion criteria include current illicit drug use, abnormal medical exam, and less than an 8th grade education or IQ below 70. Current NIMH employees, or first degree relatives of NIMH employees are prohibited from participating. Study participants are recruited through direct mailings, bulletin boards and listservs, outreach exhibits, print advertisements, and electronic media.
 

 ## Clinical Measures
 

 All potential volunteers visit [the study website](https://nimhresearchvolunteer.ctss.nih.gov), check a box indicating consent, and fill out preliminary screening questionnaires. The questionnaires include basic demographics, the World Health Organization Disability Assessment Schedule 2.0 (WHODAS 2.0), the DSM-5 Self-Rated Level 1 Cross-Cutting Symptom Measure, the DSM-5 Level 2 Cross-Cutting Symptom Measure - Substance Use, the Alcohol Use Disorders Identification Test (AUDIT), the Edinburgh Handedness Inventory, and a brief clinical history checklist. The WHODAS 2.0 is a 15 item questionnaire that assesses overall general health and disability, with 14 items distributed over 6 domains: cognition, mobility, self-care, “getting along”, life activities, and participation. The DSM-5 Level 1 cross-cutting measure uses 23 items to assess symptoms across diagnoses, although an item regarding self-injurious behavior was removed from the online self-report version. The DSM-5 Level 2 cross-cutting measure is adapted from the NIDA ASSIST measure, and contains 15 items to assess use of both illicit drugs and prescription drugs without a doctor’s prescription. The AUDIT is a 10 item screening assessment used to detect harmful levels of alcohol consumption, and the Edinburgh Handedness Inventory is a systematic assessment of handedness. These online results do not contain any personally identifiable information (PII). At the conclusion of the questionnaires, participants are prompted to send an email to the study team. These results are reviewed by the study team, who determines if the participant is appropriate for an in-person interview.
 

 Participants who meet all inclusion/exclusion criteria are scheduled for an in-person screening visit. At this visit, participants receive a Structured Clinical Interview for DSM-5 Disorders (SCID-5), the Beck Depression Inventory-II (BDI-II), Beck Anxiety Inventory (BAI), the Kaufman Brief Intelligence Test, Second Edition (KBIT-2), and the NIH Toolbox Cognition Battery. The purpose of these cognitive and psychometric tests is two-fold. First, these measures are designed to provide a sensitive test of psychopathology. Second, they provide a comprehensive picture of cognitive functioning, including mood regulation. The SCID-5 is a structured interview, administered by a clinician, that establishes the absence of any DSM-5 axis I disorder. The BDI-II is a 21 item self-report measure that assesses the presence and severity of depressive symptoms, while the BAI is a 21 item self-report measure assessing the presence and severity of anxiety symptoms. The KBIT-2 is a brief (20 minute) assessment of intellectual functioning administered by a trained examiner. There are three subtests, including verbal knowledge, riddles, and matrices. The NIH Toolbox Cognition Battery consists of a series of seven tests. A flanker inhibitory control and attention task assesses the constructs of attention and executive functioning. Executive functioning is also assessed in the battery using a dimensional change card sort test. Episodic memory is evaluated using a picture sequence memory test, while working memory is evaluated using a list sorting test. The battery includes two language tests, a picture vocabulary test and an oral reading recognition test. Finally, processing speed is assessed using a pattern comparison processing speed test.
 

 ## Biological and physiological measures
 

 Biological and physiological measures are acquired, including blood pressure, pulse, weight, height, and BMI. Blood and urine samples are taken and a complete blood count, acute care panel, hepatic panel, thyroid stimulating hormone, viral markers (HCV, HBV, HIV), c-reactive protein, creatine kinase, urine drug screen and urine pregnancy tests are performed. In addition, blood samples for genetic testing are collected and banked, and genetic information will be shared once it is available.
 

 ## Imaging Studies
 

 Participants were given the option to enroll in optional magnetic resonance imaging (MRI) and magnetoencephalography (MEG) studies.
 

 ### MRI
 

 The MRI protocol used was initially based on the ADNI-3 basic protocol, but was later modified to include portions of the ABCD protocol in the following manner:
 

 1. The T1 scan from ADNI3 was replaced by the T1 scan from the ABCD protocol.
 2. The Axial T2 2D FLAIR acquisition from ADNI2 was added, and fat saturation turned on.
 3. Fat saturation was turned on for the pCASL acquisition.
 4. The high-resolution in-plane hippocampal 2D T2 scan was removed, and replaced with the whole brain 3D T2 scan from the ABCD protocol (which is resolution and bandwidth matched to the T1 scan).
 5. The slice-select gradient reversal method was turned on for DTI acquisition, and reconstruction interpolation turned off.
 6. Scans for distortion correction were added (reversed-blip scans for DTI and resting state scans).
 7. The 3D FLAIR sequence was made optional, and replaced by one where the prescription and other acquisition parameters provide resolution and geometric correspondence between the T1 and T2 scans.
 

 ### MEG
 

 The optional MEG studies were added to the protocol approximately one year after the study was initiated, thus there are relatively fewer MEG recordings in comparison to the MRI dataset. MEG studies are performed on a 275 channel CTF MEG system. The position of the head was localized at the beginning and end of the recording using three fiducial coils. These coils were placed 1.5 cm above the nasion, and at each ear, 1.5 cm from the tragus on a line between the tragus and the outer canthus of the eye. For some participants, photographs were taken of the three coils and used to mark the points on the T1 weighted structural MRI scan for co-registration. For the remainder of the participants, a BrainSight neuronavigation unit was used to coregister the MRI, anatomical fiducials, and localizer coils directly prior to MEG data acquisition.
 

 ## Specific Survey and Test Data within Data Set
 

 ### 1. Preliminary Online Screening Questionnaires
 

 | Survey or Test  | BIDS TSV Name  |
 | --------------------------------------------------------------------------- | ------------------------------ |
 | Alcohol Use Disorders Identification Test (AUDIT) | audit.tsv  |
 | Demographics  | demographics.tsv |
 | Drug Use Questionnaire  | drug_use.tsv |
 | Edinburgh Handedness Inventory (EHI)  | ehi.tsv  |
 | Health History Questions  | health_history_questions.tsv |
 | Health Rating | health_rating.tsv  |
 | Mental Health Questions | mental_health_questions.tsv  |
 | World Health Organization Disability Assessment Schedule 2.0 (WHODAS 2.0) | whodas.tsv |
 

 ### 2. On-Campus In-Person Screening Visit
 

 | Survey | BIDS TSV Name |
 | -------------------------------------------------------------------------------------------- | ----------------------------- |
 | Adverse Childhood Experiences (ACEs) | ace.tsv |
 | Beck Anxiety Inventory (BAI) | bai.tsv |
 | Beck Depression Inventory-II (BDI-II)  | bdi.tsv |
 | Clinical Variable Form | clinical_variable_form.tsv  |
 | Family Interview for Genetic Studies (FIGS)  | figs.tsv  |
 | Kaufman Brief Intelligence Test 2nd Edition (KBIT-2) and Vocabulary Assessment Scale (VAS) | kbit2_vas.tsv |
 | NIH Toolbox Cognition Battery  | nih_toolbox.tsv |
 | Perceived Health Rating  | perceived_health_rating.tsv |
 | Satisfaction Survey  | satisfaction.tsv  |
 | Structured Clinical Interview for DSM-5 Disorders (SCID-5) | scid5.tsv |
 

 | Test | BIDS TSV Name |
 | ---------------------------------------- | --------------------------- |
 | Acute Care Panel | acute_care.tsv  |
 | Blood Chemistry  | blood_chemistry.tsv |
 | Complete Blood Count with Differential | cbc_with_differential.tsv |
 | Hematology Panel | hematology.tsv  |
 | Hepatic Function Panel | hepatic.tsv |
 | Infectious Disease Panel | infectious_disease.tsv  |
 | Lipid Panel  | lipid.tsv |
 | Other Panel  | other.tsv |
 | Urinalysis | urinalysis.tsv  |
 | Urine Chemistry  | urine_chemistry.tsv |
 | Vitamin Levels | vitamin_levels.tsv  |
 

 ### 3. Optional On-Campus In-Person MRI Visit
 

 | Survey  | BIDS TSV Name |
 | --------------- | ------------------- |
 | MRI Variables | mri_variables.tsv |
 

 ## Preparation Notes
 

 In many of the Clinical Measures data files, there exist `-999` values. `-999` means there was no response though a response was possible. The question may have been skipped over by the participant or the question flow. `-777` appears in the Edinburgh Handedness Inventory (EHI) as well. `-777` means there is no data available for a response. The question was not presented or asked to the participant.
 

 The data were prepared using the following tools and filename mappings.
 

 ### Clinical Measures Data
 

 The `ctdb_clean_up.ipynb` Jupyter Notebook contains the python functions used to clean and convert the spreadsheet downloaded from CTDB to BIDS-standard TSV files as well as their respective data dictionaries converted to BIDS-standard JSON files.
 

 ### Biological and Physiological Measures Data
 

 The `cris_clean_up.ipynb` Jupyter Notebook contains the Python functions used to clean and convert the spreadsheet with clinical measures to BIDS-standard TSV files and their data dictionaries to BIDS-standard JSON files.
 

 ### BIDS-standard MEG Files
 

 Data collected by the NIMH MEG Core was converted to BIDS-standard files using the MNE BIDS package. Associated notebooks: `1_mne_bids_extractor.ipynb` & `2_bids_editor.ipynb`.
 

 ### BIDS-standard MRI
 

 We used the `heudiconv` tool to convert MRI DICOM files to BIDS-standard files with the associated script: `heuristic_rvol.py`. A modified workflow of `pydeface` was used to deface structural scans with the associated notebook: `modified-workflow-pydeface.ipynb`
 

 Each participant received either the ADNI3 or the ABCD protocol, not both, during their MRI/MEG visit. T1w scans with acquisition label `fspgr` are ADNI3 protocol sequence and scans with `mprage` acquisition labels are ABCD protocol sequence.
 

 ### Protocol PDFs
 

 GE MRI scanner protocol PDFs are located within the `sourcedata/` folder, named after their BIDS tree file names. In the parameter called ""Auto SCIC"", if it is set to 2 then both ""Orig"" (original) and ""SCIC"" (GE’s proprietary Surface Coil Intensity Correction algorithm) images are created. Therefore one PDF with an Auto SCIC setting of ""2"" represents two images in the dataset.",1,0,0
ds005079,2024-04-09 20:37:10,Arnaud Delorme,2.0.0,The Effects of Directed Therapeutic Intent on Live and Damaged Cells,2024-04-09 21:19:25,0,12,1809230000,1.7 GB,210,1,0,0,v1.2.1,CC0,"Lorenzo Cohen, Arnaud Delorme, Peiying Yang, Andrew Cusimano, Sharmistha Chakraborty, Phuong Nguyen, Defeng Deng, Shafaqmuhammad Iqbal, Monica Nelson, Chris Fields",,,,No bibliographic reference other than the DOI for this dataset,doi:10.18112/openneuro.ds005079.v2.0.0,,mixed,8.1.0,EEG,"**Summary**: In this case study, a self-described practitioner of energy medicine (PEM) participated in a study, engaging in multiple (n=60) treatment and control (non-treatment) sessions under double-blind conditions.
 

 **Protocol:**Data were collected during 40 sessions over 10 days, with ten sessions of about 25 minutes daily. Each session was comprised of one file divided into five segments. First, there was a 2-minute control period where the PEM rested in the absence of cells (BaselinePre) for 2 minutes. Next, the cells (alive or control) were brought in, and the PEM conducted a 5-minute treatment of the cells while remaining still (TreatmentFirst5min). Next, the PEM performed another 5-minute treatment of the cells, but movement was allowed (Treatment 2). During a third treatment period (TreatmentMid5min), the PEM remained still while treating the cells, as in first treatment period (TreatmentLast5min). Finally, the cells were removed from the PEM's vicinity, and physiology data were collected for another 2-minute control period (BaselinePost). The PEM was fully blind to the type of cells presented to him, and cell type presentation to the PEM was randomized. The experimenter presenting the cell to the PEM was also blind to the type of cells. In 40 sessions, live cells were presented to the PEM (CellPresent condition). In 10 sessions, no cells (medium only) were presented to the PEM. In the other ten sessions, dead cells (x-rayed) were presented to the PEM (Control1 and Control2 conditions). In order to have control samples for the cellular outcomes and control for the passage of time and potential effects of the equipment, 40 matching set of cells were treated in a different location by a sham therapist (these are available in the behavioral files (BEH) as control cell measures.
 

 Data curators: Data acquired at the MD Anderson Cancer Research Center",1,0,0
ds005083,2024-04-10 21:53:35,Jarod L Roland,1.0.0,Safety and Accuracy of Stereoelectroencephalography for Pediatric Patients with Prior Craniotomy,2024-04-10 22:04:27,0,1,288430,0 GB,123,60,0,0,1.9.0,CC0,"Peter H Yang, Nathan Wulfekammer, Amanda V. Jenson, Elliot Neal, Stuart Tomko, John Zempel, Peter Brunner, Sean D McEvoy, Matthew D Smyth, Jarod L Roland",,,,,doi:10.18112/openneuro.ds005083.v1.0.0,Approval obtained from the Institutional Review Board at Washington University in St Louis,,,iEEG,"BIDS iEEG dataset for the SEEG electrode data used for analysis in the manuscript title ""Safety and Accuracy of Stereoelectroencephalography for Pediatric Patients with Prior Craniotomy.""
 All coordinates are recorded in the individual native post-operative CT imaging space. There was no alignment to other imaging modalities or standardized atlases.",1,0,0
ds004865,2023-11-29 19:38:59,Haydn Herrema,2.0.1,"pyFR: Delayed Free Recall of Word Lists, Preliminary Cognitive Electrophysiology Study",2023-11-29 20:55:51,0,9,104999000000,97.8 GB,1244,49,15,57,1.7.0,CC0,"Haydn G. Herrema, Michael J. Kahana","We thank our collaborators who have helped in the collection of this data: Hospital of the University of Pennsylvania, Thomas Jefferson University Hospital.",,NIH: MH055687 ===NEMAR-SEP=== NIH: MH061975,,doi:10.18112/openneuro.ds004865.v2.0.1,,pyFR,,iEEG,"### pyFR: Delayed Free Recall of Word Lists, Preliminary Cognitive Electrophysiology Study
 

 #### Description
 This dataset contains behavioral events and intracranial electrophysiological recordings from a delayed free recall task. The experiment consists of participants studying a list of words, presented visually one at a time, completing simple arithmetic problems that function as a distractor, and then freely recalled the words from the just-presented list in any order. The data was collected at clinical sites across the country as part of a collaboration with the Computational Memory Lab at the University of Pennsylvania.
 

 This study was a preliminary cogntive electrophysiology study undertaken by the Computational Memory Lab, and is a predecessor to the following datasets: [FR1](https://openneuro.org/datasets/ds004789) & [CatFR1](https://openneuro.org/datasets/ds004809)
 

 #### To Note
 * The iEEG recordings are labeled either ""monopolar"" or ""bipolar."" The monopolar recordings are referenced (typically a mastoid reference), but should always be re-referenced before analysis. The bipolar recordings are referenced according to a paired scheme indicated by the accompanying bipolar channels tables.
 * Each subject has a unique montage of electrode locations. MNI and Talairach coordinates are provided when available, along with brain region annotations.
 * Recordings were made on multiple different systems, so we have done the scaling to provide all voltage values in V.
 

 #### Contact
 For questions or inquiries, please contact sas-kahana-sysadmin@sas.upenn.edu.",1,0,0
ds005089,2024-04-17 14:24:31,Blanca Aguado-López,1.0.1,Competition_EEG,2024-04-18 7:47:09,0,1,73021300000,68 GB,255,36,18,27,1.2,CC0,"Blanca Aguado-Lopez, Ana F. Palenciano, Jose M. G. Penalver, Paloma Diaz-Gutierrez, David Lopez-Garcia, Chiara Avancini, Luis F. Ciria, Maria Ruz",,,,,doi:10.18112/openneuro.ds005089.v1.0.1,,Competition,,EEG,,1,0,0
ds005095,2024-04-18 15:53:16,Natalia Zhozhikashvili,1.0.1,STERNBERG,2024-04-19 22:01:11,0,1,15336200000,14.3 GB,485,48,19,32,1.7.0,CC0,"Natalia Zhozhikashvili, Maria Protopova, Tatiana Shkurenko, Marie Arsalidou, Ilya Zakharov, Boris Kotchoubey, Sergey Malykh, Yuri Pavlov",,https://doi.org/10.1016/j.ijpsycho.2024.112355,,,doi:10.18112/openneuro.ds005095.v1.0.1,Ethics committee of the Psychological Institute of Russian Academy of Education,STERNBERG,,EEG,"# Overview
 

 This is the ""Sternberg Difficult"" dataset contain RAW EEG data. Raven Progressive Standard Matrices scores for each participant are provided in participants.tsv
 

 # Task
 Participants completed a version of the Sternberg task (Sternberg, 1966; Fig 1) during EEG recording. Stimuli were all consonants of the Russian alphabet letters, except for “?”[sch] and “?” [ij], presented in sets of 3, 6, 9, 12, and 15 letters. No letter repeated within a set. Each trial was preceded by a 500-1000 ms fixation cross. Encoding (letter set), retention (blank screen), and retrieval (probe letter) phases were allocated 1500ms, 2000ms and 1500ms, respectively. After 1500 ms period, the probe letter disappeared from the screen. Participants were asked to recall whether the probe letter was in the letter set presented during encoding phase. They had unlimited time to respond by pressing a button: the “left arrow” for “no” and the “right arrow” for “yes”. The trial concluded immediately after a response was made, regardless of the reaction time. Participants completed 200 trials in total with 40 trials in difficulty blocks corresponding to each particular set size (i.e., 3, 6, 9, 12, and 15 letters) with an opportunity to take a break after each block. The order of blocks was random, and the number of positive and negative probes was equal in each block. All stimuli were presented and responses were recorded using Psychopy2. 
 

 # Event triggers
 Important: Triggers in the dataset correspond only to the beginning of the stimulus presentation. No additional triggers were implemented to mark the onset of the retention and retrieval periods. However, these timepoints can be computed based on the experimental design. Each sample was presented for 1500 ms, meaning that the retention time occurred strictly 1500 ms after the trigger point appeared in the data. Similarly, the time of retrieval (when participants had to explicitly state whether a new letter had been shown previously) could be marked at 3500 ms relative to the trial onset.",1,0,0
ds004809,2023-10-19 14:46:21,Haydn Herrema,2.2.0,Categorized Free Recall: Delayed Free Recall of Word Lists Organized by Semantic Categories,2023-10-20 0:10:03,0,10,512366000000,477.2 GB,7226,258,18,65,1.7.0,CC0,"Haydn G. Herrema, Michael J. Kahana","We thank our collaborators who have helped in the collection of this data: Hospital of the University of Pennsylvania, Thomas Jefferson University Hospital, Dartmouth-Hitchcock Medical Center, Mayo Clinic, NIH NINDS, Columbia University Hospital, Emory University Hospital, University of Texas Southwestern Medical Center, University of Texas Health Science Center at San Antonio, University of Colorado Anschutz Medical Campus.",,DARPA RAM: N66001-14-2-4032 ===NEMAR-SEP=== NIH: R01-NS-106611 ===NEMAR-SEP=== NIH: U01-NS-113198,,doi:10.18112/openneuro.ds004809.v2.2.0,,catFR1,,iEEG,"### Categorized Free Recall: Delayed Free Recall of Word Lists Organized by Semantic Categories
 

 #### Description
 This dataset contains behavioral events and intracranial electrophysiological recordings from a categorized free recall task. The experiment consists of participants studying a list of words, presented visually one at a time, completing simple arithmetic problems that function as a distractor, and then freely recalling the words from the just-presented list in any order. The data was collected at clinical sites across the country as part of a collaboration with the Computational Memory Lab at the University of Pennsylvania.
 

 Unique to this paradigm is the semantic construction of the word lists. Each word comes from one of 25 semantic categories, and each list of 12 items contains 6 pairs of same-category words from 3 different categories. This means that each list has 4 words from 3 semantic categories, and in each half of the list there will be 1 pair of words from each category. For example, if a list contains words from categories A, B, and C, a possible list construction would be: 
 

 **A1 - A2 - B1 - B2 - C1 - C2 - A3 - A4 - C3 - C4 - B3 - B4**
 

 #### To Note
 * The iEEG recordings are labeled either ""monopolar"" or ""bipolar."" The monopolar recordings are referenced (typically a mastoid reference), but should always be re-referenced before analysis. The bipolar recordings are referenced according to a paired scheme indicated by the accompanying bipolar channels tables.
 * Each subject has a unique montage of electrode locations. MNI and Talairach coordinates are provided when available, along with brain region annotations.
 * Recordings were made on multiple different systems, so we have done the scaling to provide all voltage values in V.
 

 #### Contact
 For questions or inquiries, please contact sas-kahana-sysadmin@sas.upenn.edu.",1,0,0
ds005106,2024-04-24 0:00:26,Tijl Grootswagers,1.0.2,200 Objects Infants EEG,2024-04-26 5:35:07,0,1,1331390000,1.2 GB,133,42,0,0,1.9.1,CC0,"Genevieve Quek, Zhen Zeng, Manuel Varlet, Tijl Grootswagers",,"Quek, Genevieve, Zhen Zeng, Manuel Varlet, and Tijl Grootswagers. 2024. “Human Infant EEG Recordings for 200 Object Images Presented in Rapid Visual Streams.” PsyArXiv. https://doi.org/10.31234/osf.io/dzrkq",,https://osf.io/preprints/psyarxiv/dzrkq ===NEMAR-SEP=== https://doi.org/10.31234/osf.io/dzrkq,doi:10.18112/openneuro.ds005106.v1.0.2,,fix,,EEG,"Data and code for the paper:
 Quek, Genevieve, Zhen Zeng, Manuel Varlet, and Tijl Grootswagers. 2024. “Human Infant EEG Recordings for 200 Object Images Presented in Rapid Visual Streams.” PsyArXiv. https://doi.org/10.31234/osf.io/dzrkq
 

 

 See the linked paper for details.
 

 The ""code"" directory contains all the code to reproduce the figures in the paper.
 It requires fieldtrip and cosmomvpa, change the paths to these toolboxes at the top of each script (or remove the lines and add them to the path manually).
 

 Then run the scripts to reproduce each step reported in the paper:
 1. run_preprocessing.m (preprocess and epoch data)
 2. run_rsa.m (makes the individual RDMs)
 3. stats_rsa.m (computes the RSA correlations)
 4. plot_design.m (produces Figure 1 in the paper)
 5. plot_peaks.m (produces Figure 2 in the paper)
 6. plot_rsa.m (produces Figure 3 in the paper)
 

 Each script can also run standalone, as intermediate results are saved in the derivates folder",1,0,0
ds005114,2024-04-29 18:28:17,James F Cavanagh,1.0.0,EEG: DPX Cog Ctl Task in Acute Mild TBI,2024-04-29 20:16:29,0,3,59996300000,55.9 GB,1796,91,18,55,1.1.1,CC0,James F Cavanagh,,,,DOI: 10.1007/s11682-019-00171-y,doi:10.18112/openneuro.ds005114.v1.0.0,,DPX,,EEG,Dot Probe Continuous Performance Task in control & sub-acute mild TBI.  Published here: https://pubmed.ncbi.nlm.nih.gov/31368085/ For CTL and sub-acute mTBI: Session 1 was from 3 to 14 days post-injury and was the only session with MRI. MRI will be uploaded later (bug issues on upload). Session 2 was ~2 months (1.5 to 3) and Session 3 was ~4 months (3 to 5) following Session 1.  There was A LOT of subject attrition over timepoints.  Same samples as reported here: https://psycnet.apa.org/record/2020-66677-001  https://pubmed.ncbi.nlm.nih.gov/31344589/ 10.1016/j.neuropsychologia.2019.107125  Task included in Matlab programming language.  Data collected 2016-2018 in the Center for Brain Recovery and Repair at the UNM Health Sciences Center. Check the .xls sheet under code folder for *LOTS* more meta data.  Analysis scripts are included. - James F Cavanagh 04/29/2024,1,0,0
ds005121,2024-05-02 4:08:54,Elizabeth M. Siefert,1.0.2,Siefert2024,2024-05-02 19:42:50,0,1,9711090000,9 GB,272,34,0,0,1.2,CC0,"Elizabeth M. Siefert, Sindhuja Uppuluri, Jianing Mu, Marlie C. Tandoc, James W. Antony, Anna C. Schapiro",,"Please cite: 
 E.M. Siefert, S. Uppuluri, J. Mu., M.C. Tandoc, J.W. Antony, A.C. Schapiro (2024). Memory reactivation during sleep does not act holistically on object memory. Journal of Neuroscience, 10.1523/JNEUROSCI.0022-24.2024",,,doi:10.18112/openneuro.ds005121.v1.0.2,,Siefert2024,,EEG,"Overview:
 This is the ""Siefert2024"" dataset. It is the sleep EEG data from Siefert et al., 2024 (https://doi.org/10.1523/JNEUROSCI.0022-24.2024). In brief, it contains sleep EEG data from 34 participants while Targeted Memory Reactivation was administered. 
 

 Please cite the following paper:
 E.M. Siefert, S. Uppuluri, J. Mu., M.C. Tandoc, J.W. Antony, A.C. Schapiro (2024). Memory reactivation during sleep does not act holistically on object memory. Journal of Neuroscience, 10.1523/JNEUROSCI.0022-24.2024
 

 The dataset is formatted according to the Brain Imaging Data Structure (BIDS). Data organization was performed using FieldTrip data2bids function.
 

 Additional details:
 Events.tsv files contain information about the different cueing events.
 - item_value is the sound index number used by the TMR system. This number corresponds to the literal code name of the satellite (i.e., ""nivex"" or ""sorex""). Here, 33 indicates no sound was played but a SO was tagged.
 - SatNum is the corresponding satellite number. This number is the same satellite number that is used in the behavioral data. 
  - 0 indicates no sound was played. 
  - Satellites 1-5 are the studied satellites from the blocked category.
  - Satellites 6-10 are the studied satellites from the interleaved category.
  - Satellites 11-15 are the studied satellites from the uncued category.
 

 System crashed and had to be restarted for participants 6, 7, 15, 17, 21, resulting in two EEG files for these participants.
 

 Please contact Liz Siefert (sieferte@pennmedicine.upenn.edu) with any additional questions.",1,0,0
ds005131,2024-05-10 9:35:59,Ole Bialas,1.0.0,Evoked Responses to Elevated sounds,2024-05-10 18:56:23,0,2,23993100000,22.3 GB,715,58,0,0,1.7.0,CC0,"Ole Bialas, Marc Schoewiesner",,,,,doi:10.18112/openneuro.ds005131.v1.0.0,,"deviantdetection, oneback",,EEG,"# Overview
 

 The dataset consists of data from two experiments in which subjects were presented bursts of noise from loudspeakers at different elevations. Subjects who participated in either experiment were initially tested in their ability to localize elevated sound sources. Both experiments were conducted in a hemi-anechoic chamber.
 Localization Tests
 

 Bursts of pink noise were presented from loudspeakers at different elevations and 10° azimuth (to the listeners right). In the localization test preceding experiment I, these loudspeakers were positioned at elevations of +50°, +25°, 0° and -25° while the localization test preceding experiment II also included a loudspeaker at -50° elevation. Localization test data is missing for sub-001, sub-002 and sub-003
 

 # Deviant Detection (Experiment 1)
 

 Subjects 001-023 participated in this experiment. Subjects heard a long trail of noise from one loudspeaker (adapter), followed by a short burst of noise from another loudspeaker (probe). The elevation of the adapter and probe are encoded in the event values: 2: adapter at 37.5°, probe at 12.5° 3: adapter at 37.5°, probe at -12.5° 4: adapter at 37.5°, probe at -37.5° 5: adapter at -37.5°, probe at 37.5° 6: adapter at -37.5°, probe at 12.5° 7: adapter at -37.5°, probe at -12.5° 8: no adapter, any non-target location (deviant) The behavioral data contains the trial numbers where a deviant was presented and weather the subject responded within one second by pressing a button.
 

 # One-Back (Experiment II)
 

 Subjects 100-134 participated in this experiment. Subjects heard a long trail of white noise through open headphones followed by a short burst of noise from one of the loudspeakers. The loudspeaker's elevation is encoded in the event values: 1: 37.5°, 2: 12.5°, 3:-23.5°, 4:-37.5° Roughly five percent of trials were targets where subjects heard a beep after the trial, prompting them to localize the previously heard sound. The number of those target trials, as well as the target's elevation and the subject's response can be found in thee behavioral data.
 A subset (sub-130-134) participated in a second session of the experiment. This session was identical to the first task with the difference that the subjects had molds inserted that disrupted their ability to perceive sound elevation.",1,0,0
ds004977,2024-02-19 19:52:02,Harvey Huang,1.2.0,CARLA: Adjusted common average referencing for cortico-cortical evoked potential data,2024-05-13 17:39:02,0,1,1604950000,1.5 GB,4459,4,16,19,v 1.14.0,CC0,"Harvey Huang, Gabriela Ojeda Valencia, Nicholas M Gregg, Gamaleldin M Osman, Morgan N Montoya, Gregory A Worrell, Kai J Miller, Dora Hermes","Cindy Nelson, Karla Crockett","This dataset is part of the paper on 'CARLA: Adjusted common average referencing for cortico-cortical evoked potential data' by H Huang, G Ojeda Valencia, NM Gregg, GM Osman, MN Montoya, GA Worrell, KJ Miller, and D Hermes. This project is funded by the National Institute Of Mental Health of the National Institutes of Health under Award Number R01MH122258 to Dora Hermes (Mayo Clinic), by the National Institute of General Medical Sciences of the National Institutes of Health under Award Number T32GM145408 to the Medical Scientist Training Program at Mayo Clinic, and by the American Epilepsy Society under award number 937450 to Harvey Huang (Mayo Clinic). The project was also supported by the Mayo Clinic DERIVE Office and the Mayo Clinic Center for Biomedical Discovery. The data were collected by Harvey Huang (Mayo Clinic), Dora Hermes (Mayo Clinic), Nicholas M. Gregg (Mayo Clinic), Gamaleldin M. Osman (UTHealth Houston), and Cindy Nelson (Mayo Clinic). The BIDS formatting was performed by Harvey Huang (Mayo Clinic), Dora Hermes (Mayo Clinic), Gabriela Ojeda Valencia (Mayo Clinic), and Morgan Montoya (Mayo Clinic). The iEEG data collection was facilitated by Gregory A. Worrell (Mayo Clinic) and Kai J. Miller (Mayo Clinic)",R01 MH122258 CRCNS: Processing speed in the human connectome across the lifespan ===NEMAR-SEP=== T32 GM145408: Medical Scientist Training Program at Mayo Clinic ===NEMAR-SEP=== 937450: AES Predoctoral Research Fellowship,https://doi.org/10.48550/arXiv.2310.00185,doi:10.18112/openneuro.ds004977.v1.2.0,,Cortico Cortical Evoked Potentials,,iEEG,"# CARLA: Adjusted common average referencing for cortico-cortical evoked potential data
 

 This dataset contains intracranial EEG recordings from four patients during single pulse electrical stimulation as described in:
  
 * H Huang, G Ojeda Valencia, NM Gregg, GM Osman, MN Montoya, GA Worrell, KJ Miller, and D Hermes. (2024). CARLA: Adjusted common average referencing for cortico-cortical evoked potential data. Journal of Neuroscience Methods, 110153. DOI: https://doi.org/10.1016/j.jneumeth.2024.110153.
 

 Currently, this dataset contains the raw data needed to generate all results EXCEPT for those pertaining to figures 7 and 8 (unavailable data samples are censored with 0). The complete data are currently being used to answer other scientific questions, and will be released in time with other manuscripts.
 

 Please cite this work when using the data. These data were recorded at the Mayo Clinic in Rochester, MN, as part of the NIH Brain Initiative supported project R01 MH122258 ""CRCNS: Processing speed in the human connectome across the lifespan"". Research reported in this publication was supported by the National Institute Of Mental Health of the National Institutes of Health under Award Number R01MH122258, by the National Institute of General Medical Sciences of the National Institutes of Health under Award Number T32GM145408, and by the American Epilepsy Society under award number 937450. The project was also supported by the Mayo Clinic DERIVE Office and the Mayo Clinic Center for Biomedical Discovery. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. The data were collected by Harvey Huang, Dora Hermes, Nicholas M. Gregg, Gamaleldin M. Osman, and Cindy Nelson. The BIDS formatting was performed by Harvey Huang, Dora Hermes, Gabriela Ojeda Valencia, and Morgan Montoya. The iEEG data collection was facilitated by Gregory A. Worrell and Kai J. Miller.
 

 Data can be analyzed using the Matlab code at: 
 * https://github.com/hharveygit/CARLA_JNM
 

 ## Format
 Data are formatted according to BIDS version 1.14.0
 

 ## Single pulse stimulation
 The patient were resting in the hospital bed, while single pulse stimulation was performed with a frequency of ~0.2 Hz. The stimulation had a duration of 200 microseconds, was biphasic and had an amplitude of 6mA.
 

 ## Contact
 Please contact Harvey Huang (huang.harvey@mayo.edu) or Dora Hermes (hermes.dora@mayo.edu) for questions.",1,0,0
ds005169,2024-05-22 9:00:55,Andrei Barborica,1.0.0,Dataset of intracranial EEG during cortical stimulation evoking visual effects,2024-05-22 11:05:49,0,16,4322720000,4 GB,1165,20,18,47,1.9.0,CC0,"Andrei Barborica, Felicia Mihai, Laurentiu Tofan, Irina Oane, Ioana Mindruta",,,Romanian UEFISCDI PN-III-P4-ID-PCE-2020-0935,"Barborica A, Oane I, Donos C, Daneasa A, Mihai F, Pistol C, Dabu A, Roceanu A, Mindruta I. Imaging the effective networks associated with cortical function through intracranial high-frequency stimulation. Hum Brain Mapp. 2022 Apr 1;43(5):1657-1675. doi: 10.1002/hbm.25749.",doi:10.18112/openneuro.ds005169.v1.0.0,Bucharest University CEC 45/11.06.2020,dcs,,"iEEG, MRI","In this dataset we included iEEG recordings of responses to 115 intracranial high frequency stimulations evoking 
 visual hallucinations, in 22 patients undergoing stereo-EEG presurgical evaluation for drug-resistant epilepsy.
 

 The dataset contains 21 seconds of iEEG data around each stimulation, 8 seconds before the start of the stimulation, 
 up to 5 seconds of intracranial stimulation and 8 seconds after the end of the stimulation.
 

 We have used high-frequency bipolar stimulations of different areas of the brain, using alternating polarity 
 biphasic pulses having a duration of 1 ms, at 43.2 Hz or 50 Hz, current intensity between 0.25 to 3 mA, for up to 5 s. 
 Alternating polarity protocol allows disambiguating neuronal responses time-locked to the stimulation pulses 
 from the artefactual components, according to Barborica et al., 2022 (doi: 10.1002/hbm.25749). It is therefore 
 possible to identify the brain networks underlying the clinical effects, and to create symptom-related 
 activation/connectivity maps.
 

 The contact pair on which stimulation is applied, the current intensity level and evoked effect are specified 
 in the events tsv. The responses are classified in 14 clinical categories: elementary (unstructured 
 flashes of light), plus hallucination (presence of light in different forms or colors overlaying the background vision), 
 minus hallucination (negative elementary phenomena described as scotoma, quadrantanopia, hemianopia or amaurosis), 
 static, dynamic, continuous hallucination, intermittent hallucination, peripheric, central, 
 whole visual field, color, non-color, combined visual symptoms, multimodal hallucinations.
 

 Not all patients in which stimulations evoked visual hallucinations met the inclusion criteria for network analysis 
 that requires running the freesurfer pipeline, for instance patients having prior resections, therefore there are subjects
 that do not contain ieeg data. However, they were kept in order to match the number of patients in the companion manuscript.
 

 Contact: andrei.barborica@fizica.unibuc.ro",1,0,0
ds004917,2024-01-04 21:48:46,Alejandra Figueroa-Vargas,1.0.1,Probability Decision-making Task with ambiguity,2024-05-23 16:20:23,0,1,40213400000,37.5 GB,874,53,18,31,1.9.0,CC0,"Alejandra Figueroa-Vargas, Gabriela Valdebenito-Oyarzo, María Paz Martínez-Molina, Francisco Zamorano, Pablo Billeke",,"If using this dataset, please cite the following papers: Valdebenito-Oyarzo et al. (2024) Plos Biology, doi:10.1371/journal.pbio.3002452",ANID FONDECYT 11140535 ===NEMAR-SEP=== ANID FONDECYT 1181295 ===NEMAR-SEP=== ANID FONDECYT 1211227 ===NEMAR-SEP=== ANID FONDEQUIP EQM150076,GitHub repository: https://github.com/neurocics/LAN_current ===NEMAR-SEP=== OSF repository: https://osf.io/zd3g7/ ===NEMAR-SEP=== Valdebenito-Oyarzo et al. (2024) Plos Biology: https://doi:10.1371/journal.pbio.3002452,doi:10.18112/openneuro.ds004917.v1.0.1,"IRB #2020-67 Comite Etico Cientifico, Universidad del Desarrollo, Chile (https://medicina.udd.cl/comite-etico-cientifico/)",$ProbabilisticDecisionMakingTask,,"MRI, EEG","Summary
 

 This dataset forms part of a study supported by the Social Neuroscience and Neuromodulation Laboratory of Universidad del Desarrollo, Chile.
 The full dataset is described in a submission to Scientific Data.
 

 Abstract
 

 In our daily lives, we frequently encounter decisions where the potential outcomes are unclear, leading to a state of heightened uncertainty. The complete or partial lack of knowledge regarding the probability of outcomes is called ambiguity and presents significant challenges for individuals. While recent studies have associated the level of ambiguity in decision-making with neural activity in the parietal cortex, the precise role of this brain region and its interactions with other brain regions during decision-making processes are not well known. Here, we present a comprehensive dataset detailing human decision-making under conditions of risk and ambiguity. This dataset includes data from 53 healthy volunteers aged between 18 and 31 years, consisting of structural magnetic resonance imaging (MRI: T1w, T2w, and DWI) and functional MRI (fMRI) acquired during task performance, as well as concurrent electrophysiological (EEG) recordings during inhibitory transcranial magnetic stimulation (TMS) applied over two parietal regions and the vertex. This dataset offers an opportunity to delve into the neurobiological mechanisms of decision-making in detail, highlighting the role of the parietal cortex.
 

 Additional Usage Notes
 

 - All code related to this dataset can be found on GitHub (https://github.com/neurocics/LAN_current) and and the additional data set of study are available in the free and open repository of OSF (https://osf.io/zd3g7/) (DOI: 10.17605/OSF.IO/ZD3G7). This includes sourcedata for the scanner tasks and also stimulus presentation scripts.",1,0,0
ds005170,2024-05-22 15:04:36,???,1.0.1,Chisco,2024-05-24 8:20:28,0,5,56223100000,52.4 GB,139,3,22,30,1.6.0,CC0,"Zihan Zhang, Yi Zhao, Yu Bao, Xiao Ding","We would like to express our heartfelt gratitude to everyone who supported and assisted in the research and construction of this dataset. First and foremost, we thank the Social Computing and Information Retrieval Laboratory at Harbin Institute of Technology for their support and resources, which made this research possible. We are especially grateful to our advisor, Professor Xiao Ding, for his guidance, advice, and encouragement throughout the entire research process. We also extend our thanks to all the volunteers who participated in the data collection process, providing valuable data and time. Without their active participation, the completion of our dataset would not have been possible.Finally, we thank all the members of our team, who made significant contributions to data collection, experimental design, and data processing. Without their efforts, the construction of this dataset would not have been achievable.",Please refer to https://github.com/zhangzihan-is-good/Chisco,National Natural Science Foundation of China under Grants 62176079,,doi:10.18112/openneuro.ds005170.v1.0.1,HIT2024035,,,EEG,"# Chisco Dataset
 

 This dataset is a Chinese imagined speech dataset with three participants, identified as sub-01 to sub-03. The dataset includes raw data and preprocessed data in both fif and pkl formats. Information also can be found in https://github.com/zhangzihan-is-good/Chisco
 

 ## Dataset Structure
 

 ### Root Directory
 

 - `dataset_description.json`
 - `participants.tsv`
 - `README`
 - `derivatives/`
 - `sub-01/` to `sub-03/`
 - `textdataset/`
 - `json/`
 

 ### Raw Data
 

 The root directory contains folders `sub-01` to `sub-03` with raw data. Each participant's folder contains 5 session folders, corresponding to data collected over 5 days.
 

 ### Preprocessed Data
 

 Preprocessed data is stored in the `derivatives` folder in both fif and pkl formats.
 

 ### Text Data
 

 The `textdataset` folder and `json` folder contain text data used to stimulate the participants.
 

 ### File Structure
 ```
 /Chisco
  /sub-01
  /ses-01
  /eeg
  sub-01_ses-01_task-imagine_eeg.edf
  ...
  /sub-02
  ...
  /sub-03
  ...
  /derivatives
  /fif
  /sub-01
  ...
  /sub-02
  ...
  /sub-03
  ...
  /pkl
  /sub-01
  ...
  /sub-02
  ...
  /sub-03
  ...
  /textdataset
  ...
  /json
  ...
  dataset_description.json
  README
  participants.tsv
 

 ```
 ## License
 This dataset is licensed under the CC0 license. You are free to use the dataset for non-commercial purposes, but the original author needs to be properly indicated.
 

 ## Citation
 If you use this dataset in your research, please cite the following link:
 

 https://github.com/zhangzihan-is-good/Chisco
 

 ## Contact Information
 For any questions, please contact the dataset authors.
 Thank you for using the Chisco!",1,0,0
ds005189,2024-05-27 22:59:06,Jason Helbing,1.0.1,Search Superiority Recollection Familiarity,2024-05-28 6:57:01,0,1,17255900000,16.1 GB,185,30,18,34,1.9.0,CC0,"Jason Helbing, Dejan Draschkow, Melissa L.-H. Võ",,,,,doi:10.18112/openneuro.ds005189.v1.0.1,"Ethics committee of the Faculty of Psychology and Sports Sciences, Goethe University Frankfurt (ethics approval ID 2014-106R1)",SearchSupRecFam,,EEG,"In this experiment, participants searched for objects in some scenes and intentionally memorized others. We then tested their memory of these objects, finding stronger (quantitative difference) and different (qualitative difference: recollection benefit) memory representations for search targets.
 

 We recorded both EEG and eye movements. Behavioral data is split into encoding (Encode_beh) and memory testing (Test_beh).
 

 Analysis scripts and preprocessed data as well as additional materials are available on the OSF at https://osf.io/esr5q/.
 

 

 Project Abstract:
 

 Most memory is not formed deliberately but as a by-product of natural behavior. These incidental representations, when generated during visual search, can be stronger than intentionally memorized content (search superiority effect). In this study, we investigate whether this effect is purely quantitative (stronger memory) or also due to qualitative memory differences; more precisely, differences in recollection and familiarity, two processes supporting recognition memory. In an EEG study with eye tracking, 30 participants searched for objects in scenes and intentionally memorized others before completing a surprise recognition memory test. We find that compared to new objects, both search targets and intentionally memorized objects elicit a more positive-going mid-frontal negativity peaking at around 400 ms post stimulus onset (FN400), which is associated with familiarity, as well as a more positive-going parietal late component (LPC), indicative of recollection. Both components show no differences between tasks, indicating equal contributions of recollection and familiarity to remembering searched and memorized objects. Behavioral data from remember–know judgments and receiver operating characteristics (ROCs), however, contrasts with the EEG findings: Search targets are more often reported as recollected and their ROCs show higher intercepts, indicating more recollection, whereas there are essentially no behavioral differences in familiarity between tasks. These results indicate that search superiority relies on increased recollection rather than familiarity. The absent LPC effect despite the behavioral task difference challenges existing assumptions about the neural correlates of recognition memory, raising the question whether they hold when investigated using real-world scenes and incidental encoding during naturalistic tasks.",1,0,0
ds005241,2024-06-12 15:20:44,Amilleah Rodriguez,1.1.0,NeuroMorph: A High-Temporal Resolution MEG Dataset for Morpheme-Based Linguistic Analysis,2024-06-12 15:58:04,0,3,150839000000,140.5 GB,554,24,0,0,1.7.0,CC0,"Amilleah Rodriguez, Dan Zhao, Kyra Wilson, Ritika Saboo, Sergey V Samsonau, Alec Marantz",,,New York University Abu Dhabi Institute Grant G1001,,doi:10.18112/openneuro.ds005241.v1.1.0,,"lexicaldecision, localizer",,MEG,"KIT/Yokogawa MEG system with 208 magnetometer channels 
 

 24 subjects amounting to over 17 hours of data
 

 Supplementary code can be found [here](github.com/amilleah/neuromorph)
 

 References
 ----------
 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896).https://doi.org/10.21105/joss.01896
 

 Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., Moreau, J., Oostenveld, R., Schoffelen, J., Tadel, F., Wexler, J., Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5, 180110.https://doi.org/10.1038/sdata.2018.110",1,0,0
ds005207,2024-05-31 23:36:01,kaare mikkelsen,1.0.0,Surrey cEEGrid sleep data set,2024-06-01 19:05:41,0,1,30627400000,28.5 GB,223,20,0,0,1.7.0,CC0,"Kaare B. Mikkelsen, James K Ebajemito, Maria A Bonmati-Carrion, Nayantara Santhi, Victoria L Revell, Giuseppe Atzori, Laura Birch, Ciro Della Monica, Stefan Debener, Derk-Jan Dijk, Annette Sterr, Maarten De Vos",,Please cite Mikkelsen et al 2018: https://doi.org/10.1111/jsr.12786,,,doi:10.18112/openneuro.ds005207.v1.0.0,,Sleep,,EEG,"Surrey sleep data set
 

 **Overview**
 

 This dataset was collected as part of a research project on wearable sleep monitoring which took place in spring 2017.
 

 The data set contains nightly EEG recordings from 20 healthy participants ('subjects'). Some recordings are full polysomnography (PSG) measurements, others are cEEGrid measurements. Most subjects have both PSG and ceegrid recordings from the same night, though a few are missing one or the other.
 

 **Format**
 

 The dataset is formatted according to the Brain Imaging Data Structure. See the 'dataset_description.json' file for the specific BIDS version used. The EEG data format chosen is the '.set' format of EEGLAB.
 

 For more information, see the following link:
 https://bids-specification.readthedocs.io/en/stable/01-introduction.html
 

 

 **Task description**
 

 The patient performed no tasks. The recording equipment was mounted immediately prior to bedtime, and the recordings took place at the sleep laboratory of the Surrey Clinical Research Centre.
 

 Note that due to a miscommunication during the study, alignment information between cEEGrid and PSG recordings has not been saved. This means that to obtain a useful comparison between the two methods, for instance to align the manual scoring with the cEEGrid recordings, some post processing has to be performed. In the derivative dataset, 'aligned1', we have shared our own best attempt at alignment. 
 

 The data set was previously described in the paper 
 'Machine-learning-derived sleep–wake staging from around-the-ear electroencephalogram outperforms manual scoring and actigraphy', Mikkelsen et al 2018, https://doi.org/10.1111/jsr.12786
 

 **Contact**
 

 For questions regarding this data set, contact: 
 Kaare Mikkelsen, Mikkelsen.kaare@ece.au.dk, https://orcid.org/0000-0002-7360-8629",1,0,0
ds004582,2023-05-30 10:41:38,An Shu Te,1.0.0,FakeFaceEmo_data,2024-06-05 14:38:29,0,1,315916000000,294.2 GB,881,73,21,40,1.6.0,CC0,"Makowski, Dominique, Te, An-Shu, Kirk, Stephanie, Ngoi, Zi Liang",,,,,doi:10.18112/openneuro.ds004582.v1.0.0,NTU-IRB-2022-187,FF,,EEG,"# Overview
 

 This dataset was collected in 2023 and comprises electroencephalography, physiological and behavioural data acquired from 73 healthy individuals (ages: 21-45). The task was administered as part of a larger study.
 

 # Task Description
 ## Fake Face (FF)
 The objective of the study was to investigate if emotional arousal would affect people's perceived realness of others' faces, given ambiguous information. To manipulate participants' emotional arousal, images of angry (high emotionality) and neutral (low emotionality) faces (selected based on the their rated intensity from the NimStim Set of Facial Expressions (Tottenham et al., 2009)), were used as subliminal primes and facial images from the Multi-Racial Mega-Resolution database (Strohminger et al., 2016) were used as target stimuli. Blank screens were flashed prior to the target presentation in control trials. Forward and backward masks, generated by scrambling the primes, were implemented to prevent the primes from breaking awareness.
 

 Each participant underwent a total of 222 trials, comprising of a forward mask,followed by the prime and backward mask, before the presentation of the target stimuli. The primes and targets were presented in a randomized order and trials were administered over a course of 3 blocks, between which participants were given a break to rest before proceeding to the next block of trials. During the presentation of the target stimulus, participants were instructed to indicate whether they thought the target was real or fake in a limited span of time (750ms), after which participants rated their confidence in their response using a sliding scale (0-100).
 

 # Data acquisition
 ## EEG data acquisition
 

 EEG signals were recorded using the EasyCap 64-channel and BrainVision Recording system. Electrodes were placed on the EEG cap according to the standard 10-5 system of electrode placement (Oostenveld & Praamsrta, 2001) and impedance was kept below 12 kOhm for each subject. The ground electrode was placed on the forehead the Cz was used as the reference channel. During recording, the sampling rate was 10000Hz. Note that channels Tp9 and Tp10 were placed near the outer canthi of each eye, and POz as well as Oz were fixed above and below one of the eyes to measure the E0G.
 

 

 

 ## Physiological data acquisition
 

 Participants' physiological signals, that is their electrocardiogram (*ECG*), photoplethysmograph (PPG) and respiration signals (*RSP*), were obtained at a sampling frequency of 1000Hz. All physiological signals were recorded via the PLUX OpenSignals software and BITalino Toolkit. 
 

 ECG was collected using three ECG electrodes placed according to a modified Lead II configuration, and RSP was acquired using a respiration belt tightened over participants' upper abdomen. PPG sensors, which record changes in blood volume, were clipped on the tip of the index finger of participants' non-dominant hand to meaure heart rate and oxygen saturation.
 

 

 

 

 

 

 References
 ----------
 Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Höchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A. and Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software 4: (1896). https://doi.org/10.21105/joss.01896
 

 Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G., Phillips, C., Delorme, A., Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Scientific Data, 6, 103. https://doi.org/10.1038/s41597-019-0104-8",1,0,0
ds005234,2024-06-07 17:16:07,Kirill Fadeev,2.1.5,Perception of vowel sounds in children with autism spectrum disorders and typically developing children (MEG/ERF study),2024-06-08 10:01:15,0,1,177702000000,165.5 GB,1829,74,6,13,1.7.0,CC0,"Kirill A. Fadeev, Ilacai V. Romero Reyes, Dzerassa D. Goiaeva, Tatyana S. Obukhova, Tatyana M. Ovsiannikova, Andrey O. Prokofyev, Anna M. Rytikova, Artyom Y. Novikov, Vladimir V. Kozunov, Tatyana A. Stroganova, Elena V. Orekhova",We sincerely thank all of volunteers who participated in this study.,"Please acknowledge this dataset by indicating that it is distributed under the CC BY license. According to this license, when using this dataset, including a reference link to it (i.e., by DOI) is mandatory.",The state assignment of the Ministry of Education of the Russian Federation (N 073-00037-24-01),https://doi.org/10.1101/2024.06.24.600191,doi:10.18112/openneuro.ds005234.v2.1.5,,"vowels, noise",,"MEG, MRI","# Project name:
 

 **Perception of vowel features (formant structure, pitch) in children with autism spectrum disorders and typically developing children (MEG/ERF study).**
 

 ### Related publications:
 - **Preprint:** 
 Attenuated processing of vowels in the left hemisphere predicts speech-in-noise perception deficit in children with autism
 K.A. Fadeev, I.V. Romero Reyes, D.E. Goiaeva, T.S. Obukhova, T.M. Ovsiannikova, A.O. Prokofyev, A.M. Rytikova, A.Y. Novikov, V.V. Kozunov, T.A. Stroganova, E.V. Orekhova
 bioRxiv 2024.06.24.600191; doi: [https://doi.org/10.1101/2024.06.24.600191](https://doi.org/10.1101/2024.06.24.600191)
 

 ### Year(s) that the project ran:
 

 2017-2024.
 

 # Brief overview:
 

 This dataset was obtained by the team at the Center for Neurocognitive Research (MEG Center) of Moscow State University of Psychology and Education as part of a study on vowel perception and their properties in children (Fadeev et al., 2024, preprint). It includes MEG recordings from 35 children with autism spectrum disorders and 39 typically developing children.
 

 # Experimental procedure:
 

 The participants were instructed to watch a silent video (movie/cartoon) of their choice and to ignore the auditory stimuli. The stimuli were delivered binaurally via plastic ear tubes inserted into the ear canals. The tubes were fixed to the MEG helmet to minimize possible noise from contact with the subject’s clothing. The intensity was set at a sound pressure level of 90 dB SPL. The experiment consisted of three blocks of 360 trials, each block lasting around 9 minutes with short breaks between blocks.
 

 # Stimuli:
 

 The experimental paradigm used in this study is identical to that described in Orekhova et al. (2023). We used four types of synthetic vowel-like stimuli previously employed by Uppenkamp et al. (2006, 2011) and downloaded from [http://medi.uni-oldenburg.de/members/stefan/phonology_1/](http://medi.uni-oldenburg.de/members/stefan/phonology_1/). **You can also find a copy of the sound files in the `stimuli` directory on this dataset page**. Five strong vowels were used: 
 - /a/ (caw, dawn), 
 - /e/ (ate, bait), 
 - /i/ (beat, peel), 
 - /o/ (coat, wrote) and 
 - /u/ (boot, pool). 
 

 A total of 270 stimuli from each of the four classes were presented, with three stimulus variants equally represented within each class (N = 90). All stimuli were presented in random order. Each stimulus lasted 812 ms, including rise/fall times of 10 ms each. The interstimulus intervals (ISI) were randomly chosen from a range of 500 to 800 ms.
 

 Short names of stimuli (used in code, filenames, and directory names):
 - `dv` - periodic vowels
 - `rv` - non-periodic vowels
 - `mp` - periodic non-vowels
 - `mr` - non-periodic non-vowels
 

 # Additional data acquired:
 

 The following tests were also administered to the study participants: 
 - Pure tone air conduction audiometry;
 - Russian Child Language Assessment Battery (Lopukhina et al., 2019);
 - Words-in-Noise (WiN) test (Fadeev et al., 2023);
 - Social Responsiveness Scale for children (Constantino, 2013);
 - Social Communication Questionnaire (SCQ-Lifetime) (Berument et al., 1999),
 - KABC-II, Mental Processing Index (MPI) as an IQ equivalent (Kaufman & Kaufman, 2004).
 

 # Dataset content:
 

 ## MEG data
 `sub-LABEL/meg/...meg.fif` -- 3 runs (in some cases, the number of runs may differ due to the subjects' features). MEG data were recorded using Elekta VectorView Neuromag 306-channel MEG detector array (Helsinki, Finland) with 0.1 - 330 Hz inbuilt filters and 1000 Hz sample frequency.
 

 ## MRI data
 `sub-LABEL/anat/` -- T1-weighted images MRI for subject.
 

 ## Freesurfer
  `derivatives/freesurfer/` - outputs of running the FreeSurfer pipeline `recon-all` on the MRI data with no additional command line options (only defaults were used):
 ```
 $ recon-all -i sub-Z201_T1w.nii.gz -s Z201 -all
 ```
 After the `recon-all` call, there were further FreeSurfer calls from the MNE
 API:
 ```
 $ mne make_scalp_surfaces -s Z201 --force
 $ mne watershed_bem -s Z201
 ```
 

 ## Code
 The code of the project was developed and tested for Ubuntu OS (20-22 x64 version) and has the following structure (directory names provide explanations of their contents):
 

 ```
 code
 +-- analysis
 |  +-- 0-preprocessing_for_clustering
 |  |  +-- ...
 |  +-- 1-tfce_clustering
 |  |  +-- ...
 |  +-- 2-clustering_results_analytics
 |  |  +-- ...
 |  +-- modules
 |  +-- clustering.py
 |  +-- data_ops.py
 +-- envs
 |  +-- envs_for_between_groups_clustering_in_auditory_cortex_with_morphological_sign_flip.json
 |  +-- envs_for_interaction_clustering_in_auditory_cortex_with_morphological_sign_flip.json
 |  +-- envs_for_within_groups_clustering_in_auditory_cortex_with_morphological_sign_flip.json
 +-- preprocessing
 |  +-- 00-maxwell_filtering.py
 |  +-- ...
 |  +-- 10-make_stc.py
 +-- README
 +-- requirements_for_ubuntu_2x.txt
 +-- requirements_for_windows_1x.txt
 ```
 

 <font color=""#ff0000"">**Please read `code/README.md` file for more detail instructions.** </font>
 

 ### Requirements for Code Usage (MNE-Python & Additional Python Libraries)""
 

 1. For installation, we recommend the Anaconda distribution. Find the installation guide here: [Anaconda Installation Guide](https://docs.anaconda.com/anaconda/install/).
 

 2. After you have a working version of Python 3, simply install a new virtual environment via this command in the Ubuntu terminal or Anaconda Prompt for Windows OS:
 

 ```sh
 conda create --name=mne1.4 --channel=conda-forge python=3.10 mne=1.4.1 numpy=1.23.1 spyder pyvista pyvistaqt pingouin scipy mne-bids pandas openpyxl autoreject
 ```
 

 

 Or use the requirements_xxxxxx.txt files located in `code` directory:
 - For Ubuntu OS (version 20 and above), please use `requirements_for_ubuntu_2x.txt`.
 - For Windows OS (version 10 and above), please use `requirements_for_windows_1x.txt`.
 

 To do this, you can run the following command in the Ubuntu terminal or Anaconda Prompt for Windows OS:
 

 ```
 $ conda create --name mne1.4 --file requirements_filename
 ```
 

 3. Activate the created environment:
 ```
 $ conda activate mne1.4
 ```
 

 4. Start your favorite IDE with this environment. For example, to start Spyder IDE, use this command after activating the environment:
 ```
 $ spyder
 ```
 

 5. To start processing data, go to the directory with the downloaded data and open the Python scripts of interest. (If you are using Spyder IDE, use the ""Files"" pane located in the upper right corner of the workspace.)
 

 ## Clustering results directories
 

 All output files from the scripts of the `code` directory are saved in the `derivatives` directory.
 

 Clustering Versions mnemonics (phrases used in python scripts names and directory names):
 - `v13`: TFCE clustering based on groups ? conditions difference response
 - `v14`: TFCE clustering based on between-conditions difference response
 - `v15`: TFCE clustering based on between groups evoked response
 

 The derivatives of project has the following structure (directory names provide explanations of their contents):
 

 ```
 derivatives/preprocessing/
 +-- fsaverage_labels_of_analytics
 |  +-- auditory_cortex_region-lh.label
 |  +-- auditory_cortex_region-rh.label
 +-- fsaverage_stcs_after_morph_flip_in_labels_of_analytics
  +-- subjects_info_for_morphological_sign_flipped_data_1000Hz
  +-- subjects_stc_for_morphological_sign_flipped_data_1000Hz
 

 derivatives/analysis/
 +-- 20240607_74subj_v13_500Hz_5000_perm_DV-MR_TD_vs_ASD_0-800_msec_tfce_interaction_in_auditory_cortex_morph_flip_with_5e-02_clusters_p_thresh
 +-- 20240608_74subj_v13_500Hz_5000_perm_MP-MR_TD_vs_ASD_0-800_msec_tfce_interaction_in_auditory_cortex_morph_flip_with_5e-02_clusters_p_thresh
 +-- 20240608_74subj_v13_500Hz_5000_perm_RV-MR_TD_vs_ASD_0-800_msec_tfce_interaction_in_auditory_cortex_morph_flip_with_5e-02_clusters_p_thresh
 +-- 20240608_74subj_v14_500Hz_5000perm_ASD_DV-MR_0-800_msec_tfce_1samp_within_groups_in_auditory_cortex_morph_flip_5e-02_clusters_p_thresh
 +-- 20240608_74subj_v14_500Hz_5000perm_ASD_MP-MR_0-800_msec_tfce_1samp_within_groups_in_auditory_cortex_morph_flip_5e-02_clusters_p_thresh
 +-- 20240608_74subj_v14_500Hz_5000perm_ASD_RV-MR_0-800_msec_tfce_1samp_within_groups_in_auditory_cortex_morph_flip_5e-02_clusters_p_thresh
 +-- 20240608_74subj_v14_500Hz_5000perm_TD_DV-MR_0-800_msec_tfce_1samp_within_groups_in_auditory_cortex_morph_flip_5e-02_clusters_p_thresh
 +-- 20240608_74subj_v14_500Hz_5000perm_TD_MP-MR_0-800_msec_tfce_1samp_within_groups_in_auditory_cortex_morph_flip_5e-02_clusters_p_thresh
 +-- 20240608_74subj_v14_500Hz_5000perm_TD_RV-MR_0-800_msec_tfce_1samp_within_groups_in_auditory_cortex_morph_flip_5e-02_clusters_p_thresh
 +-- 20240608_74subj_v15_500Hz_5000_perm_DV_TD_vs_ASD_in_0-800_msec_tfce_between_groups_in_auditory_cortex_morph_flip_with_5e-02_cluster_p_threshold
 +-- 20240608_74subj_v15_500Hz_5000_perm_MP_TD_vs_ASD_in_0-800_msec_tfce_between_groups_in_auditory_cortex_morph_flip_with_5e-02_cluster_p_threshold
 +-- 20240608_74subj_v15_500Hz_5000_perm_RV_TD_vs_ASD_in_0-800_msec_tfce_between_groups_in_auditory_cortex_morph_flip_with_5e-02_cluster_p_threshold
 

 ```
 

 # Data user agreement:
 

 Dataset is distributed under the CC BY license. According to this license, when using this dataset, including a reference link to it (i.e., by DOI) is mandatory.
 

 # Acknowledgements:
 

 We sincerely thank all of volunteers who participated in this study.
 

 # Funding:
 

 The study was funded within the framework of the state assignment of the Ministry of Education of the Russian Federation (N 073-00037-24-01).
 

 # References:
 

 1. Gutschalk, A., & Uppenkamp, S. (2011). Sustained responses for pitch and vowels map to similar sites in human auditory cortex. Neuroimage, 56(3), 1578-1587. doi:10.1016/j.neuroimage.2011.02.026
 

 2. Orekhova, E. V., Fadeev, K. A., Goiaeva, D. E., Obukhova, T. S., Ovsiannikova, T. M., Prokofyev, A. O., & Stroganova, T. A. (2023). Different Hemispheric Lateralization for Periodicity and Formant Structure of Vowels in the Auditory Cortex and Its Changes between Childhood and Adulthood. Cortex. doi:10.1016/j.cortex.2023.10.020
 

 3. Uppenkamp, S., Johnsrude, I. S., Norris, D., Marslen-Wilson, W., & Patterson, R. D. (2006). Locating the initial stages of speech-sound processing in human temporal cortex. Neuroimage, 31(3), 1284-1296. doi:10.1016/j.neuroimage.2006.01.004
 

 4. Fadeev, K. A., Goyaeva, D. E., Obukhova, T. S., Ovsyannikova, T. M., Shvedovskiy, E. F., Yu. Nikolaeva, A., Davydova, E. Y., Stroganova, T. A. & Orekhova, E. V. (2023). Difficulty with Speech Perception in the Background of Noise in Children with Autism Spectrum Disorders Is Not Related to Their Level of Intelligence. Clinical Psychology and Special Education, 12(1), 180–212. https://doi.org/10.17759/cpse.2023120108
 

 5. Fadeev, K. A., Romero Reyes, I. V., Goiaeva, D. E., Obukhova, T. S., Ovsiannikova, T. M., Prokofyev, A. O., Rytikova, A. M., Novikov, A. Y., Kozunov, V. V., Stroganova, T. A., & Orekhova, E. V. (2024). Attenuated processing of vowels in the left hemisphere predicts speech-in-noise perception deficit in children with autism. bioRxiv (Cold Spring Harbor Laboratory). https://doi.org/10.1101/2024.06.24.600191
 

 6. Lopukhina, A., Chrabaszcz, A., Khudyakova, M., Korkina, I., Yurchenko, A. & Dragoy, O. (2019). Test for assessment of language development in Russian «KORABLIK». Proceedings of the Satellite of AMLaP conference «Typical and Atypical Language Development Symposium».
 

 7. Kaufman, A. S. & Kaufman, N. L. (2004). KABC-II : Kaufman Assessment Battery for Children (2nd ed., Vol. 1–8). AGS Pub.
 

 8. Berument, S. K., Rutter, M., Lord, C., Pickles, A. & Bailey, A. (1999). Autism screening questionnaire: diagnostic validity. The British Journal of Psychiatry: The Journal of Mental Science, 175, 444–451. https://doi.org/10.1192/bjp.175.5.444
 

 9. Constantino, J. N. (2013). Social Responsiveness Scale. In F. R. Volkmar (Ed.), Encyclopedia of Autism Spectrum Disorders (pp. 2919–2929). Springer New York. https://doi.org/10.1007/978-1-4419-1698-3_296",1,0,0
ds005262,2024-06-17 17:02:01,Antony Emil,1.0.0,ArEEG: Arabic Inner Speech EEG dataset,2024-06-17 18:11:35,0,21,722210000,0.673 GB,561,12,0,0,1.9.0,CC0,"Donia Metwalli, Eslam Ahmed, Antony Emil, Yousef A. Radwan, Mariam Barakat, Anas Ahmed",,,,,doi:10.18112/openneuro.ds005262.v1.0.0,,,,EEG,,1,0,0
ds004624,2023-06-30 19:55:34,Filip Mivalt,1.2.2,Intracranial recordings using BCI2000 and the CorTec BrainInterchange,2023-06-30 21:13:17,0,7,1080560000,1 GB,1230,2,0,0,Brain Imaging Data Structure Specification v1.8.0,CC0,"F. Mivalt, F. Lampert, M.A. van den Boom, P. Brunner, J. Kim, Andrea Duque-lopez, M. Krakorova, V. Kremen, D. Hermes, G.A. Worrell, K. J. Miller",,,NIH U01NS128612,,doi:10.18112/openneuro.ds004624.v1.2.2,IACUC A00001713,,,"iEEG, MRI","An Ecosystem of Technology and Protocols for Adaptive Neuromodulation Research in Humans
 

 This study aims to develop an ecosystem for the purpose of neurmodulation using the Cortec BCI device and BCI2000 software.
 

 Contact: For questions regarding this dataset, please contact
 mivalt.filip@mayo.edu or Miller.Kai@mayo.edu
 

 Funding: NIH U01NS128612",1,0,0
ds005273,2024-06-21 12:40:05,Pablo Rodríguez-San Esteban,1.0.0,Neural representation of consciously seen and unseen information,2024-06-22 16:40:07,0,1,47690900000,44.4 GB,324,33,0,0,1.7.0,CC0,"Pablo Rodríguez-San Esteban, Ana B. Chica, José A. González-López",,,,,doi:10.18112/openneuro.ds005273.v1.0.0,,1,,EEG,,1,0,0
ds005107,2024-04-24 6:49:36,Wei Xu,1.0.3,OPM-FACE,2024-06-26 6:05:45,0,2,29629500000,27.6 GB,2555,21,0,0,1.7.0,CC0,"Wei Xu, Bingjiang Lyu, Xingyu Ru, Dongxu Li, Wenyu Gu, Xiao Ma, Fufu Zheng, Tingyue Li, Pan Liao, Hao Cheng, Rui Yang, Jingqi Song, Zeyu Jin, Congcong Li, Kaiyan He, Jia-Hong Gao",,,,,doi:10.18112/openneuro.ds005107.v1.0.3,,face,,MEG,"To run the whole analysis sequentially, please run `python face_0_main.py`. This file contains the configuration and main entrance. Please refer to `face_0_main.py` for more details. 
 `face_1_prep.py` is for preprocessing. Note that we provide a config flag named `cfg['auto_flag']` to control whether the preprocessing is done automatically or manually. This flag should be set to `cfg['auto_flag']=True`, to avoid bulky manual work. 
 `face_2_dec.py` is for SVM decoding, including SVM decoding for temporal dynamics, temporal generalization, and SVM decoding in the test-retest section. 
 `face_3_rsa.py` is for multivariate pattern analysis. 
 `face_4_stat.py` is for statistical tests mainly cluster-based permutation tests. 
 `face_6_bayes.m` is used to run Bayesian model selection to recognize the temporal generalization patterns. It should be run manually as the main entrance `face_0_main.py` only includes step `1` to `4`. Before running this Matlab script, ensure you have already installed [VBA toolbox](https://github.com/MBB-team/VBA-toolbox) in your search path. 
 Note that all of the preprocessed data and intermediate results are placed in the `derivatives` folder (of course you could change it to wherever you want simply by referring to the configuration in `face_0_main.py`). 
 In the `./code/experiment` folder is the Matlab script for the experiment. Before running the experiment, ensure you have set up [Psychtoolbox](https://github.com/Psychtoolbox-3/Psychtoolbox-3) correctly in the Matlab searching path. The usage is detailed in `main.m`. The stimuli are placed in the `./stimuli/meg` folder.",1,0,0
ds004796,2023-10-15 15:01:35,Patrycja Dzianok,1.0.8,"A Polish Electroencephalography, Alzheimer’s Risk-genes, Lifestyle and Neuroimaging (PEARL-Neuro) Database",2024-02-18 10:39:31,0,1,257897000000,240.2 GB,2056,79,50,63,1.0.2,CC0,"Dzianok Patrycja, Kublik Ewa",,"Dzianok P, Kublik E. PEARL-Neuro Database: EEG, fMRI, health and lifestyle data of middle-aged people at risk of dementia. Sci Data 11, 276 (2024). DOI: https://doi.org/10.1038/s41597-024-03106-5",Polish National Science Center (NCN) grant no. 2018/31/N/HS6/03551,,doi:10.18112/openneuro.ds004796.v1.0.8,"This study was approved by the Bioethics Committee of the Nicolaus Copernicus University in Torun at the Ludwik Rydygier Collegium Medicum in Bydgoszcz, Poland (KB 684/2019)","msit, rest, sternberg",,"MRI, EEG","## A Polish Electroencephalography, Alzheimer’s Risk-genes, Lifestyle and Neuroimaging (PEARL-Neuro) Database 
 

 ### Data Descriptor:
 * doi.org/10.1038/s41597-024-03106-5 (https://www.nature.com/articles/s41597-024-03106-5)
 

 ### Please cite the following reference if you use these data:
 * Dzianok P, Kublik E. PEARL-Neuro Database: EEG, fMRI, health and lifestyle data of middle-aged people at risk of dementia. Sci Data 11, 276 (2024). DOI: https://doi.org/10.1038/s41597-024-03106-5
 

 ### Publications related to this dataset, reporting & additional data
 

 * https://github.com/PTDZ/PEARL-Neuro — updates, additional study details, and list of research outputs related to this dataset.
 

 IMPORTANT: Please inform us of any research outputs related to the shared data, including publications, preprints, posters, abstracts, talks, and any commercial usage. This is crucial for ensuring transparency and informing users about the analyses already performed on this dataset. Additionally, such information can foster collaboration.
 

 ### Description of the database:
 

 Full cohort: 192 healthy middle-aged (50-63) individuals, balanced female and male ratio. 
 

 * Genetic data (N = 192):
  * Apolipoprotein E (APOE) 
  * Phosphatidylinositol binding clathrin assembly protein (PICALM)
 * Basic demographic and health data
 * Psychometric data (memory, intelligence, mood, personality, stress coping strategies)
 

 Cohort subgroup: 79 healthy middle-aged (50-63) individuals, balanced female and male ratio.
 

 * Neuroimaging data:
  * Functional data — electroencefalography (EEG) and functional magnetic resonance imaging (fMRI):
   * Resting-state protocol (with two conditions: eyes open and eyes closed) 
   * Cognitive tasks: multi-source interference task (MSIT) and Sternberg’s memory task
 * Blood tests data (blood count, lipid profile, HSV virus)
 

 ### Release history:
 * 10/2023: Initial release
 * 02/2024: Public release",1,0,0
ds005279,2024-06-24 16:17:02,Tiana Wei,1.0.3,Picture-Word Interference Dataset,2024-06-25 16:13:19,0,1,63222400000,58.9 GB,1590,30,0,0,1.9.0,CC0,"Hsi T. Wei, Farhan B. Faisal, Tamara Beck, Claire Shao, Jed A. Meltzer",,"please acknowledge its authors (Hsi T. Wei, Farhan B. Faisal, Tamara Beck, Claire Shao and Jed A. Meltzer)",This research was supported by NSERC Discovery Grant RGPIN-2019-06515,,doi:10.18112/openneuro.ds005279.v1.0.3,,,,"MEG, MRI","This study was conducted at the Rotman Research Institute at Baycrest Hospital in Toronto, Canada.
 

 This dataset contains 30 healthy young adults' MEG (CTF), sMRI, and behavioural data on a picture-word interference (PWI) task. Subjects were shown images of objects one by one and were instructed to retrieve the name of the pictures covertly and judge whether the name ends in a target sound given at the beginning of each task block, by pressing the yes or no buttons with their right hand. Whenever they see an image, they will often also hear a distractor word played through their earphone. The picture and word could be phonologically related, semantically related, or unrelated.
 

 There were 3 runs of the PWI task for each participant. Each run contained 120 trials, containing an equal number of trials for each picture-word condition. Behaviourally, the reaction time and accuracy of their button-pressing response were recorded. Meanwhile, the MEG data was epoched to the picture onset and response onset for event-related analyses. Each subject obtained their own structural MRI for MEG source localization.
 

 Corresponding analysis code can be found under the code folder, with the ""analysis walkthrough"" documenting more detailed explanation of the analysis.",1,0,0
ds004587,2023-06-01 5:42:34,An Shu Te,1.0.0,IllusionGameEEG_data,2024-06-28 11:20:39,0,1,235518000000,219.3 GB,1605,103,21,40,1.6.0,CC0,"Makowski, Dominique, Te, An-Shu, Jiayi, Zhang, Kirk, Stephanie, Ngoi, Zi Liang",,,,,doi:10.18112/openneuro.ds004587.v1.0.0,,IG,,EEG,"# Overview
 

 This dataset was collected in 2022-20233 and comprises electroencephalography, physiological and behavioural data acquired from 103 healthy individuals (ages: 21-45). The task was administered as part of a larger study.
 

 # Task Description
 ## Illusion Game (IG)
 The aim of this task is to investigate people's sensitivity to visual illusions as a general, common factor. Using Pyllusion, which enabled us to manipulate the objective parameters of visual illusions, we generated stimuli of varying task difficulty and illusion strength for 3 different classic illusions (Ebbinghaus, Müller-Lyer and Vertical-Horizontal). We then created an experimental task in which participants were instructed to make perceptual judgements about targets in the illusion as quickly as possible, ignoring its context, which biases their perception of the illusion. For instance, in the Müller-Lyer illusion, the same-length line segments (*targets*) appear to have different lengths if they end with inwards vs. outwards pointing arrows (*context*). The first series of the 3 illusion blocks (each comprising 64 trials) were first presented to participants in a randomized order, followed by a short break, after which participants performed the second series of blocks displayed in a newly randomized order. In total, each participant performed 384 illusion trials (6*64).
 

 ## Resting State
 Before the start of the illusion task, paricipants were instructed to keep their eyes closed for 8 minutes. At the end of the resting period, a 'beep' soundclip was played to cue participants to open their eyes. An adapted version of the Amsterdam Resting State Questionnaire (Diaz et al., 2014) was then administered to examine participants' subjective resting state experience.
 

 

 ## NOTES
 Due to a technical error, sub-FFE111 and sub-FFE116 do not have any physiological data, and sub-FFE117, sub-FFE139 and sub-FFE146 do not have behavioural data for the illusion game task. 
 

 EEG data collection was split into 6 runs corresponding to each block of illusion trials for sub-FFE111 and sub-FFE121 during pilot testing. 
 

 EEG data collection was collected twice for sub-FFE007 due to a technical glitch that occcured in the middle of illusion task trials.
 

 # Data acquisition
 ## EEG data acquisition
 

 EEG signals were recorded using the EasyCap 64-channel and BrainVision Recording system. Electrodes were placed on the EEG cap according to the standard 10-5 system of electrode placement (Oostenveld & Praamsrta, 2001) and impedance was kept below 12 kOhm for each subject. The ground electrode was placed on the forehead the Cz was used as the reference channel. During recording, the sampling rate was 10000Hz. Note that channels Tp9 and Tp10 were placed near the outer canthi of each eye, and POz as well as Oz were fixed above and below one of the eyes to measure the E0G.
 

 

 ## Physiological data acquisition
 

 Participants' physiological signals, that is their electrocardiogram (*ECG*), photoplethysmograph (PPG) and respiration signals (*RSP*), were obtained at a sampling frequency of 1000Hz. All physiological signals were recorded via the PLUX OpenSignals software and BITalino Toolkit. 
 

 ECG was collected using three ECG electrodes placed according to a modified Lead II configuration, and RSP was acquired using a respiration belt tightened over participants' upper abdomen. PPG sensors, which record changes in blood volume, were clipped on the tip of the index finger of participants' non-dominant hand to meaure heart rate and oxygen saturation.
 

 

 References
 ----------
 Diaz, B. A., Van Der Sluis, S., Benjamins, J. S., Stoffers, D., Hardstone, R., Mansvelder, H. D., ... & Linkenkaer-Hansen, K. (2014). The ARSQ 2.0 reveals age and personality effects on mind-wandering experiences. Frontiers in psychology, 5, 271.",1,0,0
ds005296,2024-06-28 23:15:10,Emily M. Akers,1.0.0,Sentence semantic and syntactic violations,2024-06-28 23:25:07,0,1,9154610000,8.5 GB,317,62,0,0,1.8.0,CC0,"Karen Emmorey, Emily M. Akers, Katherine J. Midgley, Phillip J. Holcomb",,,,,doi:10.18112/openneuro.ds005296.v1.0.0,,sentencesemanticandsyntacticviolations,8.1.0,EEG,"Data collection took place at the NeuroCognition Laboratory (NCL) in San Diego, California under the supervision of Dr. Phillip Holcomb. This project followed the San Diego State University’s IRB guidelines.  
  
 Participants sat in a comfortable chair in a darkened sound attenuated room throughout the experiment. They were given a keyboard for button pressing and wore a lightweight headset to record their verbal responses. They were instructed to watch the LCD video monitor that was at a viewing distance of 60in. 
  
 Participants were presented with 180 sentences in white font on a black background. Conditions consisted of 30 subject-verb agreement violations, 30 semantic violations, 30 double (subject-verb agreement + semantic) violations, 30 word-order violations, and 60 control (correct) sentences. Sentences were presented in an RSVP design, one word at a time, in the middle of the screen for a duration of 600ms with an ISI of 200ms.",1,0,0
ds004980,2024-02-20 16:15:54,??,1.0.0,EEG data set for a architectural affordances task,2024-07-03 13:13:08,0,1,16989500000,15.8 GB,107,17,19,31,unofficial extension,CC0,"Wang,S., Oliveira,G.S., Djebbara,Z, Gramann, K.",n/a,,China Scholarship Council,unpublished,doi:10.18112/openneuro.ds004980.v1.0.0,,DefaultTask,,EEG,,1,0,0
ds005305,2024-07-03 14:53:31,Quentin Chenot,1.0.1,EEG Resting-state Microstates Correlates of Executive Functions,2024-07-04 8:07:00,0,1,6850530000,6.4 GB,1161,165,0,0,1.8.0,CC0,"Chenot Quentin, Hamery Caroline, Truninger Moritz, De Boissezon Xavier, Langer Nicolas, Scannella Sébastien",,,,https://osf.io/zfhcn/,doi:10.18112/openneuro.ds005305.v1.0.1,,Resting-State,8.1.0,EEG,"## README
 Project name: Project Microstates & Executive Functions (Project_microstates_EFs)
 Years:   2021-2023
 Contact person: Quentin chenot (quentinchenot@gmail.com)
 

 ## Overview
 Summary: This study aimed to specifically explore the relationship between intrinsic brain spatio-temporal dynamics and Executive Functions.
 To do so, resting-state EEG microstates were used to assess brain spatio-temporal dynamics in 140 healthy participants, while a comprehensive battery of nine cognitive function tasks was employed to evaluate their executive functions.
 Correlations were computed between the EEG microstates metrics at rest and the mean score in executive function tasks.
 

 ## Dataset content
 1160 Files, 6.38GB
 165 - Subjects
 2 - Session
 Available Tasks: Resting-State, Antisaccade, Category-Switch, Color-Shape, Dual N-back, Keep-Track, Letter-Memory, Number-Letter, Stop-Signal, Stroop
 Available Modalities: EEG
 Independent variables: NA
 Dependent variables:
  DV1: Mean score in the nine executive functions tasks (z-scored)
  DV2: resting-state EEG microstates metrics (number of occurrences, mean duration)
 Control variables:
  Age
  Gender
  Education
  Handedness
 

 ## Methods
 ### Subjects
 165 participants were recruited for this experiment. 140 constitute the final sample.
 Recrutment procedure: participants were recruited with flyers, mailing-lists and mouth to hear in the Toulouse University campuses.
 Inclusion criteria: age (18-35 years); affiliation to social insurance; having read the information document about the experiment; signed informed consent form; native French language
 Exclusion criteria: addiction (alcohol, drugs); major hearing loss; major visual deficit; including hemianopsia and color blindness; neurological or psychiatric pathology; known brain injury, drugs intake targeting the central nervous system; refusal to sign the consent form
 

 ### Material
 Participants performed the experiment in a windowless room at a stable temperature, seated and facing a 24' inches screen. They underwent two sessions:
 session 1: EEG measures with a 5 min resting-state alternating between eyes closed and eyes open (30 sec each). EEG apparatus: 64 electrodes Biosemi Active-two amplifier (data acquired at 512 Hz)
 session 2: behavioral measures, with the nine cognitive tasks in the following order: antisaccade, letter-memory, color–shape, number–letter, Stroop, keep track, dual n-back, category switch, stop-signal. 
 

 ### Experimental location and acquisition timeframe
 The experiment took place in the Centre de Neuroergonomie at ISAE-SUPAERO (Toulouse, France), from february 2022 to july 2023.
 

 ## Installation and Setup
 The repository contains scripts for preprocessing and analyzing behavioral and EEG data, as well as the project's documentation and results.
 See code_documentation.pdf in \docs (available in https://osf.io/fm58p/).
 

 ## Publications
 Registered Report stage 1 IPA: EEG resting-state microstates correlates of executive functions (https://osf.io/dwz2r)
 Registered Report stage 2: Investigating the relationship between Resting-State EEG Microstates and Executive Functions: A Null Finding (https://doi.org/10.1016/j.cortex.2024.05.019)
 

 ## Authors
 Quentin Chenot, Caroline Hamery, Moritz Truninger, Xavier De Boissezon, Nicolas Langer, Sébastien Scannella
 

 ## CRediT author statement
 QC: conceptualization, methodology, software, formal analysis, investigation, data curation, writing – original draft, writing – review & editing, Visualization.
 CH: methodology, software, validation, formal analysis, data curation, writing – review & editing, visualization.
 MT: methodology, software, validation, formal analysis, data curation, writing – review & editing, visualization.
 NL: methodology, software, validation, formal analysis, resources, writing – review & editing.
 XDB: resources, writing – review & editing, supervision, funding acquisition.
 SS: conceptualization, methodology, resources, writing – review & editing, supervision, project administration, funding acquisition.
 

 ## License
 This work is licensed under a Creative Commons Attribution 4.0 International License.
 

 ## Funding
 This work is supported by the French National Research Agency (ANR) and the Defense Procurement Agency (DGA), ASTRID program [grant numbers ANR-17-ASTR-0005]
 

 

 ### Notes - Participant and sessions timeframe
 Participant Session 1   Session 2 (DD/MM/YYYY - HH) 
 sub-001  21/02/2022 - 10h 24/02/2022 - 14h 
 sub-002  21/02/2022 - 17h 01/03/2022 - 17h 
 sub-003  22/02/2022 - 8h  28/02/2022 - 11h 
 sub-004  22/02/2022 - 13h 01/03/2022 - 9h 
 sub-005  08/03/2022 - 9h  14/03/2022 - 9h15 
 sub-006  08/03/2022 - 13h 10/03/2022 - 13h30 
 sub-007  08/03/2022 - 15h30 16/03/2022 - 9h 
 sub-008  09/03/2022 - 13h 11/03/2022 - 10h 
 sub-009  09/03/2022 - 17h 17/03/2022 - 10h30 
 sub-010  10/03/2022 - 13h 11/03/2022 - 13h 
 sub-011  10/03/2022 - 19h 17/03/2022 - 18h30 
 sub-012  14/03/2022 - 8h  21/03/2022 - 8h30 
 sub-013  14/03/2022 - 17h 18/03/2022 - 17h 
 sub-014  15/03/2022 - 8h  18/03/2022 - 14h30 
 sub-015  15/03/2022 - 13h 21/03/2022 - 15h30 
 sub-016  16/03/2022 - 8h  23/03/2022 - 8h 
 sub-017  16/03/2022 - 13h 22/03/2022 - 15h00 
 sub-018  16/03/2022 - 15h30 22/03/2022 - 9h 
 sub-019  17/03/2022 - 8h  25/03/2022 - 8h15 
 sub-020  17/03/2022 - 16h30 24/03/2022 - 14h 
 sub-021  18/03/2022 - 9h  22/03/2022 - 13h 
 sub-022  18/03/2022 - 13h 24/03/2022 - 11h30 
 sub-023  18/03/2022 - 16h 22/03/2022 - 17h 
 sub-024  21/03/2022 - 16h 28/03/2022 - 16h 
 sub-025  22/03/2022 - 9h  28/03/2022 - 9h 
 sub-026  22/03/2022 - 13h 25/03/2022 - 15h 
 sub-027  23/03/2022 - 18h 30/03/2022 - 18h 
 sub-028  24/03/2022 - 17h 25/03/2022 - 13h 
 sub-029  25/03/2022 - 13h 31/03/2022 - 17h30 
 sub-030  28/03/2022 - 9h  31/03/2022 - 9h 
 sub-031  29/03/2022 - 17h 31/03/2022 - 13h 
 sub-032  30/03/2022 - 17h 01/04/2022 - 14h 
 sub-033  04/04/2022 - 9h  05/04/2022 - 14h 
 sub-034  04/04/2022 - 13h 05/04/2022 - 10h 
 sub-035  04/04/2022 - 16h 11/04/2022 - 16h 
 sub-036  06/04/2022 - 9h  08/04/2022 - 9h 
 sub-037  07/04/2022 - 15h 08/04/2022 - 13h 
 sub-038  08/04/2022 - 13h 25/04/2022 - 9h 
 sub-039  12/04/2022 - 9h  15/04/2022 - 14h 
 sub-040  14/04/2022 - 9h  25/04/2022 - 17h 
 sub-041  15/04/2022 - 13h 27/04/2022 - 14h 
 sub-042  19/04/2022 - 16h 20/04/2022 - 16h 
 sub-043  20/04/2022 - 9h  21/04/2022 - 9h 
 sub-044  21/04/2022 - 13h 22/04/2022 - 17h 
 sub-045  21/04/2022 - 16h 28/04/2022 - 17h 
 sub-046  22/04/2022 - 16h 27/04/2022 - 17h 
 sub-047  25/04/2022 - 9h  28/04/2022 - 10h 
 sub-048  25/04/2022 - 10h30 04/05/2022 - 14h 
 sub-049  25/04/2022 - 16h 29/04/2022 - 9h 
 sub-050  26/04/2022 - 13h 27/04/2022 - 10h 
 sub-051  27/04/2022 - 9h  03/05/2022 - 14h30 
 sub-052  27/04/2022 - 16h 02/05/2022 - 17h 
 sub-053  28/04/2022 - 13h 13/05/2022 - 14h 
 sub-054  28/04/2022 - 16h 04/05/2022 - 16h 
 sub-055  29/04/2022 - 13h 05/05/2022 - 16h 
 sub-056  29/04/2022 - 17h 03/05/2022 - 17h15 
 sub-057  06/05/2022 - 13h 10/05/2022 - 14h 
 sub-058  09/05/2022 - 14h 16/05/2022 - 16h 
 sub-059  09/05/2022 - 16h 16/05/2022 - 9h30 
 sub-060  10/05/2022 - 9h  11/05/2022 - 15h30 
 sub-061  10/05/2022 - 16h 12/05/2022 - 14h 
 sub-062  11/05/2022 - 16h 17/05/2022 - 16h 
 sub-063  12/05/2022 - 13h 19/05/2022 - 15h 
 sub-064  13/05/2022 - 13h 20/05/2022 - 16h 
 sub-065  17/05/2022 - 9h  18/05/2022 - 16h 
 sub-066  18/05/2022 - 9h  19/05/2022 - 9h 
 sub-067  18/05/2022 - 11h 20/05/2022 - 9h 
 sub-068  19/05/2022 - 13h 25/05/2022 - 10h 
 sub-069  20/05/2022 - 9h  23/05/2022 - 9h00 
 sub-070  24/05/2022 - 9h  31/05/2022 - 9h00 
 sub-071  25/05/2022 - 9h  31/05/2022 - 16h 
 sub-072  31/05/2022 - 13h 01/06/2022 - 13h 
 sub-073  01/05/2022 - 9h30 10/06/2022 - 14h 
 sub-074  08/06/2022 - 13h30 15/06/2022 - 15h30 
 sub-075  10/06/2022 - 13h30 15/06/2022 - 10h 
 sub-076  13/06/2022 - 16h 16/06/2022 - 17h 
 sub-077  13/06/2022 - 18h30 15/06/2022 - 18h30 
 sub-078  15/06/2022 - 13h30 20/06/2022 - 16h 
 sub-079  15/06/2022 - 16h 21/06/2022 - 16h 
 sub-080  16/06/2022 - 16h 22/06/2022 - 17h 
 sub-081  17/06/2022 - 9h30 21/06/2022 - 9h 
 sub-082  17/06/2022 - 13h30 24/06/2022 - 11h 
 sub-083  20/06/2022 - 13h30 23/06/2022 - 9h 
 sub-084  21/06/2022 - 10h30 23/06/2022 - 14h 
 sub-085  21/06/2022 - 16h 27/06/2022 - 16h 
 sub-086  22/06/2022 - 16h 29/06/2022 - 11h30 
 sub-087  23/06/2022 - 9h30 12/07/2022 - 10h 
 sub-088  24/06/2022 - 10h 30/06/2022 - 10h 
 sub-089  24/06/2022 - 16h 27/06/2022 - 18h 
 sub-090  27/06/2022 - 9h30 28/06/2022 - 14h 
 sub-091  27/06/2022 - 16h 04/07/2022 - 10h 
 sub-092  29/06/2022 - 16h 04/07/2022 - 16h 
 sub-093  01/07/2022 - 16h 05/07/2022 - 17h30 
 sub-094  04/07/2022 - 9h30 05/07/2022 - 12h 
 sub-095  08/07/2022 - 16h 13/07/2022 - 17h 
 sub-096  11/07/2022 - 17h 26/07/2022 - 17h 
 sub-097  12/07/2022 - 16h 19/07/2022 - 9h30 
 sub-098  20/07/2022 - 16h 25/07/2022 - 14h 
 sub-099  25/07/2022 - 16h 01/08/2022 - 16h 
 sub-100  26/07/2022 - 10h 01/08/2022 - 10h 
 sub-101  05/09/2022 - 13h30 07/09/2022 - 13h30 
 sub-102  07/09/2022 - 13h30 08/09/2022 - 14h 
 sub-103  26/09/2022 - 13h 27/09/2022 - 9h 
 sub-104  03/10/2022 - 9h30 04/10/2022 - 9h30 
 sub-105  07/10/2022 - 14h 11/10/2022 - 17h30 
 sub-106  07/10/2022 - 16h No session 
 sub-107  17/10/2022 - 16h30 20/10/2022 - 16h45 
 sub-108  19/10/2022 - 16h 20/10/2022 - 10h 
 sub-109  20/10/2022 - 13h30 21/10/2022 - 15h 
 sub-110  21/10/2022 - 9h30 25/10/2022 - 9h30 
 sub-111  26/10/2022 - 18h30 03/11/2022 - 18h30 
 sub-112  27/10/2022 - 16h 07/11/2022 - 14h 
 sub-113  28/10/2022 - 17h30 02/11/2022 - 9h 
 sub-114  10/11/2022 - 13h30 10/11/2022 - 15h 
 sub-115  24/11/2022 - 13h 01/12/2022 - 13h30 
 sub-116  29/11/2022 - 10h 08/12/2022 - 13h 
 sub-117  02/12/2022 - 15h30 09/12/2022 - 16h30 
 sub-118  05/12/2022 - 13h 20/12/2022 - 14h30 
 sub-119  05/12/2022 - 10h 12/12/2022 - 18h30 
 sub-120  07/12/2022 - 15h30 09/12/2022 - 15h00 
 sub-121  20/12/2022 - 10h 21/12/2022 - 10h 
 sub-122  18/01/2023 - 10h 25/01/2023 - 13h 
 sub-123  18/01/2023 - 13h 19/01/2023 - 14h 
 sub-124  19/01/2023 - 14h 23/01/2023 - 14h 
 sub-125  19/01/2023 - 16h 23/01/2023 - 9h 
 sub-126  20/01/2023 - 16h 01/02/2023 - 15h 
 sub-127  30/01/2023 - 17h 06/02/2023 - 15h30 
 sub-128  31/01/2023 - 15h30 06/02/2023 - 9h 
 sub-129  01/02/2023 - 17h 02/02/2023 - 10h 
 sub-130  03/02/2023 - 15h30 08/02/2023 - 16h 
 sub-131  08/02/2023 - 10h 09/02/2023 - 10h 
 sub-132  08/02/2023 - 15h30 15/02/2023 - 16h30 
 sub-133  14/02/2023 - 17h 20/02/2023 - 10h 
 sub-134  15/03/2023 - 15h30 22/03/2023 - 15h30 
 sub-135  16/03/2023 - 15h00 17/03/2023 - 8h30 
 sub-136  23/03/2023 - 15h30 27/03/2023 - 10h30 
 sub-137  28/03/2023 - 13h30 29/03/2023 - 17h 
 sub-138  29/03/2023 - 17h 03/04/2023 - 17h 
 sub-139  30/03/2023 - 9h  31/03/2023 - 9h 
 sub-140  31/03/2023 - 15h30 05/04/2023 - 8h30 
 sub-141  05/04/2023 - 9h  12/04/2023 - 15h30 
 sub-142  12/04/2023 - 11h 19/04/2023 - 11h 
 sub-143  13/04/2023 - 15h30 19/04/2023 - 9h 
 sub-144  17/04/2023 - 10h 20/04/2023 - 13h30 
 sub-145  19/04/2023 - 13h30 20/04/2023 - 9h 
 sub-146  21/04/2023 - 10h 21/04/2023 - 10h 
 sub-147  21/04/2023 - 17h 04/05/2023 - 14h 
 sub-148  24/04/2023 - 14h 26/04/2023 - 14h 
 sub-149  25/04/2023 - 15h 26/04/2023 - 15h30 
 sub-150  26/04/2023 - 15h 04/05/2023 - 14h 
 sub-151  27/04/2023 - 15h30 02/05/2023 - 14h 
 sub-152  05/05/2023 - 8h  09/05/2023 - 17h 
 sub-153  05/05/2023 - 14h 11/05/2023 - 16h 
 sub-154  11/05/2023 - 17h 15/05/2023 - 17h 
 sub-155  12/05/2023 - 14h 22/05/2023 - 16h30 
 sub-156  12/05/2023 - 17h30 17/05/2023 - 17h30 
 sub-157  23/05/2023 - 10h 26/05/2023 - 14h 
 sub-158  05/06/2023 - 17h 07/06/2023 - 16h15 
 sub-159  06/06/2023 - 11h 07/06/2023 - 10h 
 sub-160  29/06/2023 - 16h30 30/06/2023 - 17h 
 sub-161  06/07/2023 - 15h 10/07/2023 - 15h 
 sub-162  10/07/2023 - 9h30 11/07/2023 - 15h 
 sub-163  11/07/2023 - 10h No session 
 sub-164  12/07/2023 - 16h30 13/07/2023 - 17h 
 sub-165  17/07/2023 - 16h30 18/07/2023 - 17h",1,0,0
ds005274,2024-06-21 19:59:14,Yukako Ito,1.0.0,UV_EEG,2024-07-07 9:38:41,0,1,75399900,0.07 GB,135,22,20,26,1.6.0,CC0,Yukako Ito,,,,,doi:10.18112/openneuro.ds005274.v1.0.0,,images,,EEG,,1,0,0
ds005342,2024-07-15 16:58:04,Nayid Triana Guzman,1.0.3,EEG data offline and online during motor imagery for standing and sitting,2024-07-15 19:31:33,0,1,2181600000,2 GB,134,32,19,29,1.8.0,CC0,"Nayid Triana-Guzman, Alvaro D Orjuela-Cañon, Andres L Jutinico, Omar Mendoza-Montoya, Javier M Antelis","We would like to thank the Ministerio de Ciencia, Tecnología e Innovación of Colombia-Minciencias/Colciencias, Universidad Antonio Nariño (UAN), Universidad del Rosario, and Tecnologico de Monterrey for all their support in the development of this dataset","Triana-Guzman N, Orjuela-Cañon AD, Jutinico AL, Mendoza-Montoya O and Antelis JM (2024). EEG data offline and online during motor imagery for standing and sitting. OpenNeuro Dataset ds005342. doi: doi:10.18112/openneuro.ds005342.v1.0.3","This research was funded by Ministerio de Ciencia, Tecnología e Innovación of Colombia-Minciencias/Colciencias, contract 594-2019, project 80740-594-2019, and in part by the University of Illinois Chicago and Tecnologico de Monterrey (UIC-TEC) Seed funding Program 2021-2022","Triana-Guzman N, Orjuela-Cañon AD, Jutinico AL, Mendoza-Montoya O and Antelis JM (2022) Decoding EEG rhythms offline and online during motor imagery for standing and sitting based on a brain-computer interface. Front. Neuroinform. 16:961089. https://doi.org/10.3389/fninf.2022.961089",doi:10.18112/openneuro.ds005342.v1.0.3,The experimental protocol was approved by the ethics committee of the Universidad Antonio Nariño,sitstand,8.1.0,EEG,"The experiments were conducted in an acoustically isolated room where only the participant and the experimenter were present. Participants voluntarily signed an informed consent form in accordance with the experimental protocol approved by the ethics committee of the Universidad Antonio Nariño. The participant was seated in a chair in a posture that was comfortable for him/her but did not affect data collection. In front of the participant, a 40-inch TV screen was placed at about 3 m. On this screen, a graphical user interface (GUI) displayed images that guided the participant through the experiment. Each experimental session was divided into two phases: an offline phase and an online phase.
 

 The offline experiments consisted of recording participants´ EEG signals during motor imagery trials for standing and sitting that were guided by the GUI presented on the TV screen. Six offline runs were conducted in which the participants were standing in three runs and sitting in the other three runs. In each run, the participant had to repeat a block of 30 trials of mental tasks indicated by visual cues continuously presented on the screen in a pseudo-random sequence.
 

 The first phase of the experimental session was conducted to construct the offline parts of the dataset: (A) Sit-to-stand and (B) Stand-to-sit. The participant´s EEG data were collected from 90 sequences for part A (45 trials of MotorImageryA tasks and 45 trials of IdleStateA tasks) and 90 sequences for part B (45 trials of MotorImageryB tasks and 45 trials of IdleStateB tasks).
 

 For each participant, the two machine learning models obtained in the offline phase were used to carry out the online experiment parts of the dataset: (C) Sit-to-stand and (D) Stand-to-sit. Each participant was instructed to select, in no particular order, 30 sequences for part C (15 trials of MotorImageryA tasks and 15 trials of IdleStateA tasks) and 30 other sequences for part D (15 trials of MotorImageryB tasks and 15 trials of IdleStateB tasks). Each trial was unique and was generated pseudo-randomly before the experiment.
 

 The database consisted of 32 electroencephalographic files corresponding to the 32 participants. All recordings were collected on channels F3, Fz, F4, FC5, FC1, FC2, FC6, C3, Cz, C4, CP5, CP1, CP2, CP6, P3, Pz, and P4 according to the 10-20 EEG electrode placement standard, grounded to AFz channel and referenced to right mastoid (M2). Each data file contained the data stream in a 2D matrix where rows corresponded to channels and columns corresponded to time samples with a sampling frequency of 250Hz.
 

 The following marker numbers encoded information about the execution of the experiment. Marker numbers 200, 201, 202, and 203, indicated the beginning and end of the four steps of the sequence in a trial (resting, fixation, action observation, and imagining). Marker numbers 1, 2, 3, and 4, indicated the figure activated on the screen to the participant perform the task corresponding to 1. actively imagining the sit-to-stand movement (labeled as MotorImageryA), 2. sitting motionless without imagining the sit-to-stand movement (labeled as IdleStateA), 3. standing motionless while actively imagining the stand-to-sit movement (labeled as MotorImageryB), or 4. standing motionless without imagining the stand-to-sit movement (labeled as IdleStateB). Finally, marker numbers 101, 102, 103, and 104, indicated the task detected by the BCI in real time during the online experiment: 101. MotorImageryA, 102. IdleStateA, 103. MotorImageryB, or 104. IdleStateB.",1,0,0